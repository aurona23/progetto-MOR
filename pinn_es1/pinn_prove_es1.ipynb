{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14717,
     "status": "ok",
     "timestamp": 1747754813415,
     "user": {
      "displayName": "Du Ysuwiqiq",
      "userId": "09648783062728575936"
     },
     "user_tz": -120
    },
    "id": "jVmn8NGPocrV"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import random\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Rete fully connected con 3 hidden layers e attivazione Tanh\n",
    "        self.fc1 = nn.Linear(4, 64)    # input: x0, x1, mu0, mu1\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.out = nn.Linear(64, 1)    # output: u(x, mu)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, xmu):\n",
    "        x = self.activation(self.fc1(xmu))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Rete fully connected con 3 hidden layers e attivazione Tanh\n",
    "        self.fc1 = nn.Linear(4, 64)    # input: x0, x1, mu0, mu1\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)   # layer aggiunto\n",
    "        self.out = nn.Linear(64, 1)    # output: u(x, mu)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, xmu):\n",
    "        x = self.activation(self.fc1(xmu))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.activation(self.fc4(x))  # nuovo layer\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "class Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Rete fully connected con 3 hidden layers e attivazione Tanh\n",
    "        self.fc1 = nn.Linear(4, 128)    # input: x0, x1, mu0, mu1\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 128)   # layer aggiunto\n",
    "        self.out = nn.Linear(128, 1)    # output: u(x, mu)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, xmu):\n",
    "        x = self.activation(self.fc1(xmu))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.activation(self.fc4(x))  # nuovo layer\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Rete fully connected con 3 hidden layers e attivazione Tanh\n",
    "        self.fc1 = nn.Linear(4, 128)    # input: x0, x1, mu0, mu1\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)   # layer aggiunto\n",
    "        self.out = nn.Linear(64, 1)    # output: u(x, mu)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, xmu):\n",
    "        x = self.activation(self.fc1(xmu))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.activation(self.fc4(x))  # nuovo layer\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "# fa schifo (credo problema sia la sigmoide)\n",
    "\n",
    "class Net5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(4,5)\n",
    "        self.hidden_layer1 = nn.Linear(5,5)\n",
    "        self.hidden_layer2 = nn.Linear(5,5)\n",
    "        self.hidden_layer3 = nn.Linear(5,5)\n",
    "        self.hidden_layer4 = nn.Linear(5,5)\n",
    "        self.output_layer = nn.Linear(5,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        layer1_out = torch.sigmoid(self.input_layer(input))\n",
    "        layer2_out = torch.sigmoid(self.hidden_layer1(layer1_out))\n",
    "        layer3_out = torch.sigmoid(self.hidden_layer2(layer2_out))\n",
    "        layer4_out = torch.sigmoid(self.hidden_layer3(layer3_out))\n",
    "        layer5_out = torch.sigmoid(self.hidden_layer4(layer4_out))\n",
    "        output = self.output_layer(layer5_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Net6 – quattro hidden layer da 128→128→128→64\n",
    "# ------------------------------------------------------------------\n",
    "class Net6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)   # layer extra\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)     # layer già presente in Net4\n",
    "        self.out = nn.Linear(64, 1)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, xmu):\n",
    "        x = self.act(self.fc1(xmu))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.act(self.fc3(x))   # nuovo passaggio\n",
    "        x = self.act(self.fc4(x))\n",
    "        x = self.act(self.fc5(x))\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Net7 – cinque hidden layer a piramide 256→192→128→96→64\n",
    "# ------------------------------------------------------------------\n",
    "class Net7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 256)\n",
    "        self.fc2 = nn.Linear(256, 192)\n",
    "        self.fc3 = nn.Linear(192, 128)\n",
    "        self.fc4 = nn.Linear(128, 96)    # layer extra\n",
    "        self.fc5 = nn.Linear(96, 64)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, xmu):\n",
    "        x = self.act(self.fc1(xmu))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.act(self.fc3(x))\n",
    "        x = self.act(self.fc4(x))   # nuovo layer\n",
    "        x = self.act(self.fc5(x))\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Net8 – sei hidden layer uniformi 128↦128, con residual skip\n",
    "# ------------------------------------------------------------------\n",
    "class Net8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(4 if i == 0 else 128, 128)\n",
    "                                     for i in range(6)])  # 6 hidden layer\n",
    "        self.out = nn.Linear(128, 1)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, xmu):\n",
    "        x = xmu\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = self.act(layer(x))\n",
    "            # skip residual ogni due layer\n",
    "            if i % 2 == 1:\n",
    "                x = (x + h) / 2\n",
    "            else:\n",
    "                x = h\n",
    "        return self.out(x)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Net9 – sei hidden layer 256→256→192→128→96→64 (Tanh)\n",
    "# ------------------------------------------------------------------\n",
    "class Net9(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 192)\n",
    "        self.fc4 = nn.Linear(192, 128)\n",
    "        self.fc5 = nn.Linear(128, 96)\n",
    "        self.fc6 = nn.Linear(96, 64)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, xmu):\n",
    "        x = self.act(self.fc1(xmu))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.act(self.fc3(x))\n",
    "        x = self.act(self.fc4(x))\n",
    "        x = self.act(self.fc5(x))\n",
    "        x = self.act(self.fc6(x))\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Net10 – cinque hidden layer piramidale 256→192→128→96→64 (ReLU)\n",
    "# ------------------------------------------------------------------\n",
    "class Net10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 256)\n",
    "        self.fc2 = nn.Linear(256, 192)\n",
    "        self.fc3 = nn.Linear(192, 128)\n",
    "        self.fc4 = nn.Linear(128, 96)\n",
    "        self.fc5 = nn.Linear(96, 64)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, xmu):\n",
    "        x = self.act(self.fc1(xmu))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.act(self.fc3(x))\n",
    "        x = self.act(self.fc4(x))\n",
    "        x = self.act(self.fc5(x))\n",
    "        return self.out(x)\n",
    "# ------------------------------------------------------------------\n",
    "# Net11 – 8 hidden layer con skip‑connection ogni 2 layer (Tanh)\n",
    "# schema: 4 → 256 → 256  ↘\n",
    "#                     256 → 192 → 192  ↘\n",
    "#                                   192 → 128 → 128  ↘\n",
    "#                                                 128 → 64 → 64 → out\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "class Net11(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_in = nn.Linear(4, 256)\n",
    "\n",
    "        # blocco 1\n",
    "        self.b1_1 = nn.Linear(256, 256)\n",
    "        self.b1_2 = nn.Linear(256, 256)   # skip: in + out\n",
    "\n",
    "        # blocco 2\n",
    "        self.b2_1 = nn.Linear(256, 192)\n",
    "        self.b2_2 = nn.Linear(192, 192)   # skip\n",
    "\n",
    "        # blocco 3\n",
    "        self.b3_1 = nn.Linear(192, 128)\n",
    "        self.b3_2 = nn.Linear(128, 128)   # skip\n",
    "\n",
    "        # testa finale\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, xmu):\n",
    "        x = self.act(self.fc_in(xmu))       # 4 → 256\n",
    "\n",
    "        # ---- blocco 1 ------------------------------------------------\n",
    "        h = self.act(self.b1_1(x))\n",
    "        h = self.act(self.b1_2(h))\n",
    "        x = x + h                           # skip (256)\n",
    "\n",
    "        # ---- blocco 2 ------------------------------------------------\n",
    "        h = self.act(self.b2_1(x))          # 256 → 192\n",
    "        h = self.act(self.b2_2(h))\n",
    "        x = h + nn.functional.pad(x, (0, -64))  # align dims 256→192\n",
    "\n",
    "        # ---- blocco 3 ------------------------------------------------\n",
    "        h = self.act(self.b3_1(x))          # 192 → 128\n",
    "        h = self.act(self.b3_2(h))\n",
    "        x = h + x[:, :128]                  # skip (taglia a 128)\n",
    "\n",
    "        # ---- testa finale -------------------------------------------\n",
    "        x = self.act(self.fc4(x))\n",
    "        x = self.act(self.fc5(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Net12 – sei hidden layer uniformi 128↦128, con residual skip\n",
    "# ------------------------------------------------------------------\n",
    "class Net12(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(4 if i == 0 else 128, 128)\n",
    "                                     for i in range(10)])  # 10 hidden layer\n",
    "        self.out = nn.Linear(128, 1)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, xmu):\n",
    "        x = xmu\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = self.act(layer(x))\n",
    "            # skip residual ogni due layer\n",
    "            if i % 2 == 1:\n",
    "                x = (x + h) / 2\n",
    "            else:\n",
    "                x = h\n",
    "        return self.out(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747754813429,
     "user": {
      "displayName": "Du Ysuwiqiq",
      "userId": "09648783062728575936"
     },
     "user_tz": -120
    },
    "id": "oBFshyW2pfXo"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=23):\n",
    "    random.seed(seed)                        # seed Python random\n",
    "    np.random.seed(seed)                     # seed numpy random\n",
    "    torch.manual_seed(seed)                  # seed CPU\n",
    "    torch.cuda.manual_seed(seed)             # seed GPU (if available)\n",
    "    torch.cuda.manual_seed_all(seed)         # se hai più GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747754813436,
     "user": {
      "displayName": "Du Ysuwiqiq",
      "userId": "09648783062728575936"
     },
     "user_tz": -120
    },
    "id": "0rfCIMVnpSD2"
   },
   "outputs": [],
   "source": [
    "seed = 23\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1747754813463,
     "user": {
      "displayName": "Du Ysuwiqiq",
      "userId": "09648783062728575936"
     },
     "user_tz": -120
    },
    "id": "PH64bh8rpohD"
   },
   "outputs": [],
   "source": [
    "# ------------------------ Calcolo del residuo PDE ------------------------\n",
    "def pde_residual(xmu, net):\n",
    "    \"\"\"\n",
    "    Calcola il residuo della PDE in ogni punto (interno al dominio)\n",
    "    -xmu: tensor Nx4 con (x0, x1, mu0, mu1)\n",
    "    -net: la rete neurale che approssima u\n",
    "    \"\"\"\n",
    "    xmu.requires_grad_(True)      # necessario per calcolare derivate rispetto a xmu\n",
    "    u = net(xmu)                 # output della rete: u(x,mu)\n",
    "\n",
    "    # Calcolo gradiente di u rispetto a input (x0, x1) per le derivate prime\n",
    "    grads = torch.autograd.grad(u.sum(), xmu, create_graph=True)[0]\n",
    "    u_x0 = grads[:, 0:1]          # ∂u/∂x0\n",
    "    u_x1 = grads[:, 1:2]          # ∂u/∂x1\n",
    "\n",
    "    # Calcolo derivate seconde per laplaciano\n",
    "    u_x0x0 = torch.autograd.grad(u_x0.sum(), xmu, create_graph=True)[0][:, 0:1]  # ∂²u/∂x0²\n",
    "    u_x1x1 = torch.autograd.grad(u_x1.sum(), xmu, create_graph=True)[0][:, 1:2]  # ∂²u/∂x1²\n",
    "    laplacian_u = u_x0x0 + u_x1x1  # ∆u\n",
    "\n",
    "    # Estrazione parametri mu0 e mu1\n",
    "    mu0 = xmu[:, 2:3]\n",
    "    mu1 = xmu[:, 3:4]\n",
    "\n",
    "    # Coordinate spaziali\n",
    "    x0 = xmu[:, 0:1]\n",
    "    x1 = xmu[:, 1:2]\n",
    "\n",
    "    # Termine sorgente g(x;mu)\n",
    "    g = 100 * torch.sin(2 * np.pi * x0) * torch.cos(2 * np.pi * x1)\n",
    "\n",
    "    # Termine non lineare con mu0, mu1\n",
    "    nonlinear = (mu0 / mu1) * (torch.exp(mu1 * u) - 1)\n",
    "\n",
    "    # Residuo PDE: -∆u + nonlinear - g = 0\n",
    "    residual = -laplacian_u + nonlinear - g\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747754813467,
     "user": {
      "displayName": "Du Ysuwiqiq",
      "userId": "09648783062728575936"
     },
     "user_tz": -120
    },
    "id": "i2yW0GYupprf"
   },
   "outputs": [],
   "source": [
    "# ------------------------ Generazione punti dominio e bordo ------------------------\n",
    "def generate_domain_points(N_interior, N_boundary, mu0_range, mu1_range):\n",
    "    \"\"\"\n",
    "    Genera punti interni e di bordo del dominio con campionamento uniforme\n",
    "    anche dei parametri mu0, mu1 negli intervalli specificati\n",
    "    \"\"\"\n",
    "    mu0_min, mu0_max = mu0_range\n",
    "    mu1_min, mu1_max = mu1_range\n",
    "\n",
    "    # --- Punti interni ---\n",
    "    x0 = torch.rand(N_interior, 1)  # coordinate x0 in (0,1)\n",
    "    x1 = torch.rand(N_interior, 1)  # coordinate x1 in (0,1)\n",
    "    mu0 = mu0_min + (mu0_max - mu0_min) * torch.rand(N_interior, 1)  # parametri mu0 casuali nell’intervallo\n",
    "    mu1 = mu1_min + (mu1_max - mu1_min) * torch.rand(N_interior, 1)  # parametri mu1 casuali nell’intervallo\n",
    "    xmu_interior = torch.cat([x0, x1, mu0, mu1], dim=1)\n",
    "\n",
    "    # --- Punti sul bordo ---\n",
    "    xb = []\n",
    "    for side in range(4):\n",
    "        s = torch.rand(N_boundary, 1)          # coordinata variabile sul lato\n",
    "        zeros = torch.zeros_like(s)             # vettore di zeri\n",
    "        ones = torch.ones_like(s)               # vettore di uni\n",
    "\n",
    "        if side == 0:\n",
    "            # lato inferiore y=0\n",
    "            x0b, x1b = s, zeros\n",
    "        elif side == 1:\n",
    "            # lato superiore y=1\n",
    "            x0b, x1b = s, ones\n",
    "        elif side == 2:\n",
    "            # lato sinistro x=0\n",
    "            x0b, x1b = zeros, s\n",
    "        else:\n",
    "            # lato destro x=1\n",
    "            x0b, x1b = ones, s\n",
    "\n",
    "        mu0b = mu0_min + (mu0_max - mu0_min) * torch.rand(N_boundary, 1)  # mu0 bordo\n",
    "        mu1b = mu1_min + (mu1_max - mu1_min) * torch.rand(N_boundary, 1)  # mu1 bordo\n",
    "\n",
    "        xb.append(torch.cat([x0b, x1b, mu0b, mu1b], dim=1))\n",
    "\n",
    "    xmu_boundary = torch.cat(xb, dim=0)\n",
    "    n_boundary_points = len(xmu_boundary)\n",
    "\n",
    "\n",
    "\n",
    "    return xmu_interior, xmu_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1747754813524,
     "user": {
      "displayName": "Du Ysuwiqiq",
      "userId": "09648783062728575936"
     },
     "user_tz": -120
    },
    "id": "VfQ29fWap2UP"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_pinn_poly(\n",
    "        net, net_name,\n",
    "        epochs=20000, N_interior=1500, N_boundary=200,\n",
    "        lr=1e-3, mu0_range=(0.1, 1.0), mu1_range=(0.1, 1.0),\n",
    "        power=2.0, seed=23,\n",
    "        lambda_weight=1.0,               # <-- aggiunto parametro lambda di default 1.0\n",
    "        csv_path=None,\n",
    "        log_step=200\n",
    "):\n",
    "    # Se non specificato, crea il nome del file CSV includendo lambda_weight\n",
    "    if csv_path is None:\n",
    "        csv_path = f\"training_log_lambda{lambda_weight:.3f}.csv\"\n",
    "\n",
    "    # reproducibility --------------------------------------------------\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: (1 - epoch / epochs) ** power)\n",
    "\n",
    "    # prepara CSV ------------------------------------------------------\n",
    "    first_write = not os.path.exists(csv_path)\n",
    "    with open(csv_path, \"a\", newline=\"\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        if first_write:\n",
    "            writer.writerow([\"net\", \"epoch\",\n",
    "                             \"loss\", \"pde_loss\", \"bc_loss\",\n",
    "                             \"epoch_time_s\", \"elapsed_s\", \"lambda_weight\"])\n",
    "\n",
    "    # training ---------------------------------------------------------\n",
    "    t0 = time.time()\n",
    "    prev_time = t0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        optimizer.zero_grad()\n",
    "        net.train()\n",
    "\n",
    "        # sample points\n",
    "        # sono i punti che uso ad ogni epoca per addestrarre come se fosserro un unico grande batch\n",
    "        # in questo caso 1500 punti interni e 200*4 punti sui bordi ad ogni epoca\n",
    "\n",
    "        xmu_int, xmu_bnd = generate_domain_points(\n",
    "            N_interior, N_boundary, mu0_range, mu1_range\n",
    "        )\n",
    "        xmu_int, xmu_bnd = xmu_int.to(device), xmu_bnd.to(device)\n",
    "\n",
    "        # losses\n",
    "        res_int  = pde_residual(xmu_int, net)\n",
    "        loss_pde = torch.mean(res_int ** 2)\n",
    "        u_bnd    = net(xmu_bnd)\n",
    "        loss_bc  = torch.mean(u_bnd ** 2)\n",
    "        loss     = loss_pde + lambda_weight * loss_bc    # usa lambda_weight qui\n",
    "\n",
    "        # optimise\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # timing\n",
    "        now = time.time()\n",
    "        epoch_time = now - prev_time\n",
    "        elapsed    = now - t0\n",
    "        prev_time  = now\n",
    "\n",
    "\n",
    "        if epoch % log_step == 0 or epoch == 1 or epoch == epochs:\n",
    "            log = (f\"[{net_name}] Ep {epoch}: \"\n",
    "                   f\"loss={loss.item():.6f}, pde={loss_pde.item():.6f}, \"\n",
    "                   f\"bc={loss_bc.item():.6f}, λ={lambda_weight:.3f}, \"\n",
    "                   f\"dt={epoch_time:.3f}s, \"\n",
    "                   f\"elapsed={elapsed/60:.1f}m\")\n",
    "            print(log)\n",
    "\n",
    "            with open(csv_path, \"a\", newline=\"\") as csv_file:\n",
    "                writer = csv.writer(csv_file)\n",
    "                writer.writerow([net_name, epoch,\n",
    "                                 f\"{loss.item():.6f}\",\n",
    "                                 f\"{loss_pde.item():.6f}\",\n",
    "                                 f\"{loss_bc.item():.6f}\",\n",
    "                                 f\"{epoch_time:.3f}\",\n",
    "                                 f\"{elapsed:.3f}\",\n",
    "                                 f\"{lambda_weight:.3f}\"])\n",
    "\n",
    "    total_min = (time.time() - t0) / 60\n",
    "    print(f\"{net_name} training complete in {total_min:.1f} min \"\n",
    "          f\"(final loss {loss.item():.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3131779,
     "status": "ok",
     "timestamp": 1747757945305,
     "user": {
      "displayName": "Du Ysuwiqiq",
      "userId": "09648783062728575936"
     },
     "user_tz": -120
    },
    "id": "3bvMGoXbOc2n",
    "outputId": "3b3e52a6-714d-46af-bc3e-3ed09fe7e133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Net1] Ep 1: loss=2597.005127, pde=2596.995605, bc=0.009457, λ=1.000, dt=0.833s, elapsed=0.0m\n",
      "[Net1] Ep 200: loss=139.826950, pde=133.907547, bc=5.919397, λ=1.000, dt=0.008s, elapsed=0.0m\n",
      "[Net1] Ep 400: loss=19.142414, pde=10.957758, bc=8.184657, λ=1.000, dt=0.008s, elapsed=0.1m\n",
      "[Net1] Ep 600: loss=8.822965, pde=3.275696, bc=5.547268, λ=1.000, dt=0.011s, elapsed=0.1m\n",
      "[Net1] Ep 800: loss=6.066253, pde=1.866035, bc=4.200218, λ=1.000, dt=0.008s, elapsed=0.1m\n",
      "[Net1] Ep 1000: loss=4.639891, pde=1.347470, bc=3.292421, λ=1.000, dt=0.008s, elapsed=0.2m\n",
      "[Net1] Ep 1200: loss=3.386878, pde=0.845654, bc=2.541224, λ=1.000, dt=0.008s, elapsed=0.2m\n",
      "[Net1] Ep 1400: loss=2.935032, pde=0.797536, bc=2.137496, λ=1.000, dt=0.008s, elapsed=0.2m\n",
      "[Net1] Ep 1600: loss=2.693820, pde=0.726453, bc=1.967367, λ=1.000, dt=0.008s, elapsed=0.2m\n",
      "[Net1] Ep 1800: loss=2.309440, pde=0.508185, bc=1.801255, λ=1.000, dt=0.008s, elapsed=0.3m\n",
      "[Net1] Ep 2000: loss=2.342826, pde=0.545673, bc=1.797154, λ=1.000, dt=0.008s, elapsed=0.3m\n",
      "[Net1] Ep 2200: loss=2.074890, pde=0.438883, bc=1.636007, λ=1.000, dt=0.008s, elapsed=0.3m\n",
      "[Net1] Ep 2400: loss=2.034908, pde=0.396206, bc=1.638702, λ=1.000, dt=0.008s, elapsed=0.4m\n",
      "[Net1] Ep 2600: loss=1.942396, pde=0.358885, bc=1.583511, λ=1.000, dt=0.008s, elapsed=0.4m\n",
      "[Net1] Ep 2800: loss=1.871427, pde=0.326167, bc=1.545260, λ=1.000, dt=0.008s, elapsed=0.4m\n",
      "[Net1] Ep 3000: loss=1.756453, pde=0.314679, bc=1.441774, λ=1.000, dt=0.008s, elapsed=0.4m\n",
      "[Net1] Ep 3200: loss=1.721565, pde=0.337447, bc=1.384118, λ=1.000, dt=0.008s, elapsed=0.5m\n",
      "[Net1] Ep 3400: loss=1.649269, pde=0.261882, bc=1.387387, λ=1.000, dt=0.008s, elapsed=0.5m\n",
      "[Net1] Ep 3600: loss=1.696580, pde=0.269916, bc=1.426664, λ=1.000, dt=0.008s, elapsed=0.5m\n",
      "[Net1] Ep 3800: loss=1.460694, pde=0.210748, bc=1.249945, λ=1.000, dt=0.008s, elapsed=0.6m\n",
      "[Net1] Ep 4000: loss=1.490211, pde=0.241322, bc=1.248889, λ=1.000, dt=0.008s, elapsed=0.6m\n",
      "[Net1] Ep 4200: loss=1.283974, pde=0.197893, bc=1.086081, λ=1.000, dt=0.008s, elapsed=0.6m\n",
      "[Net1] Ep 4400: loss=1.292990, pde=0.185811, bc=1.107179, λ=1.000, dt=0.010s, elapsed=0.6m\n",
      "[Net1] Ep 4600: loss=1.230720, pde=0.172034, bc=1.058685, λ=1.000, dt=0.008s, elapsed=0.7m\n",
      "[Net1] Ep 4800: loss=1.166446, pde=0.202721, bc=0.963725, λ=1.000, dt=0.008s, elapsed=0.7m\n",
      "[Net1] Ep 5000: loss=1.220969, pde=0.231290, bc=0.989679, λ=1.000, dt=0.008s, elapsed=0.7m\n",
      "[Net1] Ep 5200: loss=1.076734, pde=0.188939, bc=0.887795, λ=1.000, dt=0.008s, elapsed=0.8m\n",
      "[Net1] Ep 5400: loss=1.033572, pde=0.177722, bc=0.855850, λ=1.000, dt=0.008s, elapsed=0.8m\n",
      "[Net1] Ep 5600: loss=1.004399, pde=0.173749, bc=0.830650, λ=1.000, dt=0.008s, elapsed=0.8m\n",
      "[Net1] Ep 5800: loss=0.905196, pde=0.152732, bc=0.752465, λ=1.000, dt=0.012s, elapsed=0.8m\n",
      "[Net1] Ep 6000: loss=0.830176, pde=0.142782, bc=0.687394, λ=1.000, dt=0.008s, elapsed=0.9m\n",
      "[Net1] Ep 6200: loss=0.784294, pde=0.131128, bc=0.653166, λ=1.000, dt=0.008s, elapsed=0.9m\n",
      "[Net1] Ep 6400: loss=0.723102, pde=0.143172, bc=0.579930, λ=1.000, dt=0.008s, elapsed=0.9m\n",
      "[Net1] Ep 6600: loss=0.677355, pde=0.144449, bc=0.532905, λ=1.000, dt=0.008s, elapsed=1.0m\n",
      "[Net1] Ep 6800: loss=0.658837, pde=0.137354, bc=0.521483, λ=1.000, dt=0.008s, elapsed=1.0m\n",
      "[Net1] Ep 7000: loss=0.568515, pde=0.115374, bc=0.453140, λ=1.000, dt=0.008s, elapsed=1.0m\n",
      "[Net1] Ep 7200: loss=0.554310, pde=0.124236, bc=0.430074, λ=1.000, dt=0.010s, elapsed=1.0m\n",
      "[Net1] Ep 7400: loss=0.511974, pde=0.111552, bc=0.400422, λ=1.000, dt=0.008s, elapsed=1.1m\n",
      "[Net1] Ep 7600: loss=0.481652, pde=0.129361, bc=0.352290, λ=1.000, dt=0.008s, elapsed=1.1m\n",
      "[Net1] Ep 7800: loss=0.405629, pde=0.102003, bc=0.303626, λ=1.000, dt=0.008s, elapsed=1.1m\n",
      "[Net1] Ep 8000: loss=0.394356, pde=0.114666, bc=0.279691, λ=1.000, dt=0.008s, elapsed=1.2m\n",
      "[Net1] Ep 8200: loss=0.371103, pde=0.120159, bc=0.250944, λ=1.000, dt=0.008s, elapsed=1.2m\n",
      "[Net1] Ep 8400: loss=0.361505, pde=0.113157, bc=0.248348, λ=1.000, dt=0.008s, elapsed=1.2m\n",
      "[Net1] Ep 8600: loss=0.306801, pde=0.094707, bc=0.212094, λ=1.000, dt=0.013s, elapsed=1.3m\n",
      "[Net1] Ep 8800: loss=0.278175, pde=0.085465, bc=0.192709, λ=1.000, dt=0.016s, elapsed=1.3m\n",
      "[Net1] Ep 9000: loss=0.280177, pde=0.097693, bc=0.182483, λ=1.000, dt=0.009s, elapsed=1.3m\n",
      "[Net1] Ep 9200: loss=0.252867, pde=0.090394, bc=0.162473, λ=1.000, dt=0.008s, elapsed=1.4m\n",
      "[Net1] Ep 9400: loss=0.225662, pde=0.080873, bc=0.144788, λ=1.000, dt=0.008s, elapsed=1.4m\n",
      "[Net1] Ep 9600: loss=0.211811, pde=0.075279, bc=0.136532, λ=1.000, dt=0.008s, elapsed=1.4m\n",
      "[Net1] Ep 9800: loss=0.209114, pde=0.088599, bc=0.120515, λ=1.000, dt=0.011s, elapsed=1.4m\n",
      "[Net1] Ep 10000: loss=0.202826, pde=0.074689, bc=0.128137, λ=1.000, dt=0.012s, elapsed=1.5m\n",
      "[Net1] Ep 10200: loss=0.178489, pde=0.067082, bc=0.111408, λ=1.000, dt=0.008s, elapsed=1.5m\n",
      "[Net1] Ep 10400: loss=0.175843, pde=0.074983, bc=0.100860, λ=1.000, dt=0.008s, elapsed=1.5m\n",
      "[Net1] Ep 10600: loss=0.171385, pde=0.065994, bc=0.105390, λ=1.000, dt=0.008s, elapsed=1.6m\n",
      "[Net1] Ep 10800: loss=0.150424, pde=0.054486, bc=0.095938, λ=1.000, dt=0.008s, elapsed=1.6m\n",
      "[Net1] Ep 11000: loss=0.155991, pde=0.060012, bc=0.095979, λ=1.000, dt=0.008s, elapsed=1.6m\n",
      "[Net1] Ep 11200: loss=0.146966, pde=0.047977, bc=0.098989, λ=1.000, dt=0.008s, elapsed=1.6m\n",
      "[Net1] Ep 11400: loss=0.136785, pde=0.051146, bc=0.085639, λ=1.000, dt=0.012s, elapsed=1.7m\n",
      "[Net1] Ep 11600: loss=0.137402, pde=0.051837, bc=0.085565, λ=1.000, dt=0.008s, elapsed=1.7m\n",
      "[Net1] Ep 11800: loss=0.133033, pde=0.050820, bc=0.082213, λ=1.000, dt=0.008s, elapsed=1.7m\n",
      "[Net1] Ep 12000: loss=0.134922, pde=0.055724, bc=0.079198, λ=1.000, dt=0.008s, elapsed=1.8m\n",
      "[Net1] Ep 12200: loss=0.121305, pde=0.044355, bc=0.076950, λ=1.000, dt=0.008s, elapsed=1.8m\n",
      "[Net1] Ep 12400: loss=0.117497, pde=0.043356, bc=0.074141, λ=1.000, dt=0.008s, elapsed=1.8m\n",
      "[Net1] Ep 12600: loss=0.118424, pde=0.041366, bc=0.077059, λ=1.000, dt=0.008s, elapsed=1.8m\n",
      "[Net1] Ep 12800: loss=0.113824, pde=0.043428, bc=0.070396, λ=1.000, dt=0.012s, elapsed=1.9m\n",
      "[Net1] Ep 13000: loss=0.114479, pde=0.044506, bc=0.069972, λ=1.000, dt=0.008s, elapsed=1.9m\n",
      "[Net1] Ep 13200: loss=0.107457, pde=0.035960, bc=0.071497, λ=1.000, dt=0.008s, elapsed=1.9m\n",
      "[Net1] Ep 13400: loss=0.111782, pde=0.037832, bc=0.073950, λ=1.000, dt=0.008s, elapsed=2.0m\n",
      "[Net1] Ep 13600: loss=0.110707, pde=0.041197, bc=0.069511, λ=1.000, dt=0.008s, elapsed=2.0m\n",
      "[Net1] Ep 13800: loss=0.104442, pde=0.036037, bc=0.068405, λ=1.000, dt=0.008s, elapsed=2.0m\n",
      "[Net1] Ep 14000: loss=0.101189, pde=0.034689, bc=0.066500, λ=1.000, dt=0.008s, elapsed=2.0m\n",
      "[Net1] Ep 14200: loss=0.110592, pde=0.041893, bc=0.068699, λ=1.000, dt=0.012s, elapsed=2.1m\n",
      "[Net1] Ep 14400: loss=0.092605, pde=0.031158, bc=0.061447, λ=1.000, dt=0.008s, elapsed=2.1m\n",
      "[Net1] Ep 14600: loss=0.099147, pde=0.034196, bc=0.064951, λ=1.000, dt=0.008s, elapsed=2.1m\n",
      "[Net1] Ep 14800: loss=0.104716, pde=0.039442, bc=0.065274, λ=1.000, dt=0.008s, elapsed=2.2m\n",
      "[Net1] Ep 15000: loss=0.098953, pde=0.035088, bc=0.063865, λ=1.000, dt=0.008s, elapsed=2.2m\n",
      "[Net1] Ep 15200: loss=0.092254, pde=0.034901, bc=0.057353, λ=1.000, dt=0.008s, elapsed=2.2m\n",
      "[Net1] Ep 15400: loss=0.092793, pde=0.032616, bc=0.060176, λ=1.000, dt=0.008s, elapsed=2.2m\n",
      "[Net1] Ep 15600: loss=0.090723, pde=0.031969, bc=0.058754, λ=1.000, dt=0.013s, elapsed=2.3m\n",
      "[Net1] Ep 15800: loss=0.088652, pde=0.027343, bc=0.061309, λ=1.000, dt=0.008s, elapsed=2.3m\n",
      "[Net1] Ep 16000: loss=0.089161, pde=0.029392, bc=0.059768, λ=1.000, dt=0.008s, elapsed=2.3m\n",
      "[Net1] Ep 16200: loss=0.086320, pde=0.030092, bc=0.056228, λ=1.000, dt=0.008s, elapsed=2.4m\n",
      "[Net1] Ep 16400: loss=0.085011, pde=0.028257, bc=0.056754, λ=1.000, dt=0.008s, elapsed=2.4m\n",
      "[Net1] Ep 16600: loss=0.087421, pde=0.027553, bc=0.059868, λ=1.000, dt=0.008s, elapsed=2.4m\n",
      "[Net1] Ep 16800: loss=0.080820, pde=0.025908, bc=0.054912, λ=1.000, dt=0.009s, elapsed=2.4m\n",
      "[Net1] Ep 17000: loss=0.089955, pde=0.030680, bc=0.059275, λ=1.000, dt=0.012s, elapsed=2.5m\n",
      "[Net1] Ep 17200: loss=0.086699, pde=0.028168, bc=0.058531, λ=1.000, dt=0.009s, elapsed=2.5m\n",
      "[Net1] Ep 17400: loss=0.092300, pde=0.030064, bc=0.062236, λ=1.000, dt=0.008s, elapsed=2.5m\n",
      "[Net1] Ep 17600: loss=0.083825, pde=0.026954, bc=0.056871, λ=1.000, dt=0.008s, elapsed=2.6m\n",
      "[Net1] Ep 17800: loss=0.084795, pde=0.027239, bc=0.057556, λ=1.000, dt=0.008s, elapsed=2.6m\n",
      "[Net1] Ep 18000: loss=0.088734, pde=0.031342, bc=0.057392, λ=1.000, dt=0.008s, elapsed=2.6m\n",
      "[Net1] Ep 18200: loss=0.081765, pde=0.024372, bc=0.057393, λ=1.000, dt=0.008s, elapsed=2.6m\n",
      "[Net1] Ep 18400: loss=0.080641, pde=0.025359, bc=0.055283, λ=1.000, dt=0.012s, elapsed=2.7m\n",
      "[Net1] Ep 18600: loss=0.084452, pde=0.030559, bc=0.053893, λ=1.000, dt=0.008s, elapsed=2.7m\n",
      "[Net1] Ep 18800: loss=0.084300, pde=0.025333, bc=0.058967, λ=1.000, dt=0.008s, elapsed=2.7m\n",
      "[Net1] Ep 19000: loss=0.078452, pde=0.023755, bc=0.054697, λ=1.000, dt=0.008s, elapsed=2.8m\n",
      "[Net1] Ep 19200: loss=0.081572, pde=0.030033, bc=0.051538, λ=1.000, dt=0.008s, elapsed=2.8m\n",
      "[Net1] Ep 19400: loss=0.078977, pde=0.023334, bc=0.055643, λ=1.000, dt=0.008s, elapsed=2.8m\n",
      "[Net1] Ep 19600: loss=0.076005, pde=0.023460, bc=0.052545, λ=1.000, dt=0.008s, elapsed=2.8m\n",
      "[Net1] Ep 19800: loss=0.084169, pde=0.025488, bc=0.058681, λ=1.000, dt=0.012s, elapsed=2.9m\n",
      "[Net1] Ep 20000: loss=0.078257, pde=0.022326, bc=0.055931, λ=1.000, dt=0.010s, elapsed=2.9m\n",
      "Net1 training complete in 2.9 min (final loss 0.078257)\n",
      "[Net2] Ep 1: loss=2597.089844, pde=2597.088379, bc=0.001403, λ=1.000, dt=0.010s, elapsed=0.0m\n",
      "[Net2] Ep 200: loss=1166.627197, pde=1156.799805, bc=9.827371, λ=1.000, dt=0.009s, elapsed=0.0m\n",
      "[Net2] Ep 400: loss=46.650063, pde=37.631958, bc=9.018105, λ=1.000, dt=0.010s, elapsed=0.1m\n",
      "[Net2] Ep 600: loss=10.174486, pde=4.657455, bc=5.517031, λ=1.000, dt=0.009s, elapsed=0.1m\n",
      "[Net2] Ep 800: loss=6.385496, pde=2.286616, bc=4.098880, λ=1.000, dt=0.009s, elapsed=0.1m\n",
      "[Net2] Ep 1000: loss=4.934849, pde=1.744645, bc=3.190204, λ=1.000, dt=0.014s, elapsed=0.2m\n",
      "[Net2] Ep 1200: loss=3.400568, pde=0.899783, bc=2.500785, λ=1.000, dt=0.010s, elapsed=0.2m\n",
      "[Net2] Ep 1400: loss=2.808075, pde=0.734693, bc=2.073383, λ=1.000, dt=0.009s, elapsed=0.2m\n",
      "[Net2] Ep 1600: loss=2.743000, pde=0.866072, bc=1.876928, λ=1.000, dt=0.009s, elapsed=0.3m\n",
      "[Net2] Ep 1800: loss=2.150973, pde=0.460752, bc=1.690222, λ=1.000, dt=0.011s, elapsed=0.3m\n",
      "[Net2] Ep 2000: loss=2.083570, pde=0.411320, bc=1.672250, λ=1.000, dt=0.010s, elapsed=0.3m\n",
      "[Net2] Ep 2200: loss=1.839705, pde=0.328669, bc=1.511035, λ=1.000, dt=0.012s, elapsed=0.4m\n",
      "[Net2] Ep 2400: loss=1.794060, pde=0.288224, bc=1.505836, λ=1.000, dt=0.009s, elapsed=0.4m\n",
      "[Net2] Ep 2600: loss=1.832577, pde=0.380586, bc=1.451991, λ=1.000, dt=0.011s, elapsed=0.4m\n",
      "[Net2] Ep 2800: loss=1.678810, pde=0.280574, bc=1.398236, λ=1.000, dt=0.014s, elapsed=0.5m\n",
      "[Net2] Ep 3000: loss=1.799652, pde=0.482812, bc=1.316840, λ=1.000, dt=0.010s, elapsed=0.5m\n",
      "[Net2] Ep 3200: loss=1.446321, pde=0.191853, bc=1.254468, λ=1.000, dt=0.010s, elapsed=0.5m\n",
      "[Net2] Ep 3400: loss=1.464337, pde=0.202538, bc=1.261799, λ=1.000, dt=0.012s, elapsed=0.6m\n",
      "[Net2] Ep 3600: loss=1.570686, pde=0.273631, bc=1.297055, λ=1.000, dt=0.010s, elapsed=0.6m\n",
      "[Net2] Ep 3800: loss=1.338772, pde=0.201791, bc=1.136981, λ=1.000, dt=0.010s, elapsed=0.6m\n",
      "[Net2] Ep 4000: loss=1.296175, pde=0.154209, bc=1.141966, λ=1.000, dt=0.009s, elapsed=0.7m\n",
      "[Net2] Ep 4200: loss=1.187047, pde=0.185421, bc=1.001626, λ=1.000, dt=0.009s, elapsed=0.7m\n",
      "[Net2] Ep 4400: loss=1.240051, pde=0.221603, bc=1.018448, λ=1.000, dt=0.010s, elapsed=0.7m\n",
      "[Net2] Ep 4600: loss=1.195629, pde=0.212976, bc=0.982653, λ=1.000, dt=0.012s, elapsed=0.8m\n",
      "[Net2] Ep 4800: loss=1.013665, pde=0.122591, bc=0.891074, λ=1.000, dt=0.009s, elapsed=0.8m\n",
      "[Net2] Ep 5000: loss=1.344765, pde=0.428783, bc=0.915982, λ=1.000, dt=0.009s, elapsed=0.8m\n",
      "[Net2] Ep 5200: loss=0.962015, pde=0.140370, bc=0.821645, λ=1.000, dt=0.009s, elapsed=0.9m\n",
      "[Net2] Ep 5400: loss=1.079020, pde=0.288843, bc=0.790177, λ=1.000, dt=0.009s, elapsed=0.9m\n",
      "[Net2] Ep 5600: loss=0.911645, pde=0.147069, bc=0.764577, λ=1.000, dt=0.009s, elapsed=0.9m\n",
      "[Net2] Ep 5800: loss=1.070527, pde=0.372156, bc=0.698371, λ=1.000, dt=0.012s, elapsed=1.0m\n",
      "[Net2] Ep 6000: loss=0.760093, pde=0.127546, bc=0.632547, λ=1.000, dt=0.009s, elapsed=1.0m\n",
      "[Net2] Ep 6200: loss=0.894000, pde=0.285423, bc=0.608577, λ=1.000, dt=0.009s, elapsed=1.0m\n",
      "[Net2] Ep 6400: loss=0.687618, pde=0.141789, bc=0.545828, λ=1.000, dt=0.009s, elapsed=1.1m\n",
      "[Net2] Ep 6600: loss=0.633138, pde=0.134125, bc=0.499012, λ=1.000, dt=0.010s, elapsed=1.1m\n",
      "[Net2] Ep 6800: loss=0.620433, pde=0.124081, bc=0.496352, λ=1.000, dt=0.016s, elapsed=1.1m\n",
      "[Net2] Ep 7000: loss=0.540306, pde=0.119407, bc=0.420898, λ=1.000, dt=0.011s, elapsed=1.2m\n",
      "[Net2] Ep 7200: loss=0.567663, pde=0.158729, bc=0.408934, λ=1.000, dt=0.010s, elapsed=1.2m\n",
      "[Net2] Ep 7400: loss=0.488451, pde=0.106519, bc=0.381931, λ=1.000, dt=0.009s, elapsed=1.3m\n",
      "[Net2] Ep 7600: loss=0.454792, pde=0.114923, bc=0.339869, λ=1.000, dt=0.009s, elapsed=1.3m\n",
      "[Net2] Ep 7800: loss=0.384707, pde=0.086031, bc=0.298676, λ=1.000, dt=0.010s, elapsed=1.3m\n",
      "[Net2] Ep 8000: loss=0.403606, pde=0.122718, bc=0.280888, λ=1.000, dt=0.009s, elapsed=1.3m\n",
      "[Net2] Ep 8200: loss=0.369245, pde=0.114404, bc=0.254841, λ=1.000, dt=0.016s, elapsed=1.4m\n",
      "[Net2] Ep 8400: loss=0.410148, pde=0.151942, bc=0.258206, λ=1.000, dt=0.010s, elapsed=1.4m\n",
      "[Net2] Ep 8600: loss=0.326210, pde=0.101347, bc=0.224863, λ=1.000, dt=0.009s, elapsed=1.5m\n",
      "[Net2] Ep 8800: loss=0.343462, pde=0.136019, bc=0.207443, λ=1.000, dt=0.010s, elapsed=1.5m\n",
      "[Net2] Ep 9000: loss=0.279381, pde=0.080124, bc=0.199257, λ=1.000, dt=0.010s, elapsed=1.5m\n",
      "[Net2] Ep 9200: loss=0.274230, pde=0.094625, bc=0.179605, λ=1.000, dt=0.009s, elapsed=1.6m\n",
      "[Net2] Ep 9400: loss=0.237369, pde=0.073737, bc=0.163632, λ=1.000, dt=0.013s, elapsed=1.6m\n",
      "[Net2] Ep 9600: loss=0.240452, pde=0.086002, bc=0.154451, λ=1.000, dt=0.011s, elapsed=1.6m\n",
      "[Net2] Ep 9800: loss=0.223256, pde=0.085688, bc=0.137567, λ=1.000, dt=0.011s, elapsed=1.7m\n",
      "[Net2] Ep 10000: loss=0.216279, pde=0.068536, bc=0.147744, λ=1.000, dt=0.010s, elapsed=1.7m\n",
      "[Net2] Ep 10200: loss=0.194255, pde=0.064410, bc=0.129845, λ=1.000, dt=0.010s, elapsed=1.7m\n",
      "[Net2] Ep 10400: loss=0.184893, pde=0.070071, bc=0.114822, λ=1.000, dt=0.009s, elapsed=1.8m\n",
      "[Net2] Ep 10600: loss=0.192036, pde=0.068284, bc=0.123752, λ=1.000, dt=0.012s, elapsed=1.8m\n",
      "[Net2] Ep 10800: loss=0.177382, pde=0.067321, bc=0.110061, λ=1.000, dt=0.009s, elapsed=1.8m\n",
      "[Net2] Ep 11000: loss=0.179311, pde=0.069253, bc=0.110057, λ=1.000, dt=0.009s, elapsed=1.9m\n",
      "[Net2] Ep 11200: loss=0.172080, pde=0.056900, bc=0.115180, λ=1.000, dt=0.010s, elapsed=1.9m\n",
      "[Net2] Ep 11400: loss=0.151802, pde=0.052831, bc=0.098971, λ=1.000, dt=0.009s, elapsed=1.9m\n",
      "[Net2] Ep 11600: loss=0.160428, pde=0.062372, bc=0.098056, λ=1.000, dt=0.009s, elapsed=2.0m\n",
      "[Net2] Ep 11800: loss=0.142595, pde=0.048566, bc=0.094028, λ=1.000, dt=0.009s, elapsed=2.0m\n",
      "[Net2] Ep 12000: loss=0.155157, pde=0.063107, bc=0.092049, λ=1.000, dt=0.010s, elapsed=2.0m\n",
      "[Net2] Ep 12200: loss=0.144752, pde=0.057406, bc=0.087346, λ=1.000, dt=0.010s, elapsed=2.1m\n",
      "[Net2] Ep 12400: loss=0.135221, pde=0.049600, bc=0.085621, λ=1.000, dt=0.010s, elapsed=2.1m\n",
      "[Net2] Ep 12600: loss=0.133788, pde=0.046885, bc=0.086902, λ=1.000, dt=0.009s, elapsed=2.1m\n",
      "[Net2] Ep 12800: loss=0.131376, pde=0.050512, bc=0.080864, λ=1.000, dt=0.009s, elapsed=2.2m\n",
      "[Net2] Ep 13000: loss=0.124445, pde=0.044589, bc=0.079855, λ=1.000, dt=0.010s, elapsed=2.2m\n",
      "[Net2] Ep 13200: loss=0.127118, pde=0.046519, bc=0.080599, λ=1.000, dt=0.009s, elapsed=2.2m\n",
      "[Net2] Ep 13400: loss=0.129599, pde=0.047509, bc=0.082090, λ=1.000, dt=0.010s, elapsed=2.3m\n",
      "[Net2] Ep 13600: loss=0.123898, pde=0.046920, bc=0.076978, λ=1.000, dt=0.009s, elapsed=2.3m\n",
      "[Net2] Ep 13800: loss=0.122560, pde=0.047381, bc=0.075179, λ=1.000, dt=0.009s, elapsed=2.3m\n",
      "[Net2] Ep 14000: loss=0.113211, pde=0.040625, bc=0.072585, λ=1.000, dt=0.009s, elapsed=2.4m\n",
      "[Net2] Ep 14200: loss=0.115134, pde=0.039944, bc=0.075191, λ=1.000, dt=0.009s, elapsed=2.4m\n",
      "[Net2] Ep 14400: loss=0.105428, pde=0.037399, bc=0.068029, λ=1.000, dt=0.009s, elapsed=2.4m\n",
      "[Net2] Ep 14600: loss=0.111803, pde=0.039466, bc=0.072337, λ=1.000, dt=0.010s, elapsed=2.5m\n",
      "[Net2] Ep 14800: loss=0.116176, pde=0.043858, bc=0.072318, λ=1.000, dt=0.010s, elapsed=2.5m\n",
      "[Net2] Ep 15000: loss=0.106839, pde=0.035643, bc=0.071196, λ=1.000, dt=0.009s, elapsed=2.5m\n",
      "[Net2] Ep 15200: loss=0.102096, pde=0.038822, bc=0.063273, λ=1.000, dt=0.015s, elapsed=2.6m\n",
      "[Net2] Ep 15400: loss=0.102671, pde=0.036392, bc=0.066279, λ=1.000, dt=0.010s, elapsed=2.6m\n",
      "[Net2] Ep 15600: loss=0.103167, pde=0.039169, bc=0.063997, λ=1.000, dt=0.009s, elapsed=2.6m\n",
      "[Net2] Ep 15800: loss=0.100157, pde=0.034100, bc=0.066057, λ=1.000, dt=0.009s, elapsed=2.7m\n",
      "[Net2] Ep 16000: loss=0.101112, pde=0.035335, bc=0.065777, λ=1.000, dt=0.009s, elapsed=2.7m\n",
      "[Net2] Ep 16200: loss=0.098604, pde=0.036822, bc=0.061782, λ=1.000, dt=0.009s, elapsed=2.7m\n",
      "[Net2] Ep 16400: loss=0.097295, pde=0.035036, bc=0.062259, λ=1.000, dt=0.012s, elapsed=2.8m\n",
      "[Net2] Ep 16600: loss=0.098695, pde=0.033579, bc=0.065116, λ=1.000, dt=0.010s, elapsed=2.8m\n",
      "[Net2] Ep 16800: loss=0.091047, pde=0.030767, bc=0.060280, λ=1.000, dt=0.010s, elapsed=2.8m\n",
      "[Net2] Ep 17000: loss=0.100410, pde=0.036063, bc=0.064347, λ=1.000, dt=0.009s, elapsed=2.9m\n",
      "[Net2] Ep 17200: loss=0.095176, pde=0.032303, bc=0.062873, λ=1.000, dt=0.009s, elapsed=2.9m\n",
      "[Net2] Ep 17400: loss=0.099508, pde=0.032629, bc=0.066879, λ=1.000, dt=0.009s, elapsed=2.9m\n",
      "[Net2] Ep 17600: loss=0.097313, pde=0.035724, bc=0.061590, λ=1.000, dt=0.012s, elapsed=3.0m\n",
      "[Net2] Ep 17800: loss=0.093230, pde=0.031794, bc=0.061436, λ=1.000, dt=0.009s, elapsed=3.0m\n",
      "[Net2] Ep 18000: loss=0.096702, pde=0.035051, bc=0.061651, λ=1.000, dt=0.009s, elapsed=3.0m\n",
      "[Net2] Ep 18200: loss=0.094939, pde=0.033118, bc=0.061820, λ=1.000, dt=0.009s, elapsed=3.1m\n",
      "[Net2] Ep 18400: loss=0.092481, pde=0.032578, bc=0.059903, λ=1.000, dt=0.010s, elapsed=3.1m\n",
      "[Net2] Ep 18600: loss=0.093056, pde=0.033989, bc=0.059067, λ=1.000, dt=0.009s, elapsed=3.1m\n",
      "[Net2] Ep 18800: loss=0.099146, pde=0.035258, bc=0.063888, λ=1.000, dt=0.012s, elapsed=3.2m\n",
      "[Net2] Ep 19000: loss=0.089094, pde=0.030684, bc=0.058410, λ=1.000, dt=0.009s, elapsed=3.2m\n",
      "[Net2] Ep 19200: loss=0.093328, pde=0.037265, bc=0.056063, λ=1.000, dt=0.009s, elapsed=3.2m\n",
      "[Net2] Ep 19400: loss=0.090300, pde=0.029444, bc=0.060856, λ=1.000, dt=0.009s, elapsed=3.3m\n",
      "[Net2] Ep 19600: loss=0.085693, pde=0.028552, bc=0.057140, λ=1.000, dt=0.010s, elapsed=3.3m\n",
      "[Net2] Ep 19800: loss=0.095523, pde=0.031633, bc=0.063890, λ=1.000, dt=0.009s, elapsed=3.3m\n",
      "[Net2] Ep 20000: loss=0.090919, pde=0.031040, bc=0.059879, λ=1.000, dt=0.011s, elapsed=3.4m\n",
      "Net2 training complete in 3.4 min (final loss 0.090919)\n",
      "[Net3] Ep 1: loss=2596.989014, pde=2596.969971, bc=0.019121, λ=1.000, dt=0.017s, elapsed=0.0m\n",
      "[Net3] Ep 200: loss=77.163589, pde=62.667686, bc=14.495903, λ=1.000, dt=0.010s, elapsed=0.0m\n",
      "[Net3] Ep 400: loss=7.266944, pde=2.115814, bc=5.151130, λ=1.000, dt=0.010s, elapsed=0.1m\n",
      "[Net3] Ep 600: loss=3.591593, pde=0.968484, bc=2.623109, λ=1.000, dt=0.009s, elapsed=0.1m\n",
      "[Net3] Ep 800: loss=2.410054, pde=0.606312, bc=1.803742, λ=1.000, dt=0.009s, elapsed=0.1m\n",
      "[Net3] Ep 1000: loss=2.080657, pde=0.500960, bc=1.579697, λ=1.000, dt=0.009s, elapsed=0.2m\n",
      "[Net3] Ep 1200: loss=1.726181, pde=0.278097, bc=1.448084, λ=1.000, dt=0.012s, elapsed=0.2m\n",
      "[Net3] Ep 1400: loss=1.569161, pde=0.234555, bc=1.334606, λ=1.000, dt=0.010s, elapsed=0.2m\n",
      "[Net3] Ep 1600: loss=1.581936, pde=0.277247, bc=1.304689, λ=1.000, dt=0.009s, elapsed=0.3m\n",
      "[Net3] Ep 1800: loss=1.662633, pde=0.456873, bc=1.205760, λ=1.000, dt=0.010s, elapsed=0.3m\n",
      "[Net3] Ep 2000: loss=1.687993, pde=0.468473, bc=1.219520, λ=1.000, dt=0.009s, elapsed=0.3m\n",
      "[Net3] Ep 2200: loss=1.448169, pde=0.331748, bc=1.116420, λ=1.000, dt=0.009s, elapsed=0.4m\n",
      "[Net3] Ep 2400: loss=1.431565, pde=0.311696, bc=1.119869, λ=1.000, dt=0.017s, elapsed=0.4m\n",
      "[Net3] Ep 2600: loss=1.796320, pde=0.739947, bc=1.056373, λ=1.000, dt=0.009s, elapsed=0.4m\n",
      "[Net3] Ep 2800: loss=1.428169, pde=0.393111, bc=1.035058, λ=1.000, dt=0.010s, elapsed=0.5m\n",
      "[Net3] Ep 3000: loss=1.617817, pde=0.666475, bc=0.951342, λ=1.000, dt=0.010s, elapsed=0.5m\n",
      "[Net3] Ep 3200: loss=1.044761, pde=0.142478, bc=0.902283, λ=1.000, dt=0.009s, elapsed=0.5m\n",
      "[Net3] Ep 3400: loss=1.033015, pde=0.121308, bc=0.911707, λ=1.000, dt=0.010s, elapsed=0.6m\n",
      "[Net3] Ep 3600: loss=1.182314, pde=0.248328, bc=0.933986, λ=1.000, dt=0.016s, elapsed=0.6m\n",
      "[Net3] Ep 3800: loss=1.005098, pde=0.214758, bc=0.790340, λ=1.000, dt=0.009s, elapsed=0.6m\n",
      "[Net3] Ep 4000: loss=0.988169, pde=0.209867, bc=0.778301, λ=1.000, dt=0.010s, elapsed=0.7m\n",
      "[Net3] Ep 4200: loss=0.908229, pde=0.235037, bc=0.673192, λ=1.000, dt=0.009s, elapsed=0.7m\n",
      "[Net3] Ep 4400: loss=0.888895, pde=0.195166, bc=0.693729, λ=1.000, dt=0.009s, elapsed=0.7m\n",
      "[Net3] Ep 4600: loss=0.932712, pde=0.294976, bc=0.637736, λ=1.000, dt=0.009s, elapsed=0.8m\n",
      "[Net3] Ep 4800: loss=0.757234, pde=0.172563, bc=0.584670, λ=1.000, dt=0.010s, elapsed=0.8m\n",
      "[Net3] Ep 5000: loss=0.763996, pde=0.168728, bc=0.595268, λ=1.000, dt=0.010s, elapsed=0.9m\n",
      "[Net3] Ep 5200: loss=0.849715, pde=0.325043, bc=0.524671, λ=1.000, dt=0.009s, elapsed=0.9m\n",
      "[Net3] Ep 5400: loss=0.627938, pde=0.126128, bc=0.501810, λ=1.000, dt=0.010s, elapsed=0.9m\n",
      "[Net3] Ep 5600: loss=0.776957, pde=0.296093, bc=0.480864, λ=1.000, dt=0.010s, elapsed=0.9m\n",
      "[Net3] Ep 5800: loss=0.571305, pde=0.133471, bc=0.437834, λ=1.000, dt=0.011s, elapsed=1.0m\n",
      "[Net3] Ep 6000: loss=0.502972, pde=0.106939, bc=0.396033, λ=1.000, dt=0.009s, elapsed=1.0m\n",
      "[Net3] Ep 6200: loss=0.509366, pde=0.142650, bc=0.366716, λ=1.000, dt=0.009s, elapsed=1.1m\n",
      "[Net3] Ep 6400: loss=0.533850, pde=0.190647, bc=0.343203, λ=1.000, dt=0.012s, elapsed=1.1m\n",
      "[Net3] Ep 6600: loss=0.504244, pde=0.196117, bc=0.308127, λ=1.000, dt=0.009s, elapsed=1.1m\n",
      "[Net3] Ep 6800: loss=0.412262, pde=0.101473, bc=0.310789, λ=1.000, dt=0.011s, elapsed=1.2m\n",
      "[Net3] Ep 7000: loss=0.352688, pde=0.079724, bc=0.272964, λ=1.000, dt=0.012s, elapsed=1.2m\n",
      "[Net3] Ep 7200: loss=0.345949, pde=0.078348, bc=0.267601, λ=1.000, dt=0.010s, elapsed=1.2m\n",
      "[Net3] Ep 7400: loss=0.373030, pde=0.111233, bc=0.261797, λ=1.000, dt=0.011s, elapsed=1.3m\n",
      "[Net3] Ep 7600: loss=0.564320, pde=0.340413, bc=0.223907, λ=1.000, dt=0.010s, elapsed=1.3m\n",
      "[Net3] Ep 7800: loss=0.261620, pde=0.071951, bc=0.189669, λ=1.000, dt=0.010s, elapsed=1.3m\n",
      "[Net3] Ep 8000: loss=0.260724, pde=0.073036, bc=0.187688, λ=1.000, dt=0.009s, elapsed=1.4m\n",
      "[Net3] Ep 8200: loss=0.266008, pde=0.099363, bc=0.166645, λ=1.000, dt=0.012s, elapsed=1.4m\n",
      "[Net3] Ep 8400: loss=0.239844, pde=0.058982, bc=0.180862, λ=1.000, dt=0.010s, elapsed=1.4m\n",
      "[Net3] Ep 8600: loss=0.221625, pde=0.062854, bc=0.158772, λ=1.000, dt=0.009s, elapsed=1.5m\n",
      "[Net3] Ep 8800: loss=0.373870, pde=0.230207, bc=0.143664, λ=1.000, dt=0.010s, elapsed=1.5m\n",
      "[Net3] Ep 9000: loss=0.287157, pde=0.150683, bc=0.136474, λ=1.000, dt=0.010s, elapsed=1.5m\n",
      "[Net3] Ep 9200: loss=0.201066, pde=0.075001, bc=0.126065, λ=1.000, dt=0.009s, elapsed=1.6m\n",
      "[Net3] Ep 9400: loss=0.183918, pde=0.066316, bc=0.117602, λ=1.000, dt=0.011s, elapsed=1.6m\n",
      "[Net3] Ep 9600: loss=0.162332, pde=0.047748, bc=0.114584, λ=1.000, dt=0.009s, elapsed=1.6m\n",
      "[Net3] Ep 9800: loss=0.179064, pde=0.081600, bc=0.097464, λ=1.000, dt=0.009s, elapsed=1.7m\n",
      "[Net3] Ep 10000: loss=0.159383, pde=0.052067, bc=0.107316, λ=1.000, dt=0.010s, elapsed=1.7m\n",
      "[Net3] Ep 10200: loss=0.134392, pde=0.041138, bc=0.093253, λ=1.000, dt=0.009s, elapsed=1.7m\n",
      "[Net3] Ep 10400: loss=0.138996, pde=0.053357, bc=0.085639, λ=1.000, dt=0.010s, elapsed=1.8m\n",
      "[Net3] Ep 10600: loss=0.179719, pde=0.087084, bc=0.092635, λ=1.000, dt=0.011s, elapsed=1.8m\n",
      "[Net3] Ep 10800: loss=0.112510, pde=0.032544, bc=0.079966, λ=1.000, dt=0.010s, elapsed=1.8m\n",
      "[Net3] Ep 11000: loss=0.138582, pde=0.053588, bc=0.084994, λ=1.000, dt=0.009s, elapsed=1.9m\n",
      "[Net3] Ep 11200: loss=0.120206, pde=0.035442, bc=0.084765, λ=1.000, dt=0.015s, elapsed=1.9m\n",
      "[Net3] Ep 11400: loss=0.111865, pde=0.036857, bc=0.075008, λ=1.000, dt=0.010s, elapsed=1.9m\n",
      "[Net3] Ep 11600: loss=0.116926, pde=0.043840, bc=0.073085, λ=1.000, dt=0.009s, elapsed=2.0m\n",
      "[Net3] Ep 11800: loss=0.101651, pde=0.033164, bc=0.068486, λ=1.000, dt=0.010s, elapsed=2.0m\n",
      "[Net3] Ep 12000: loss=0.101603, pde=0.033619, bc=0.067984, λ=1.000, dt=0.009s, elapsed=2.1m\n",
      "[Net3] Ep 12200: loss=0.098451, pde=0.033832, bc=0.064619, λ=1.000, dt=0.009s, elapsed=2.1m\n",
      "[Net3] Ep 12400: loss=0.091757, pde=0.029104, bc=0.062653, λ=1.000, dt=0.012s, elapsed=2.1m\n",
      "[Net3] Ep 12600: loss=0.099125, pde=0.035506, bc=0.063619, λ=1.000, dt=0.009s, elapsed=2.2m\n",
      "[Net3] Ep 12800: loss=0.090109, pde=0.031398, bc=0.058711, λ=1.000, dt=0.013s, elapsed=2.2m\n",
      "[Net3] Ep 13000: loss=0.082743, pde=0.025969, bc=0.056773, λ=1.000, dt=0.011s, elapsed=2.2m\n",
      "[Net3] Ep 13200: loss=0.084511, pde=0.027152, bc=0.057359, λ=1.000, dt=0.009s, elapsed=2.3m\n",
      "[Net3] Ep 13400: loss=0.081060, pde=0.026140, bc=0.054920, λ=1.000, dt=0.010s, elapsed=2.3m\n",
      "[Net3] Ep 13600: loss=0.081869, pde=0.028764, bc=0.053105, λ=1.000, dt=0.009s, elapsed=2.3m\n",
      "[Net3] Ep 13800: loss=0.079697, pde=0.027796, bc=0.051901, λ=1.000, dt=0.010s, elapsed=2.4m\n",
      "[Net3] Ep 14000: loss=0.075633, pde=0.026405, bc=0.049228, λ=1.000, dt=0.012s, elapsed=2.4m\n",
      "[Net3] Ep 14200: loss=0.075254, pde=0.025310, bc=0.049944, λ=1.000, dt=0.010s, elapsed=2.4m\n",
      "[Net3] Ep 14400: loss=0.067983, pde=0.022474, bc=0.045509, λ=1.000, dt=0.010s, elapsed=2.5m\n",
      "[Net3] Ep 14600: loss=0.070205, pde=0.022429, bc=0.047776, λ=1.000, dt=0.009s, elapsed=2.5m\n",
      "[Net3] Ep 14800: loss=0.070388, pde=0.024561, bc=0.045827, λ=1.000, dt=0.010s, elapsed=2.5m\n",
      "[Net3] Ep 15000: loss=0.071276, pde=0.024280, bc=0.046996, λ=1.000, dt=0.011s, elapsed=2.6m\n",
      "[Net3] Ep 15200: loss=0.063385, pde=0.021551, bc=0.041834, λ=1.000, dt=0.012s, elapsed=2.6m\n",
      "[Net3] Ep 15400: loss=0.062977, pde=0.020297, bc=0.042681, λ=1.000, dt=0.010s, elapsed=2.6m\n",
      "[Net3] Ep 15600: loss=0.062156, pde=0.021766, bc=0.040390, λ=1.000, dt=0.010s, elapsed=2.7m\n",
      "[Net3] Ep 15800: loss=0.061552, pde=0.020163, bc=0.041389, λ=1.000, dt=0.009s, elapsed=2.7m\n",
      "[Net3] Ep 16000: loss=0.060423, pde=0.020404, bc=0.040019, λ=1.000, dt=0.010s, elapsed=2.7m\n",
      "[Net3] Ep 16200: loss=0.056500, pde=0.018100, bc=0.038400, λ=1.000, dt=0.014s, elapsed=2.8m\n",
      "[Net3] Ep 16400: loss=0.058236, pde=0.019569, bc=0.038667, λ=1.000, dt=0.011s, elapsed=2.8m\n",
      "[Net3] Ep 16600: loss=0.059924, pde=0.020404, bc=0.039520, λ=1.000, dt=0.009s, elapsed=2.8m\n",
      "[Net3] Ep 16800: loss=0.054556, pde=0.017530, bc=0.037026, λ=1.000, dt=0.010s, elapsed=2.9m\n",
      "[Net3] Ep 17000: loss=0.059138, pde=0.020982, bc=0.038156, λ=1.000, dt=0.010s, elapsed=2.9m\n",
      "[Net3] Ep 17200: loss=0.054185, pde=0.016363, bc=0.037822, λ=1.000, dt=0.010s, elapsed=2.9m\n",
      "[Net3] Ep 17400: loss=0.056519, pde=0.017195, bc=0.039325, λ=1.000, dt=0.015s, elapsed=3.0m\n",
      "[Net3] Ep 17600: loss=0.053799, pde=0.016995, bc=0.036804, λ=1.000, dt=0.013s, elapsed=3.0m\n",
      "[Net3] Ep 17800: loss=0.054448, pde=0.017783, bc=0.036665, λ=1.000, dt=0.010s, elapsed=3.0m\n",
      "[Net3] Ep 18000: loss=0.052053, pde=0.016188, bc=0.035865, λ=1.000, dt=0.010s, elapsed=3.1m\n",
      "[Net3] Ep 18200: loss=0.054572, pde=0.017529, bc=0.037043, λ=1.000, dt=0.010s, elapsed=3.1m\n",
      "[Net3] Ep 18400: loss=0.051247, pde=0.015204, bc=0.036043, λ=1.000, dt=0.010s, elapsed=3.1m\n",
      "[Net3] Ep 18600: loss=0.051538, pde=0.016852, bc=0.034686, λ=1.000, dt=0.012s, elapsed=3.2m\n",
      "[Net3] Ep 18800: loss=0.052286, pde=0.014735, bc=0.037551, λ=1.000, dt=0.011s, elapsed=3.2m\n",
      "[Net3] Ep 19000: loss=0.051391, pde=0.016475, bc=0.034916, λ=1.000, dt=0.010s, elapsed=3.3m\n",
      "[Net3] Ep 19200: loss=0.053046, pde=0.019026, bc=0.034020, λ=1.000, dt=0.012s, elapsed=3.3m\n",
      "[Net3] Ep 19400: loss=0.051036, pde=0.015261, bc=0.035775, λ=1.000, dt=0.010s, elapsed=3.3m\n",
      "[Net3] Ep 19600: loss=0.045624, pde=0.012774, bc=0.032850, λ=1.000, dt=0.013s, elapsed=3.4m\n",
      "[Net3] Ep 19800: loss=0.050596, pde=0.014344, bc=0.036252, λ=1.000, dt=0.009s, elapsed=3.4m\n",
      "[Net3] Ep 20000: loss=0.049315, pde=0.014672, bc=0.034643, λ=1.000, dt=0.010s, elapsed=3.4m\n",
      "Net3 training complete in 3.4 min (final loss 0.049315)\n",
      "[Net4] Ep 1: loss=2596.919678, pde=2596.884277, bc=0.035494, λ=1.000, dt=0.013s, elapsed=0.0m\n",
      "[Net4] Ep 200: loss=61.667309, pde=52.369896, bc=9.297413, λ=1.000, dt=0.010s, elapsed=0.0m\n",
      "[Net4] Ep 400: loss=10.915008, pde=4.359645, bc=6.555362, λ=1.000, dt=0.009s, elapsed=0.1m\n",
      "[Net4] Ep 600: loss=5.996663, pde=1.547243, bc=4.449419, λ=1.000, dt=0.010s, elapsed=0.1m\n",
      "[Net4] Ep 800: loss=3.886026, pde=0.996417, bc=2.889609, λ=1.000, dt=0.009s, elapsed=0.1m\n",
      "[Net4] Ep 1000: loss=2.613549, pde=0.691817, bc=1.921731, λ=1.000, dt=0.011s, elapsed=0.2m\n",
      "[Net4] Ep 1200: loss=1.852948, pde=0.453012, bc=1.399936, λ=1.000, dt=0.010s, elapsed=0.2m\n",
      "[Net4] Ep 1400: loss=1.646418, pde=0.453272, bc=1.193145, λ=1.000, dt=0.010s, elapsed=0.2m\n",
      "[Net4] Ep 1600: loss=1.330397, pde=0.211733, bc=1.118663, λ=1.000, dt=0.010s, elapsed=0.3m\n",
      "[Net4] Ep 1800: loss=1.640565, pde=0.637007, bc=1.003558, λ=1.000, dt=0.009s, elapsed=0.3m\n",
      "[Net4] Ep 2000: loss=1.144444, pde=0.151524, bc=0.992920, λ=1.000, dt=0.010s, elapsed=0.3m\n",
      "[Net4] Ep 2200: loss=1.187369, pde=0.310968, bc=0.876401, λ=1.000, dt=0.012s, elapsed=0.4m\n",
      "[Net4] Ep 2400: loss=1.032055, pde=0.188062, bc=0.843994, λ=1.000, dt=0.015s, elapsed=0.4m\n",
      "[Net4] Ep 2600: loss=1.459105, pde=0.684909, bc=0.774196, λ=1.000, dt=0.010s, elapsed=0.4m\n",
      "[Net4] Ep 2800: loss=0.967615, pde=0.239368, bc=0.728247, λ=1.000, dt=0.009s, elapsed=0.5m\n",
      "[Net4] Ep 3000: loss=0.852983, pde=0.204854, bc=0.648129, λ=1.000, dt=0.009s, elapsed=0.5m\n",
      "[Net4] Ep 3200: loss=0.767242, pde=0.183721, bc=0.583521, λ=1.000, dt=0.010s, elapsed=0.5m\n",
      "[Net4] Ep 3400: loss=0.860848, pde=0.296663, bc=0.564185, λ=1.000, dt=0.012s, elapsed=0.6m\n",
      "[Net4] Ep 3600: loss=0.715467, pde=0.170575, bc=0.544892, λ=1.000, dt=0.010s, elapsed=0.6m\n",
      "[Net4] Ep 3800: loss=0.681896, pde=0.227572, bc=0.454324, λ=1.000, dt=0.009s, elapsed=0.6m\n",
      "[Net4] Ep 4000: loss=0.567487, pde=0.151876, bc=0.415611, λ=1.000, dt=0.009s, elapsed=0.7m\n",
      "[Net4] Ep 4200: loss=0.464843, pde=0.122986, bc=0.341857, λ=1.000, dt=0.010s, elapsed=0.7m\n",
      "[Net4] Ep 4400: loss=0.530346, pde=0.195256, bc=0.335089, λ=1.000, dt=0.010s, elapsed=0.7m\n",
      "[Net4] Ep 4600: loss=0.433924, pde=0.140745, bc=0.293179, λ=1.000, dt=0.015s, elapsed=0.8m\n",
      "[Net4] Ep 4800: loss=0.378747, pde=0.122634, bc=0.256113, λ=1.000, dt=0.011s, elapsed=0.8m\n",
      "[Net4] Ep 5000: loss=0.374215, pde=0.123609, bc=0.250607, λ=1.000, dt=0.010s, elapsed=0.9m\n",
      "[Net4] Ep 5200: loss=0.394996, pde=0.180646, bc=0.214349, λ=1.000, dt=0.010s, elapsed=0.9m\n",
      "[Net4] Ep 5400: loss=0.645951, pde=0.449086, bc=0.196865, λ=1.000, dt=0.010s, elapsed=0.9m\n",
      "[Net4] Ep 5600: loss=0.381565, pde=0.196764, bc=0.184800, λ=1.000, dt=0.011s, elapsed=1.0m\n",
      "[Net4] Ep 5800: loss=0.604982, pde=0.443305, bc=0.161676, λ=1.000, dt=0.014s, elapsed=1.0m\n",
      "[Net4] Ep 6000: loss=0.234918, pde=0.094881, bc=0.140037, λ=1.000, dt=0.010s, elapsed=1.0m\n",
      "[Net4] Ep 6200: loss=0.358373, pde=0.230346, bc=0.128027, λ=1.000, dt=0.009s, elapsed=1.1m\n",
      "[Net4] Ep 6400: loss=0.216615, pde=0.101374, bc=0.115241, λ=1.000, dt=0.009s, elapsed=1.1m\n",
      "[Net4] Ep 6600: loss=0.187683, pde=0.087831, bc=0.099852, λ=1.000, dt=0.009s, elapsed=1.1m\n",
      "[Net4] Ep 6800: loss=0.217669, pde=0.120639, bc=0.097031, λ=1.000, dt=0.010s, elapsed=1.2m\n",
      "[Net4] Ep 7000: loss=0.167662, pde=0.087602, bc=0.080060, λ=1.000, dt=0.010s, elapsed=1.2m\n",
      "[Net4] Ep 7200: loss=0.167055, pde=0.092900, bc=0.074155, λ=1.000, dt=0.010s, elapsed=1.2m\n",
      "[Net4] Ep 7400: loss=0.128610, pde=0.061495, bc=0.067114, λ=1.000, dt=0.009s, elapsed=1.3m\n",
      "[Net4] Ep 7600: loss=0.282593, pde=0.221854, bc=0.060739, λ=1.000, dt=0.011s, elapsed=1.3m\n",
      "[Net4] Ep 7800: loss=0.106844, pde=0.056334, bc=0.050510, λ=1.000, dt=0.010s, elapsed=1.3m\n",
      "[Net4] Ep 8000: loss=0.101544, pde=0.056627, bc=0.044916, λ=1.000, dt=0.013s, elapsed=1.4m\n",
      "[Net4] Ep 8200: loss=0.098508, pde=0.057539, bc=0.040969, λ=1.000, dt=0.010s, elapsed=1.4m\n",
      "[Net4] Ep 8400: loss=0.100719, pde=0.057169, bc=0.043551, λ=1.000, dt=0.010s, elapsed=1.4m\n",
      "[Net4] Ep 8600: loss=0.125903, pde=0.088433, bc=0.037470, λ=1.000, dt=0.010s, elapsed=1.5m\n",
      "[Net4] Ep 8800: loss=0.077719, pde=0.043890, bc=0.033829, λ=1.000, dt=0.010s, elapsed=1.5m\n",
      "[Net4] Ep 9000: loss=0.076905, pde=0.045404, bc=0.031500, λ=1.000, dt=0.010s, elapsed=1.5m\n",
      "[Net4] Ep 9200: loss=0.066682, pde=0.037850, bc=0.028832, λ=1.000, dt=0.012s, elapsed=1.6m\n",
      "[Net4] Ep 9400: loss=0.083288, pde=0.054545, bc=0.028744, λ=1.000, dt=0.010s, elapsed=1.6m\n",
      "[Net4] Ep 9600: loss=0.077913, pde=0.050301, bc=0.027612, λ=1.000, dt=0.010s, elapsed=1.6m\n",
      "[Net4] Ep 9800: loss=0.103789, pde=0.079878, bc=0.023911, λ=1.000, dt=0.009s, elapsed=1.7m\n",
      "[Net4] Ep 10000: loss=0.070041, pde=0.044258, bc=0.025783, λ=1.000, dt=0.010s, elapsed=1.7m\n",
      "[Net4] Ep 10200: loss=0.083301, pde=0.060917, bc=0.022384, λ=1.000, dt=0.009s, elapsed=1.7m\n",
      "[Net4] Ep 10400: loss=0.067639, pde=0.046498, bc=0.021140, λ=1.000, dt=0.011s, elapsed=1.8m\n",
      "[Net4] Ep 10600: loss=0.059334, pde=0.036675, bc=0.022660, λ=1.000, dt=0.009s, elapsed=1.8m\n",
      "[Net4] Ep 10800: loss=0.043123, pde=0.023813, bc=0.019309, λ=1.000, dt=0.011s, elapsed=1.8m\n",
      "[Net4] Ep 11000: loss=0.056814, pde=0.035250, bc=0.021564, λ=1.000, dt=0.010s, elapsed=1.9m\n",
      "[Net4] Ep 11200: loss=0.047598, pde=0.025904, bc=0.021694, λ=1.000, dt=0.010s, elapsed=1.9m\n",
      "[Net4] Ep 11400: loss=0.043239, pde=0.024040, bc=0.019199, λ=1.000, dt=0.009s, elapsed=1.9m\n",
      "[Net4] Ep 11600: loss=0.047783, pde=0.028947, bc=0.018836, λ=1.000, dt=0.016s, elapsed=2.0m\n",
      "[Net4] Ep 11800: loss=0.061249, pde=0.042861, bc=0.018388, λ=1.000, dt=0.010s, elapsed=2.0m\n",
      "[Net4] Ep 12000: loss=0.038533, pde=0.020801, bc=0.017732, λ=1.000, dt=0.010s, elapsed=2.1m\n",
      "[Net4] Ep 12200: loss=0.051011, pde=0.033951, bc=0.017060, λ=1.000, dt=0.011s, elapsed=2.1m\n",
      "[Net4] Ep 12400: loss=0.036249, pde=0.019508, bc=0.016741, λ=1.000, dt=0.011s, elapsed=2.1m\n",
      "[Net4] Ep 12600: loss=0.040181, pde=0.023265, bc=0.016916, λ=1.000, dt=0.010s, elapsed=2.2m\n",
      "[Net4] Ep 12800: loss=0.033645, pde=0.016580, bc=0.017065, λ=1.000, dt=0.015s, elapsed=2.2m\n",
      "[Net4] Ep 13000: loss=0.040219, pde=0.024064, bc=0.016155, λ=1.000, dt=0.009s, elapsed=2.2m\n",
      "[Net4] Ep 13200: loss=0.041015, pde=0.024225, bc=0.016790, λ=1.000, dt=0.010s, elapsed=2.3m\n",
      "[Net4] Ep 13400: loss=0.035706, pde=0.020407, bc=0.015299, λ=1.000, dt=0.010s, elapsed=2.3m\n",
      "[Net4] Ep 13600: loss=0.039788, pde=0.024280, bc=0.015508, λ=1.000, dt=0.009s, elapsed=2.3m\n",
      "[Net4] Ep 13800: loss=0.032115, pde=0.016863, bc=0.015251, λ=1.000, dt=0.010s, elapsed=2.4m\n",
      "[Net4] Ep 14000: loss=0.034109, pde=0.019624, bc=0.014485, λ=1.000, dt=0.010s, elapsed=2.4m\n",
      "[Net4] Ep 14200: loss=0.032438, pde=0.017372, bc=0.015066, λ=1.000, dt=0.010s, elapsed=2.4m\n",
      "[Net4] Ep 14400: loss=0.027353, pde=0.013432, bc=0.013921, λ=1.000, dt=0.010s, elapsed=2.5m\n",
      "[Net4] Ep 14600: loss=0.030369, pde=0.015785, bc=0.014584, λ=1.000, dt=0.009s, elapsed=2.5m\n",
      "[Net4] Ep 14800: loss=0.030700, pde=0.016759, bc=0.013941, λ=1.000, dt=0.009s, elapsed=2.5m\n",
      "[Net4] Ep 15000: loss=0.030799, pde=0.016438, bc=0.014362, λ=1.000, dt=0.014s, elapsed=2.6m\n",
      "[Net4] Ep 15200: loss=0.028969, pde=0.015583, bc=0.013386, λ=1.000, dt=0.009s, elapsed=2.6m\n",
      "[Net4] Ep 15400: loss=0.027428, pde=0.014041, bc=0.013387, λ=1.000, dt=0.010s, elapsed=2.6m\n",
      "[Net4] Ep 15600: loss=0.027052, pde=0.014186, bc=0.012865, λ=1.000, dt=0.012s, elapsed=2.7m\n",
      "[Net4] Ep 15800: loss=0.026596, pde=0.013779, bc=0.012817, λ=1.000, dt=0.010s, elapsed=2.7m\n",
      "[Net4] Ep 16000: loss=0.024199, pde=0.011190, bc=0.013009, λ=1.000, dt=0.010s, elapsed=2.7m\n",
      "[Net4] Ep 16200: loss=0.024802, pde=0.012367, bc=0.012436, λ=1.000, dt=0.012s, elapsed=2.8m\n",
      "[Net4] Ep 16400: loss=0.024718, pde=0.011778, bc=0.012940, λ=1.000, dt=0.010s, elapsed=2.8m\n",
      "[Net4] Ep 16600: loss=0.028578, pde=0.015229, bc=0.013349, λ=1.000, dt=0.011s, elapsed=2.8m\n",
      "[Net4] Ep 16800: loss=0.023367, pde=0.010987, bc=0.012381, λ=1.000, dt=0.016s, elapsed=2.9m\n",
      "[Net4] Ep 17000: loss=0.025450, pde=0.013232, bc=0.012217, λ=1.000, dt=0.010s, elapsed=2.9m\n",
      "[Net4] Ep 17200: loss=0.024471, pde=0.011950, bc=0.012520, λ=1.000, dt=0.009s, elapsed=2.9m\n",
      "[Net4] Ep 17400: loss=0.024527, pde=0.011691, bc=0.012836, λ=1.000, dt=0.014s, elapsed=3.0m\n",
      "[Net4] Ep 17600: loss=0.023494, pde=0.010717, bc=0.012777, λ=1.000, dt=0.011s, elapsed=3.0m\n",
      "[Net4] Ep 17800: loss=0.025417, pde=0.013018, bc=0.012398, λ=1.000, dt=0.010s, elapsed=3.1m\n",
      "[Net4] Ep 18000: loss=0.023195, pde=0.011580, bc=0.011615, λ=1.000, dt=0.010s, elapsed=3.1m\n",
      "[Net4] Ep 18200: loss=0.026152, pde=0.013488, bc=0.012664, λ=1.000, dt=0.009s, elapsed=3.1m\n",
      "[Net4] Ep 18400: loss=0.022639, pde=0.010528, bc=0.012111, λ=1.000, dt=0.010s, elapsed=3.2m\n",
      "[Net4] Ep 18600: loss=0.021649, pde=0.009976, bc=0.011673, λ=1.000, dt=0.010s, elapsed=3.2m\n",
      "[Net4] Ep 18800: loss=0.022181, pde=0.009341, bc=0.012840, λ=1.000, dt=0.010s, elapsed=3.2m\n",
      "[Net4] Ep 19000: loss=0.021782, pde=0.009942, bc=0.011840, λ=1.000, dt=0.010s, elapsed=3.3m\n",
      "[Net4] Ep 19200: loss=0.023156, pde=0.011631, bc=0.011525, λ=1.000, dt=0.010s, elapsed=3.3m\n",
      "[Net4] Ep 19400: loss=0.021785, pde=0.009067, bc=0.012717, λ=1.000, dt=0.010s, elapsed=3.3m\n",
      "[Net4] Ep 19600: loss=0.023011, pde=0.011536, bc=0.011475, λ=1.000, dt=0.013s, elapsed=3.4m\n",
      "[Net4] Ep 19800: loss=0.023487, pde=0.010994, bc=0.012493, λ=1.000, dt=0.010s, elapsed=3.4m\n",
      "[Net4] Ep 20000: loss=0.022318, pde=0.010565, bc=0.011753, λ=1.000, dt=0.009s, elapsed=3.4m\n",
      "Net4 training complete in 3.4 min (final loss 0.022318)\n",
      "[Net5] Ep 1: loss=2596.912598, pde=2596.761719, bc=0.150845, λ=1.000, dt=0.043s, elapsed=0.0m\n",
      "[Net5] Ep 200: loss=2601.387695, pde=2601.380615, bc=0.006967, λ=1.000, dt=0.012s, elapsed=0.0m\n",
      "[Net5] Ep 400: loss=2442.703613, pde=2442.703613, bc=0.000007, λ=1.000, dt=0.012s, elapsed=0.1m\n",
      "[Net5] Ep 600: loss=2491.281006, pde=2491.280762, bc=0.000126, λ=1.000, dt=0.012s, elapsed=0.1m\n",
      "[Net5] Ep 800: loss=2559.111816, pde=2559.111572, bc=0.000142, λ=1.000, dt=0.012s, elapsed=0.2m\n",
      "[Net5] Ep 1000: loss=2585.236816, pde=2585.236328, bc=0.000518, λ=1.000, dt=0.012s, elapsed=0.2m\n",
      "[Net5] Ep 1200: loss=2505.959717, pde=2505.959717, bc=0.000073, λ=1.000, dt=0.011s, elapsed=0.2m\n",
      "[Net5] Ep 1400: loss=2601.298828, pde=2601.298340, bc=0.000547, λ=1.000, dt=0.011s, elapsed=0.3m\n",
      "[Net5] Ep 1600: loss=2566.551270, pde=2566.550537, bc=0.000703, λ=1.000, dt=0.011s, elapsed=0.3m\n",
      "[Net5] Ep 1800: loss=2502.737305, pde=2502.737061, bc=0.000232, λ=1.000, dt=0.011s, elapsed=0.4m\n",
      "[Net5] Ep 2000: loss=2593.586914, pde=2593.584229, bc=0.002679, λ=1.000, dt=0.017s, elapsed=0.4m\n",
      "[Net5] Ep 2200: loss=2531.259521, pde=2531.258301, bc=0.001169, λ=1.000, dt=0.011s, elapsed=0.4m\n",
      "[Net5] Ep 2400: loss=2606.760254, pde=2606.760010, bc=0.000126, λ=1.000, dt=0.011s, elapsed=0.5m\n",
      "[Net5] Ep 2600: loss=2401.879150, pde=2401.878418, bc=0.000703, λ=1.000, dt=0.018s, elapsed=0.5m\n",
      "[Net5] Ep 2800: loss=2599.618896, pde=2599.618896, bc=0.000118, λ=1.000, dt=0.012s, elapsed=0.6m\n",
      "[Net5] Ep 3000: loss=2524.693359, pde=2524.693115, bc=0.000242, λ=1.000, dt=0.011s, elapsed=0.6m\n",
      "[Net5] Ep 3200: loss=2631.811035, pde=2631.811035, bc=0.000084, λ=1.000, dt=0.013s, elapsed=0.6m\n",
      "[Net5] Ep 3400: loss=2536.284424, pde=2536.284424, bc=0.000002, λ=1.000, dt=0.014s, elapsed=0.7m\n",
      "[Net5] Ep 3600: loss=2449.732666, pde=2449.732666, bc=0.000043, λ=1.000, dt=0.015s, elapsed=0.7m\n",
      "[Net5] Ep 3800: loss=2392.967285, pde=2392.967041, bc=0.000272, λ=1.000, dt=0.011s, elapsed=0.8m\n",
      "[Net5] Ep 4000: loss=2441.361328, pde=2441.358398, bc=0.002834, λ=1.000, dt=0.011s, elapsed=0.8m\n",
      "[Net5] Ep 4200: loss=2604.913818, pde=2604.913818, bc=0.000121, λ=1.000, dt=0.011s, elapsed=0.9m\n",
      "[Net5] Ep 4400: loss=2439.359131, pde=2439.358887, bc=0.000227, λ=1.000, dt=0.012s, elapsed=0.9m\n",
      "[Net5] Ep 4600: loss=2391.430420, pde=2391.430176, bc=0.000261, λ=1.000, dt=0.014s, elapsed=0.9m\n",
      "[Net5] Ep 4800: loss=2398.669189, pde=2398.669189, bc=0.000000, λ=1.000, dt=0.011s, elapsed=1.0m\n",
      "[Net5] Ep 5000: loss=2527.216064, pde=2527.215088, bc=0.000910, λ=1.000, dt=0.012s, elapsed=1.0m\n",
      "[Net5] Ep 5200: loss=2563.626953, pde=2563.626953, bc=0.000022, λ=1.000, dt=0.012s, elapsed=1.1m\n",
      "[Net5] Ep 5400: loss=2494.578369, pde=2494.578369, bc=0.000053, λ=1.000, dt=0.011s, elapsed=1.1m\n",
      "[Net5] Ep 5600: loss=2364.966309, pde=2364.966309, bc=0.000050, λ=1.000, dt=0.014s, elapsed=1.1m\n",
      "[Net5] Ep 5800: loss=2405.434814, pde=2405.434570, bc=0.000150, λ=1.000, dt=0.011s, elapsed=1.2m\n",
      "[Net5] Ep 6000: loss=2563.467773, pde=2563.467773, bc=0.000076, λ=1.000, dt=0.017s, elapsed=1.2m\n",
      "[Net5] Ep 6200: loss=2522.343994, pde=2522.343994, bc=0.000001, λ=1.000, dt=0.011s, elapsed=1.3m\n",
      "[Net5] Ep 6400: loss=2412.983154, pde=2412.983154, bc=0.000000, λ=1.000, dt=0.013s, elapsed=1.3m\n",
      "[Net5] Ep 6600: loss=2431.626221, pde=2431.625977, bc=0.000237, λ=1.000, dt=0.018s, elapsed=1.3m\n",
      "[Net5] Ep 6800: loss=2560.302246, pde=2560.302246, bc=0.000114, λ=1.000, dt=0.014s, elapsed=1.4m\n",
      "[Net5] Ep 7000: loss=2453.142090, pde=2453.142090, bc=0.000059, λ=1.000, dt=0.012s, elapsed=1.4m\n",
      "[Net5] Ep 7200: loss=2514.532227, pde=2514.532227, bc=0.000011, λ=1.000, dt=0.012s, elapsed=1.5m\n",
      "[Net5] Ep 7400: loss=2484.616455, pde=2484.615234, bc=0.001122, λ=1.000, dt=0.011s, elapsed=1.5m\n",
      "[Net5] Ep 7600: loss=2418.468994, pde=2418.468994, bc=0.000122, λ=1.000, dt=0.018s, elapsed=1.6m\n",
      "[Net5] Ep 7800: loss=2397.859863, pde=2397.859863, bc=0.000050, λ=1.000, dt=0.011s, elapsed=1.6m\n",
      "[Net5] Ep 8000: loss=2536.238281, pde=2536.237549, bc=0.000651, λ=1.000, dt=0.011s, elapsed=1.6m\n",
      "[Net5] Ep 8200: loss=2592.254883, pde=2592.253418, bc=0.001343, λ=1.000, dt=0.012s, elapsed=1.7m\n",
      "[Net5] Ep 8400: loss=2493.813965, pde=2493.813477, bc=0.000384, λ=1.000, dt=0.012s, elapsed=1.7m\n",
      "[Net5] Ep 8600: loss=2473.964600, pde=2473.963379, bc=0.001103, λ=1.000, dt=0.017s, elapsed=1.8m\n",
      "[Net5] Ep 8800: loss=2376.236572, pde=2376.236328, bc=0.000133, λ=1.000, dt=0.011s, elapsed=1.8m\n",
      "[Net5] Ep 9000: loss=2525.411865, pde=2525.411865, bc=0.000047, λ=1.000, dt=0.011s, elapsed=1.8m\n",
      "[Net5] Ep 9200: loss=2520.502197, pde=2520.502197, bc=0.000073, λ=1.000, dt=0.011s, elapsed=1.9m\n",
      "[Net5] Ep 9400: loss=2454.650146, pde=2454.649902, bc=0.000306, λ=1.000, dt=0.011s, elapsed=1.9m\n",
      "[Net5] Ep 9600: loss=2448.757568, pde=2448.756592, bc=0.001068, λ=1.000, dt=0.023s, elapsed=2.0m\n",
      "[Net5] Ep 9800: loss=2487.881836, pde=2487.881348, bc=0.000517, λ=1.000, dt=0.012s, elapsed=2.0m\n",
      "[Net5] Ep 10000: loss=2500.694824, pde=2500.694824, bc=0.000009, λ=1.000, dt=0.014s, elapsed=2.1m\n",
      "[Net5] Ep 10200: loss=2382.793945, pde=2382.793945, bc=0.000005, λ=1.000, dt=0.012s, elapsed=2.1m\n",
      "[Net5] Ep 10400: loss=2601.136963, pde=2601.136963, bc=0.000078, λ=1.000, dt=0.011s, elapsed=2.1m\n",
      "[Net5] Ep 10600: loss=2542.677246, pde=2542.677246, bc=0.000022, λ=1.000, dt=0.018s, elapsed=2.2m\n",
      "[Net5] Ep 10800: loss=2532.165527, pde=2532.165527, bc=0.000034, λ=1.000, dt=0.011s, elapsed=2.2m\n",
      "[Net5] Ep 11000: loss=2630.216797, pde=2630.216064, bc=0.000639, λ=1.000, dt=0.011s, elapsed=2.3m\n",
      "[Net5] Ep 11200: loss=2452.998291, pde=2452.998291, bc=0.000000, λ=1.000, dt=0.019s, elapsed=2.3m\n",
      "[Net5] Ep 11400: loss=2532.586426, pde=2532.586426, bc=0.000095, λ=1.000, dt=0.012s, elapsed=2.3m\n",
      "[Net5] Ep 11600: loss=2543.872070, pde=2543.871826, bc=0.000355, λ=1.000, dt=0.025s, elapsed=2.4m\n",
      "[Net5] Ep 11800: loss=2618.338379, pde=2618.337891, bc=0.000453, λ=1.000, dt=0.011s, elapsed=2.4m\n",
      "[Net5] Ep 12000: loss=2650.336914, pde=2650.336670, bc=0.000294, λ=1.000, dt=0.012s, elapsed=2.5m\n",
      "[Net5] Ep 12200: loss=2477.194336, pde=2477.194092, bc=0.000289, λ=1.000, dt=0.012s, elapsed=2.5m\n",
      "[Net5] Ep 12400: loss=2458.732178, pde=2458.731689, bc=0.000445, λ=1.000, dt=0.011s, elapsed=2.5m\n",
      "[Net5] Ep 12600: loss=2608.901611, pde=2608.901367, bc=0.000211, λ=1.000, dt=0.011s, elapsed=2.6m\n",
      "[Net5] Ep 12800: loss=2402.265137, pde=2402.265137, bc=0.000097, λ=1.000, dt=0.018s, elapsed=2.6m\n",
      "[Net5] Ep 13000: loss=2498.972168, pde=2498.971924, bc=0.000221, λ=1.000, dt=0.011s, elapsed=2.7m\n",
      "[Net5] Ep 13200: loss=2537.760986, pde=2537.760742, bc=0.000148, λ=1.000, dt=0.011s, elapsed=2.7m\n",
      "[Net5] Ep 13400: loss=2601.021729, pde=2601.021240, bc=0.000467, λ=1.000, dt=0.013s, elapsed=2.7m\n",
      "[Net5] Ep 13600: loss=2560.839844, pde=2560.839111, bc=0.000614, λ=1.000, dt=0.012s, elapsed=2.8m\n",
      "[Net5] Ep 13800: loss=2603.578857, pde=2603.578369, bc=0.000604, λ=1.000, dt=0.012s, elapsed=2.8m\n",
      "[Net5] Ep 14000: loss=2453.966553, pde=2453.966064, bc=0.000487, λ=1.000, dt=0.011s, elapsed=2.9m\n",
      "[Net5] Ep 14200: loss=2628.052002, pde=2628.051758, bc=0.000341, λ=1.000, dt=0.012s, elapsed=2.9m\n",
      "[Net5] Ep 14400: loss=2563.923340, pde=2563.923096, bc=0.000180, λ=1.000, dt=0.011s, elapsed=2.9m\n",
      "[Net5] Ep 14600: loss=2479.384277, pde=2479.384033, bc=0.000146, λ=1.000, dt=0.011s, elapsed=3.0m\n",
      "[Net5] Ep 14800: loss=2423.060303, pde=2423.060059, bc=0.000146, λ=1.000, dt=0.012s, elapsed=3.0m\n",
      "[Net5] Ep 15000: loss=2389.363770, pde=2389.363525, bc=0.000222, λ=1.000, dt=0.011s, elapsed=3.1m\n",
      "[Net5] Ep 15200: loss=2394.318848, pde=2394.318604, bc=0.000143, λ=1.000, dt=0.012s, elapsed=3.1m\n",
      "[Net5] Ep 15400: loss=2468.024902, pde=2468.024658, bc=0.000164, λ=1.000, dt=0.011s, elapsed=3.1m\n",
      "[Net5] Ep 15600: loss=2523.845215, pde=2523.844971, bc=0.000135, λ=1.000, dt=0.011s, elapsed=3.2m\n",
      "[Net5] Ep 15800: loss=2451.070557, pde=2451.070557, bc=0.000109, λ=1.000, dt=0.011s, elapsed=3.2m\n",
      "[Net5] Ep 16000: loss=2536.803711, pde=2536.803711, bc=0.000065, λ=1.000, dt=0.011s, elapsed=3.3m\n",
      "[Net5] Ep 16200: loss=2407.923584, pde=2407.923584, bc=0.000045, λ=1.000, dt=0.011s, elapsed=3.3m\n",
      "[Net5] Ep 16400: loss=2425.782959, pde=2425.782959, bc=0.000003, λ=1.000, dt=0.017s, elapsed=3.4m\n",
      "[Net5] Ep 16600: loss=2429.026855, pde=2429.026855, bc=0.000000, λ=1.000, dt=0.011s, elapsed=3.4m\n",
      "[Net5] Ep 16800: loss=2379.749756, pde=2379.749756, bc=0.000000, λ=1.000, dt=0.011s, elapsed=3.4m\n",
      "[Net5] Ep 17000: loss=2555.398926, pde=2555.398926, bc=0.000000, λ=1.000, dt=0.012s, elapsed=3.5m\n",
      "[Net5] Ep 17200: loss=2488.649414, pde=2488.649414, bc=0.000001, λ=1.000, dt=0.014s, elapsed=3.5m\n",
      "[Net5] Ep 17400: loss=2481.625488, pde=2481.625488, bc=0.000001, λ=1.000, dt=0.014s, elapsed=3.6m\n",
      "[Net5] Ep 17600: loss=2644.911865, pde=2644.911865, bc=0.000001, λ=1.000, dt=0.012s, elapsed=3.6m\n",
      "[Net5] Ep 17800: loss=2391.609863, pde=2391.609863, bc=0.000000, λ=1.000, dt=0.011s, elapsed=3.6m\n",
      "[Net5] Ep 18000: loss=2423.474609, pde=2423.474609, bc=0.000001, λ=1.000, dt=0.018s, elapsed=3.7m\n",
      "[Net5] Ep 18200: loss=2384.758301, pde=2384.758301, bc=0.000001, λ=1.000, dt=0.011s, elapsed=3.7m\n",
      "[Net5] Ep 18400: loss=2457.598877, pde=2457.598877, bc=0.000001, λ=1.000, dt=0.014s, elapsed=3.8m\n",
      "[Net5] Ep 18600: loss=2448.164551, pde=2448.164551, bc=0.000000, λ=1.000, dt=0.011s, elapsed=3.8m\n",
      "[Net5] Ep 18800: loss=2506.612061, pde=2506.612061, bc=0.000000, λ=1.000, dt=0.011s, elapsed=3.8m\n",
      "[Net5] Ep 19000: loss=2453.615723, pde=2453.615723, bc=0.000000, λ=1.000, dt=0.011s, elapsed=3.9m\n",
      "[Net5] Ep 19200: loss=2485.452148, pde=2485.452148, bc=0.000000, λ=1.000, dt=0.011s, elapsed=3.9m\n",
      "[Net5] Ep 19400: loss=2511.286865, pde=2511.286865, bc=0.000000, λ=1.000, dt=0.015s, elapsed=4.0m\n",
      "[Net5] Ep 19600: loss=2401.531738, pde=2401.531738, bc=0.000000, λ=1.000, dt=0.012s, elapsed=4.0m\n",
      "[Net5] Ep 19800: loss=2503.957520, pde=2503.957520, bc=0.000000, λ=1.000, dt=0.011s, elapsed=4.0m\n",
      "[Net5] Ep 20000: loss=2503.573730, pde=2503.573730, bc=0.000000, λ=1.000, dt=0.011s, elapsed=4.1m\n",
      "Net5 training complete in 4.1 min (final loss 2503.573730)\n",
      "[Net6] Ep 1: loss=2597.436035, pde=2597.400391, bc=0.035692, λ=1.000, dt=0.011s, elapsed=0.0m\n",
      "[Net6] Ep 200: loss=627.649536, pde=618.048157, bc=9.601387, λ=1.000, dt=0.011s, elapsed=0.0m\n",
      "[Net6] Ep 400: loss=11.474938, pde=5.969943, bc=5.504996, λ=1.000, dt=0.016s, elapsed=0.1m\n",
      "[Net6] Ep 600: loss=5.639782, pde=2.200822, bc=3.438959, λ=1.000, dt=0.011s, elapsed=0.1m\n",
      "[Net6] Ep 800: loss=6.576822, pde=4.283647, bc=2.293175, λ=1.000, dt=0.011s, elapsed=0.2m\n",
      "[Net6] Ep 1000: loss=2.581851, pde=0.930650, bc=1.651202, λ=1.000, dt=0.011s, elapsed=0.2m\n",
      "[Net6] Ep 1200: loss=1.739316, pde=0.402056, bc=1.337260, λ=1.000, dt=0.011s, elapsed=0.2m\n",
      "[Net6] Ep 1400: loss=1.453001, pde=0.395086, bc=1.057915, λ=1.000, dt=0.014s, elapsed=0.3m\n",
      "[Net6] Ep 1600: loss=1.181081, pde=0.246359, bc=0.934722, λ=1.000, dt=0.011s, elapsed=0.3m\n",
      "[Net6] Ep 1800: loss=1.083630, pde=0.360767, bc=0.722863, λ=1.000, dt=0.016s, elapsed=0.4m\n",
      "[Net6] Ep 2000: loss=0.869841, pde=0.220528, bc=0.649313, λ=1.000, dt=0.033s, elapsed=0.5m\n",
      "[Net6] Ep 2200: loss=1.081598, pde=0.575772, bc=0.505826, λ=1.000, dt=0.011s, elapsed=0.5m\n",
      "[Net6] Ep 2400: loss=0.646886, pde=0.215939, bc=0.430947, λ=1.000, dt=0.011s, elapsed=0.6m\n",
      "[Net6] Ep 2600: loss=0.696217, pde=0.333136, bc=0.363080, λ=1.000, dt=0.011s, elapsed=0.6m\n",
      "[Net6] Ep 2800: loss=0.500693, pde=0.183554, bc=0.317139, λ=1.000, dt=0.012s, elapsed=0.6m\n",
      "[Net6] Ep 3000: loss=0.641507, pde=0.383775, bc=0.257732, λ=1.000, dt=0.018s, elapsed=0.7m\n",
      "[Net6] Ep 3200: loss=0.380296, pde=0.165029, bc=0.215267, λ=1.000, dt=0.011s, elapsed=0.7m\n",
      "[Net6] Ep 3400: loss=0.381834, pde=0.183160, bc=0.198674, λ=1.000, dt=0.012s, elapsed=0.8m\n",
      "[Net6] Ep 3600: loss=0.702133, pde=0.506626, bc=0.195508, λ=1.000, dt=0.011s, elapsed=0.8m\n",
      "[Net6] Ep 3800: loss=0.580482, pde=0.432010, bc=0.148472, λ=1.000, dt=0.011s, elapsed=0.8m\n",
      "[Net6] Ep 4000: loss=0.405592, pde=0.259159, bc=0.146432, λ=1.000, dt=0.012s, elapsed=0.9m\n",
      "[Net6] Ep 4200: loss=1.294034, pde=1.169125, bc=0.124909, λ=1.000, dt=0.011s, elapsed=0.9m\n",
      "[Net6] Ep 4400: loss=0.331321, pde=0.210643, bc=0.120678, λ=1.000, dt=0.012s, elapsed=1.0m\n",
      "[Net6] Ep 4600: loss=0.216197, pde=0.107978, bc=0.108220, λ=1.000, dt=0.011s, elapsed=1.0m\n",
      "[Net6] Ep 4800: loss=0.189967, pde=0.087729, bc=0.102238, λ=1.000, dt=0.011s, elapsed=1.0m\n",
      "[Net6] Ep 5000: loss=0.277231, pde=0.175646, bc=0.101584, λ=1.000, dt=0.017s, elapsed=1.1m\n",
      "[Net6] Ep 5200: loss=0.369565, pde=0.276152, bc=0.093412, λ=1.000, dt=0.011s, elapsed=1.1m\n",
      "[Net6] Ep 5400: loss=0.195995, pde=0.106245, bc=0.089749, λ=1.000, dt=0.011s, elapsed=1.2m\n",
      "[Net6] Ep 5600: loss=0.360186, pde=0.276950, bc=0.083236, λ=1.000, dt=0.011s, elapsed=1.2m\n",
      "[Net6] Ep 5800: loss=0.968912, pde=0.887107, bc=0.081806, λ=1.000, dt=0.011s, elapsed=1.2m\n",
      "[Net6] Ep 6000: loss=0.146430, pde=0.069297, bc=0.077133, λ=1.000, dt=0.016s, elapsed=1.3m\n",
      "[Net6] Ep 6200: loss=0.202480, pde=0.130175, bc=0.072304, λ=1.000, dt=0.011s, elapsed=1.3m\n",
      "[Net6] Ep 6400: loss=0.139844, pde=0.068546, bc=0.071298, λ=1.000, dt=0.011s, elapsed=1.4m\n",
      "[Net6] Ep 6600: loss=0.200976, pde=0.136421, bc=0.064555, λ=1.000, dt=0.011s, elapsed=1.4m\n",
      "[Net6] Ep 6800: loss=0.180624, pde=0.109763, bc=0.070861, λ=1.000, dt=0.011s, elapsed=1.4m\n",
      "[Net6] Ep 7000: loss=1.081645, pde=1.017433, bc=0.064211, λ=1.000, dt=0.011s, elapsed=1.5m\n",
      "[Net6] Ep 7200: loss=0.323033, pde=0.256676, bc=0.066358, λ=1.000, dt=0.012s, elapsed=1.5m\n",
      "[Net6] Ep 7400: loss=0.125077, pde=0.059689, bc=0.065388, λ=1.000, dt=0.011s, elapsed=1.6m\n",
      "[Net6] Ep 7600: loss=0.126798, pde=0.067646, bc=0.059152, λ=1.000, dt=0.011s, elapsed=1.6m\n",
      "[Net6] Ep 7800: loss=0.162882, pde=0.110534, bc=0.052347, λ=1.000, dt=0.011s, elapsed=1.6m\n",
      "[Net6] Ep 8000: loss=0.164514, pde=0.112158, bc=0.052356, λ=1.000, dt=0.012s, elapsed=1.7m\n",
      "[Net6] Ep 8200: loss=0.117594, pde=0.066688, bc=0.050906, λ=1.000, dt=0.011s, elapsed=1.7m\n",
      "[Net6] Ep 8400: loss=0.194496, pde=0.137898, bc=0.056599, λ=1.000, dt=0.011s, elapsed=1.8m\n",
      "[Net6] Ep 8600: loss=0.098218, pde=0.043715, bc=0.054503, λ=1.000, dt=0.012s, elapsed=1.8m\n",
      "[Net6] Ep 8800: loss=0.099740, pde=0.051476, bc=0.048264, λ=1.000, dt=0.011s, elapsed=1.8m\n",
      "[Net6] Ep 9000: loss=0.100643, pde=0.053347, bc=0.047296, λ=1.000, dt=0.011s, elapsed=1.9m\n",
      "[Net6] Ep 9200: loss=0.859253, pde=0.813057, bc=0.046196, λ=1.000, dt=0.011s, elapsed=1.9m\n",
      "[Net6] Ep 9400: loss=0.085035, pde=0.040118, bc=0.044918, λ=1.000, dt=0.011s, elapsed=2.0m\n",
      "[Net6] Ep 9600: loss=0.108957, pde=0.064053, bc=0.044905, λ=1.000, dt=0.011s, elapsed=2.0m\n",
      "[Net6] Ep 9800: loss=0.126012, pde=0.087311, bc=0.038701, λ=1.000, dt=0.013s, elapsed=2.0m\n",
      "[Net6] Ep 10000: loss=0.080980, pde=0.036470, bc=0.044510, λ=1.000, dt=0.011s, elapsed=2.1m\n",
      "[Net6] Ep 10200: loss=0.112343, pde=0.071965, bc=0.040379, λ=1.000, dt=0.011s, elapsed=2.1m\n",
      "[Net6] Ep 10400: loss=0.084452, pde=0.048030, bc=0.036422, λ=1.000, dt=0.011s, elapsed=2.2m\n",
      "[Net6] Ep 10600: loss=0.094353, pde=0.054047, bc=0.040306, λ=1.000, dt=0.011s, elapsed=2.2m\n",
      "[Net6] Ep 10800: loss=0.113953, pde=0.077805, bc=0.036148, λ=1.000, dt=0.011s, elapsed=2.2m\n",
      "[Net6] Ep 11000: loss=0.079367, pde=0.039617, bc=0.039750, λ=1.000, dt=0.011s, elapsed=2.3m\n",
      "[Net6] Ep 11200: loss=0.061583, pde=0.021070, bc=0.040513, λ=1.000, dt=0.017s, elapsed=2.3m\n",
      "[Net6] Ep 11400: loss=0.063319, pde=0.026063, bc=0.037256, λ=1.000, dt=0.011s, elapsed=2.3m\n",
      "[Net6] Ep 11600: loss=0.107919, pde=0.072238, bc=0.035681, λ=1.000, dt=0.011s, elapsed=2.4m\n",
      "[Net6] Ep 11800: loss=0.079385, pde=0.045379, bc=0.034006, λ=1.000, dt=0.012s, elapsed=2.4m\n",
      "[Net6] Ep 12000: loss=0.075637, pde=0.042083, bc=0.033554, λ=1.000, dt=0.012s, elapsed=2.5m\n",
      "[Net6] Ep 12200: loss=0.053387, pde=0.020885, bc=0.032501, λ=1.000, dt=0.011s, elapsed=2.5m\n",
      "[Net6] Ep 12400: loss=0.073312, pde=0.040697, bc=0.032615, λ=1.000, dt=0.011s, elapsed=2.6m\n",
      "[Net6] Ep 12600: loss=0.062508, pde=0.029602, bc=0.032906, λ=1.000, dt=0.011s, elapsed=2.6m\n",
      "[Net6] Ep 12800: loss=0.059962, pde=0.028266, bc=0.031696, λ=1.000, dt=0.012s, elapsed=2.6m\n",
      "[Net6] Ep 13000: loss=0.057181, pde=0.025760, bc=0.031421, λ=1.000, dt=0.011s, elapsed=2.7m\n",
      "[Net6] Ep 13200: loss=0.068070, pde=0.035984, bc=0.032086, λ=1.000, dt=0.012s, elapsed=2.7m\n",
      "[Net6] Ep 13400: loss=0.052084, pde=0.021654, bc=0.030430, λ=1.000, dt=0.011s, elapsed=2.8m\n",
      "[Net6] Ep 13600: loss=0.066117, pde=0.036858, bc=0.029259, λ=1.000, dt=0.012s, elapsed=2.8m\n",
      "[Net6] Ep 13800: loss=0.056724, pde=0.026903, bc=0.029820, λ=1.000, dt=0.013s, elapsed=2.8m\n",
      "[Net6] Ep 14000: loss=0.047735, pde=0.019347, bc=0.028388, λ=1.000, dt=0.011s, elapsed=2.9m\n",
      "[Net6] Ep 14200: loss=0.060703, pde=0.030967, bc=0.029736, λ=1.000, dt=0.011s, elapsed=2.9m\n",
      "[Net6] Ep 14400: loss=0.047714, pde=0.020512, bc=0.027202, λ=1.000, dt=0.012s, elapsed=3.0m\n",
      "[Net6] Ep 14600: loss=0.050762, pde=0.021885, bc=0.028877, λ=1.000, dt=0.011s, elapsed=3.0m\n",
      "[Net6] Ep 14800: loss=0.047993, pde=0.020527, bc=0.027466, λ=1.000, dt=0.011s, elapsed=3.0m\n",
      "[Net6] Ep 15000: loss=0.046169, pde=0.018653, bc=0.027517, λ=1.000, dt=0.012s, elapsed=3.1m\n",
      "[Net6] Ep 15200: loss=0.043981, pde=0.019076, bc=0.024905, λ=1.000, dt=0.012s, elapsed=3.1m\n",
      "[Net6] Ep 15400: loss=0.047401, pde=0.022110, bc=0.025291, λ=1.000, dt=0.011s, elapsed=3.2m\n",
      "[Net6] Ep 15600: loss=0.042401, pde=0.017422, bc=0.024978, λ=1.000, dt=0.011s, elapsed=3.2m\n",
      "[Net6] Ep 15800: loss=0.046334, pde=0.020576, bc=0.025757, λ=1.000, dt=0.011s, elapsed=3.2m\n",
      "[Net6] Ep 16000: loss=0.043089, pde=0.018023, bc=0.025066, λ=1.000, dt=0.011s, elapsed=3.3m\n",
      "[Net6] Ep 16200: loss=0.043179, pde=0.019129, bc=0.024050, λ=1.000, dt=0.017s, elapsed=3.3m\n",
      "[Net6] Ep 16400: loss=0.038472, pde=0.015167, bc=0.023305, λ=1.000, dt=0.011s, elapsed=3.4m\n",
      "[Net6] Ep 16600: loss=0.045624, pde=0.020713, bc=0.024911, λ=1.000, dt=0.012s, elapsed=3.4m\n",
      "[Net6] Ep 16800: loss=0.038860, pde=0.015112, bc=0.023748, λ=1.000, dt=0.011s, elapsed=3.4m\n",
      "[Net6] Ep 17000: loss=0.041993, pde=0.018027, bc=0.023966, λ=1.000, dt=0.011s, elapsed=3.5m\n",
      "[Net6] Ep 17200: loss=0.039354, pde=0.015574, bc=0.023780, λ=1.000, dt=0.015s, elapsed=3.5m\n",
      "[Net6] Ep 17400: loss=0.040381, pde=0.014292, bc=0.026089, λ=1.000, dt=0.011s, elapsed=3.6m\n",
      "[Net6] Ep 17600: loss=0.038186, pde=0.014468, bc=0.023718, λ=1.000, dt=0.011s, elapsed=3.6m\n",
      "[Net6] Ep 17800: loss=0.036524, pde=0.013848, bc=0.022675, λ=1.000, dt=0.018s, elapsed=3.6m\n",
      "[Net6] Ep 18000: loss=0.038035, pde=0.015631, bc=0.022404, λ=1.000, dt=0.011s, elapsed=3.7m\n",
      "[Net6] Ep 18200: loss=0.036581, pde=0.012918, bc=0.023664, λ=1.000, dt=0.016s, elapsed=3.7m\n",
      "[Net6] Ep 18400: loss=0.037222, pde=0.013979, bc=0.023242, λ=1.000, dt=0.012s, elapsed=3.8m\n",
      "[Net6] Ep 18600: loss=0.035896, pde=0.013884, bc=0.022012, λ=1.000, dt=0.011s, elapsed=3.8m\n",
      "[Net6] Ep 18800: loss=0.037902, pde=0.013875, bc=0.024027, λ=1.000, dt=0.011s, elapsed=3.8m\n",
      "[Net6] Ep 19000: loss=0.037017, pde=0.014828, bc=0.022189, λ=1.000, dt=0.011s, elapsed=3.9m\n",
      "[Net6] Ep 19200: loss=0.037195, pde=0.016543, bc=0.020652, λ=1.000, dt=0.013s, elapsed=3.9m\n",
      "[Net6] Ep 19400: loss=0.036012, pde=0.013281, bc=0.022732, λ=1.000, dt=0.011s, elapsed=4.0m\n",
      "[Net6] Ep 19600: loss=0.033184, pde=0.012567, bc=0.020617, λ=1.000, dt=0.012s, elapsed=4.0m\n",
      "[Net6] Ep 19800: loss=0.038329, pde=0.014612, bc=0.023717, λ=1.000, dt=0.011s, elapsed=4.0m\n",
      "[Net6] Ep 20000: loss=0.036108, pde=0.012573, bc=0.023536, λ=1.000, dt=0.012s, elapsed=4.1m\n",
      "Net6 training complete in 4.1 min (final loss 0.036108)\n",
      "[Net7] Ep 1: loss=2597.311523, pde=2597.296387, bc=0.015194, λ=1.000, dt=0.014s, elapsed=0.0m\n",
      "[Net7] Ep 200: loss=408.334991, pde=404.799652, bc=3.535328, λ=1.000, dt=0.014s, elapsed=0.0m\n",
      "[Net7] Ep 400: loss=5.689075, pde=4.390065, bc=1.299010, λ=1.000, dt=0.011s, elapsed=0.1m\n",
      "[Net7] Ep 600: loss=1.812163, pde=1.028788, bc=0.783375, λ=1.000, dt=0.014s, elapsed=0.1m\n",
      "[Net7] Ep 800: loss=1.231703, pde=0.693561, bc=0.538141, λ=1.000, dt=0.011s, elapsed=0.2m\n",
      "[Net7] Ep 1000: loss=1.060625, pde=0.600400, bc=0.460224, λ=1.000, dt=0.012s, elapsed=0.2m\n",
      "[Net7] Ep 1200: loss=0.611489, pde=0.278569, bc=0.332921, λ=1.000, dt=0.017s, elapsed=0.2m\n",
      "[Net7] Ep 1400: loss=1.285882, pde=1.006458, bc=0.279424, λ=1.000, dt=0.014s, elapsed=0.3m\n",
      "[Net7] Ep 1600: loss=0.733275, pde=0.499477, bc=0.233798, λ=1.000, dt=0.011s, elapsed=0.3m\n",
      "[Net7] Ep 1800: loss=0.525735, pde=0.325532, bc=0.200203, λ=1.000, dt=0.011s, elapsed=0.4m\n",
      "[Net7] Ep 2000: loss=0.320664, pde=0.154850, bc=0.165814, λ=1.000, dt=0.011s, elapsed=0.4m\n",
      "[Net7] Ep 2200: loss=0.401471, pde=0.253047, bc=0.148425, λ=1.000, dt=0.019s, elapsed=0.4m\n",
      "[Net7] Ep 2400: loss=0.850513, pde=0.707544, bc=0.142969, λ=1.000, dt=0.011s, elapsed=0.5m\n",
      "[Net7] Ep 2600: loss=1.142628, pde=1.026186, bc=0.116442, λ=1.000, dt=0.011s, elapsed=0.5m\n",
      "[Net7] Ep 2800: loss=1.220531, pde=1.100405, bc=0.120126, λ=1.000, dt=0.012s, elapsed=0.6m\n",
      "[Net7] Ep 3000: loss=0.305963, pde=0.207704, bc=0.098259, λ=1.000, dt=0.011s, elapsed=0.6m\n",
      "[Net7] Ep 3200: loss=0.345499, pde=0.253713, bc=0.091786, λ=1.000, dt=0.013s, elapsed=0.6m\n",
      "[Net7] Ep 3400: loss=0.188854, pde=0.099578, bc=0.089276, λ=1.000, dt=0.014s, elapsed=0.7m\n",
      "[Net7] Ep 3600: loss=0.420377, pde=0.331249, bc=0.089128, λ=1.000, dt=0.011s, elapsed=0.7m\n",
      "[Net7] Ep 3800: loss=0.173183, pde=0.094603, bc=0.078580, λ=1.000, dt=0.011s, elapsed=0.8m\n",
      "[Net7] Ep 4000: loss=0.239944, pde=0.164541, bc=0.075403, λ=1.000, dt=0.011s, elapsed=0.8m\n",
      "[Net7] Ep 4200: loss=0.277526, pde=0.205470, bc=0.072056, λ=1.000, dt=0.011s, elapsed=0.9m\n",
      "[Net7] Ep 4400: loss=0.307902, pde=0.237299, bc=0.070603, λ=1.000, dt=0.013s, elapsed=0.9m\n",
      "[Net7] Ep 4600: loss=0.295736, pde=0.234578, bc=0.061158, λ=1.000, dt=0.011s, elapsed=0.9m\n",
      "[Net7] Ep 4800: loss=0.142574, pde=0.078127, bc=0.064446, λ=1.000, dt=0.012s, elapsed=1.0m\n",
      "[Net7] Ep 5000: loss=1.336422, pde=1.275584, bc=0.060838, λ=1.000, dt=0.011s, elapsed=1.0m\n",
      "[Net7] Ep 5200: loss=0.133328, pde=0.076682, bc=0.056646, λ=1.000, dt=0.012s, elapsed=1.1m\n",
      "[Net7] Ep 5400: loss=0.124460, pde=0.070599, bc=0.053861, λ=1.000, dt=0.011s, elapsed=1.1m\n",
      "[Net7] Ep 5600: loss=0.138243, pde=0.087151, bc=0.051091, λ=1.000, dt=0.011s, elapsed=1.1m\n",
      "[Net7] Ep 5800: loss=0.198145, pde=0.146718, bc=0.051427, λ=1.000, dt=0.018s, elapsed=1.2m\n",
      "[Net7] Ep 6000: loss=0.255987, pde=0.208504, bc=0.047483, λ=1.000, dt=0.011s, elapsed=1.2m\n",
      "[Net7] Ep 6200: loss=0.145156, pde=0.098572, bc=0.046583, λ=1.000, dt=0.012s, elapsed=1.3m\n",
      "[Net7] Ep 6400: loss=0.194377, pde=0.149134, bc=0.045243, λ=1.000, dt=0.011s, elapsed=1.3m\n",
      "[Net7] Ep 6600: loss=0.214660, pde=0.175975, bc=0.038685, λ=1.000, dt=0.011s, elapsed=1.3m\n",
      "[Net7] Ep 6800: loss=0.202859, pde=0.158749, bc=0.044111, λ=1.000, dt=0.011s, elapsed=1.4m\n",
      "[Net7] Ep 7000: loss=0.241363, pde=0.201177, bc=0.040187, λ=1.000, dt=0.011s, elapsed=1.4m\n",
      "[Net7] Ep 7200: loss=0.106198, pde=0.064995, bc=0.041203, λ=1.000, dt=0.018s, elapsed=1.5m\n",
      "[Net7] Ep 7400: loss=0.152949, pde=0.113604, bc=0.039346, λ=1.000, dt=0.011s, elapsed=1.5m\n",
      "[Net7] Ep 7600: loss=0.102280, pde=0.066548, bc=0.035733, λ=1.000, dt=0.011s, elapsed=1.5m\n",
      "[Net7] Ep 7800: loss=0.205740, pde=0.172535, bc=0.033205, λ=1.000, dt=0.012s, elapsed=1.6m\n",
      "[Net7] Ep 8000: loss=0.073899, pde=0.041085, bc=0.032813, λ=1.000, dt=0.011s, elapsed=1.6m\n",
      "[Net7] Ep 8200: loss=0.182327, pde=0.152328, bc=0.029999, λ=1.000, dt=0.017s, elapsed=1.7m\n",
      "[Net7] Ep 8400: loss=0.086244, pde=0.051141, bc=0.035103, λ=1.000, dt=0.011s, elapsed=1.7m\n",
      "[Net7] Ep 8600: loss=0.121785, pde=0.089929, bc=0.031856, λ=1.000, dt=0.012s, elapsed=1.7m\n",
      "[Net7] Ep 8800: loss=0.064918, pde=0.035994, bc=0.028924, λ=1.000, dt=0.011s, elapsed=1.8m\n",
      "[Net7] Ep 9000: loss=0.078100, pde=0.050787, bc=0.027313, λ=1.000, dt=0.012s, elapsed=1.8m\n",
      "[Net7] Ep 9200: loss=0.194445, pde=0.167932, bc=0.026513, λ=1.000, dt=0.019s, elapsed=1.9m\n",
      "[Net7] Ep 9400: loss=0.107321, pde=0.081455, bc=0.025866, λ=1.000, dt=0.012s, elapsed=1.9m\n",
      "[Net7] Ep 9600: loss=0.100145, pde=0.073088, bc=0.027057, λ=1.000, dt=0.011s, elapsed=1.9m\n",
      "[Net7] Ep 9800: loss=0.062995, pde=0.040233, bc=0.022762, λ=1.000, dt=0.011s, elapsed=2.0m\n",
      "[Net7] Ep 10000: loss=0.073597, pde=0.048621, bc=0.024977, λ=1.000, dt=0.011s, elapsed=2.0m\n",
      "[Net7] Ep 10200: loss=0.059183, pde=0.035765, bc=0.023418, λ=1.000, dt=0.018s, elapsed=2.1m\n",
      "[Net7] Ep 10400: loss=0.047834, pde=0.028012, bc=0.019822, λ=1.000, dt=0.011s, elapsed=2.1m\n",
      "[Net7] Ep 10600: loss=0.057613, pde=0.034321, bc=0.023292, λ=1.000, dt=0.011s, elapsed=2.1m\n",
      "[Net7] Ep 10800: loss=0.060649, pde=0.040340, bc=0.020309, λ=1.000, dt=0.011s, elapsed=2.2m\n",
      "[Net7] Ep 11000: loss=0.074439, pde=0.053368, bc=0.021071, λ=1.000, dt=0.011s, elapsed=2.2m\n",
      "[Net7] Ep 11200: loss=0.067255, pde=0.044912, bc=0.022343, λ=1.000, dt=0.018s, elapsed=2.2m\n",
      "[Net7] Ep 11400: loss=0.104439, pde=0.083659, bc=0.020780, λ=1.000, dt=0.011s, elapsed=2.3m\n",
      "[Net7] Ep 11600: loss=0.042595, pde=0.023158, bc=0.019437, λ=1.000, dt=0.011s, elapsed=2.3m\n",
      "[Net7] Ep 11800: loss=0.099870, pde=0.081215, bc=0.018655, λ=1.000, dt=0.012s, elapsed=2.4m\n",
      "[Net7] Ep 12000: loss=0.062289, pde=0.043727, bc=0.018562, λ=1.000, dt=0.011s, elapsed=2.4m\n",
      "[Net7] Ep 12200: loss=0.035150, pde=0.018469, bc=0.016682, λ=1.000, dt=0.018s, elapsed=2.4m\n",
      "[Net7] Ep 12400: loss=0.039149, pde=0.022915, bc=0.016234, λ=1.000, dt=0.011s, elapsed=2.5m\n",
      "[Net7] Ep 12600: loss=0.045739, pde=0.028301, bc=0.017438, λ=1.000, dt=0.011s, elapsed=2.5m\n",
      "[Net7] Ep 12800: loss=0.040091, pde=0.023343, bc=0.016748, λ=1.000, dt=0.011s, elapsed=2.6m\n",
      "[Net7] Ep 13000: loss=0.036090, pde=0.019244, bc=0.016846, λ=1.000, dt=0.011s, elapsed=2.6m\n",
      "[Net7] Ep 13200: loss=0.035381, pde=0.018915, bc=0.016466, λ=1.000, dt=0.013s, elapsed=2.6m\n",
      "[Net7] Ep 13400: loss=0.031922, pde=0.017464, bc=0.014458, λ=1.000, dt=0.011s, elapsed=2.7m\n",
      "[Net7] Ep 13600: loss=0.030968, pde=0.016389, bc=0.014579, λ=1.000, dt=0.011s, elapsed=2.7m\n",
      "[Net7] Ep 13800: loss=0.030729, pde=0.016591, bc=0.014138, λ=1.000, dt=0.011s, elapsed=2.8m\n",
      "[Net7] Ep 14000: loss=0.040074, pde=0.026880, bc=0.013194, λ=1.000, dt=0.011s, elapsed=2.8m\n",
      "[Net7] Ep 14200: loss=0.044340, pde=0.030491, bc=0.013849, λ=1.000, dt=0.013s, elapsed=2.8m\n",
      "[Net7] Ep 14400: loss=0.032354, pde=0.019311, bc=0.013042, λ=1.000, dt=0.011s, elapsed=2.9m\n",
      "[Net7] Ep 14600: loss=0.030808, pde=0.017135, bc=0.013673, λ=1.000, dt=0.011s, elapsed=2.9m\n",
      "[Net7] Ep 14800: loss=0.027939, pde=0.014527, bc=0.013412, λ=1.000, dt=0.012s, elapsed=3.0m\n",
      "[Net7] Ep 15000: loss=0.026817, pde=0.013428, bc=0.013390, λ=1.000, dt=0.011s, elapsed=3.0m\n",
      "[Net7] Ep 15200: loss=0.025092, pde=0.013294, bc=0.011797, λ=1.000, dt=0.014s, elapsed=3.0m\n",
      "[Net7] Ep 15400: loss=0.026618, pde=0.014294, bc=0.012324, λ=1.000, dt=0.011s, elapsed=3.1m\n",
      "[Net7] Ep 15600: loss=0.026958, pde=0.015594, bc=0.011364, λ=1.000, dt=0.011s, elapsed=3.1m\n",
      "[Net7] Ep 15800: loss=0.022497, pde=0.011032, bc=0.011464, λ=1.000, dt=0.011s, elapsed=3.2m\n",
      "[Net7] Ep 16000: loss=0.025488, pde=0.013536, bc=0.011951, λ=1.000, dt=0.011s, elapsed=3.2m\n",
      "[Net7] Ep 16200: loss=0.022269, pde=0.011079, bc=0.011189, λ=1.000, dt=0.014s, elapsed=3.2m\n",
      "[Net7] Ep 16400: loss=0.023035, pde=0.012206, bc=0.010829, λ=1.000, dt=0.011s, elapsed=3.3m\n",
      "[Net7] Ep 16600: loss=0.028503, pde=0.017097, bc=0.011406, λ=1.000, dt=0.011s, elapsed=3.3m\n",
      "[Net7] Ep 16800: loss=0.023719, pde=0.013112, bc=0.010607, λ=1.000, dt=0.011s, elapsed=3.4m\n",
      "[Net7] Ep 17000: loss=0.023555, pde=0.012776, bc=0.010779, λ=1.000, dt=0.011s, elapsed=3.4m\n",
      "[Net7] Ep 17200: loss=0.021270, pde=0.010574, bc=0.010695, λ=1.000, dt=0.014s, elapsed=3.4m\n",
      "[Net7] Ep 17400: loss=0.022502, pde=0.011389, bc=0.011112, λ=1.000, dt=0.011s, elapsed=3.5m\n",
      "[Net7] Ep 17600: loss=0.019122, pde=0.008957, bc=0.010165, λ=1.000, dt=0.011s, elapsed=3.5m\n",
      "[Net7] Ep 17800: loss=0.020846, pde=0.010733, bc=0.010113, λ=1.000, dt=0.011s, elapsed=3.6m\n",
      "[Net7] Ep 18000: loss=0.022306, pde=0.012521, bc=0.009786, λ=1.000, dt=0.011s, elapsed=3.6m\n",
      "[Net7] Ep 18200: loss=0.019265, pde=0.008610, bc=0.010655, λ=1.000, dt=0.014s, elapsed=3.6m\n",
      "[Net7] Ep 18400: loss=0.019131, pde=0.008794, bc=0.010337, λ=1.000, dt=0.012s, elapsed=3.7m\n",
      "[Net7] Ep 18600: loss=0.019135, pde=0.009379, bc=0.009755, λ=1.000, dt=0.011s, elapsed=3.7m\n",
      "[Net7] Ep 18800: loss=0.019276, pde=0.008624, bc=0.010652, λ=1.000, dt=0.011s, elapsed=3.8m\n",
      "[Net7] Ep 19000: loss=0.017741, pde=0.008191, bc=0.009550, λ=1.000, dt=0.018s, elapsed=3.8m\n",
      "[Net7] Ep 19200: loss=0.021179, pde=0.011690, bc=0.009490, λ=1.000, dt=0.014s, elapsed=3.8m\n",
      "[Net7] Ep 19400: loss=0.019817, pde=0.009093, bc=0.010724, λ=1.000, dt=0.012s, elapsed=3.9m\n",
      "[Net7] Ep 19600: loss=0.017232, pde=0.008099, bc=0.009133, λ=1.000, dt=0.011s, elapsed=3.9m\n",
      "[Net7] Ep 19800: loss=0.018961, pde=0.008557, bc=0.010404, λ=1.000, dt=0.012s, elapsed=4.0m\n",
      "[Net7] Ep 20000: loss=0.018217, pde=0.008642, bc=0.009575, λ=1.000, dt=0.011s, elapsed=4.0m\n",
      "Net7 training complete in 4.0 min (final loss 0.018217)\n",
      "[Net8] Ep 1: loss=2597.159912, pde=2597.159912, bc=0.000119, λ=1.000, dt=0.018s, elapsed=0.0m\n",
      "[Net8] Ep 200: loss=1126.211670, pde=1117.843628, bc=8.368064, λ=1.000, dt=0.023s, elapsed=0.1m\n",
      "[Net8] Ep 400: loss=33.204872, pde=22.526667, bc=10.678206, λ=1.000, dt=0.013s, elapsed=0.1m\n",
      "[Net8] Ep 600: loss=14.986810, pde=7.051264, bc=7.935545, λ=1.000, dt=0.015s, elapsed=0.2m\n",
      "[Net8] Ep 800: loss=9.908723, pde=3.873546, bc=6.035177, λ=1.000, dt=0.021s, elapsed=0.2m\n",
      "[Net8] Ep 1000: loss=6.327175, pde=2.158827, bc=4.168348, λ=1.000, dt=0.020s, elapsed=0.3m\n",
      "[Net8] Ep 1200: loss=3.801313, pde=1.192703, bc=2.608610, λ=1.000, dt=0.015s, elapsed=0.3m\n",
      "[Net8] Ep 1400: loss=3.204878, pde=1.428772, bc=1.776106, λ=1.000, dt=0.014s, elapsed=0.4m\n",
      "[Net8] Ep 1600: loss=2.952348, pde=1.493693, bc=1.458654, λ=1.000, dt=0.014s, elapsed=0.4m\n",
      "[Net8] Ep 1800: loss=1.741798, pde=0.458239, bc=1.283559, λ=1.000, dt=0.017s, elapsed=0.4m\n",
      "[Net8] Ep 2000: loss=1.981496, pde=0.725343, bc=1.256153, λ=1.000, dt=0.014s, elapsed=0.5m\n",
      "[Net8] Ep 2200: loss=1.434268, pde=0.287422, bc=1.146845, λ=1.000, dt=0.014s, elapsed=0.5m\n",
      "[Net8] Ep 2400: loss=1.494266, pde=0.367593, bc=1.126673, λ=1.000, dt=0.014s, elapsed=0.6m\n",
      "[Net8] Ep 2600: loss=1.362184, pde=0.276544, bc=1.085641, λ=1.000, dt=0.018s, elapsed=0.6m\n",
      "[Net8] Ep 2800: loss=1.471070, pde=0.434914, bc=1.036156, λ=1.000, dt=0.014s, elapsed=0.7m\n",
      "[Net8] Ep 3000: loss=1.130699, pde=0.172581, bc=0.958117, λ=1.000, dt=0.015s, elapsed=0.7m\n",
      "[Net8] Ep 3200: loss=1.750059, pde=0.853959, bc=0.896099, λ=1.000, dt=0.013s, elapsed=0.8m\n",
      "[Net8] Ep 3400: loss=1.121031, pde=0.227319, bc=0.893711, λ=1.000, dt=0.020s, elapsed=0.8m\n",
      "[Net8] Ep 3600: loss=1.569372, pde=0.680097, bc=0.889275, λ=1.000, dt=0.014s, elapsed=0.9m\n",
      "[Net8] Ep 3800: loss=1.135041, pde=0.366065, bc=0.768976, λ=1.000, dt=0.014s, elapsed=0.9m\n",
      "[Net8] Ep 4000: loss=1.288982, pde=0.554473, bc=0.734509, λ=1.000, dt=0.019s, elapsed=1.0m\n",
      "[Net8] Ep 4200: loss=0.989845, pde=0.358892, bc=0.630952, λ=1.000, dt=0.018s, elapsed=1.0m\n",
      "[Net8] Ep 4400: loss=1.181024, pde=0.557695, bc=0.623328, λ=1.000, dt=0.014s, elapsed=1.1m\n",
      "[Net8] Ep 4600: loss=0.742278, pde=0.177194, bc=0.565084, λ=1.000, dt=0.014s, elapsed=1.1m\n",
      "[Net8] Ep 4800: loss=1.217723, pde=0.722244, bc=0.495479, λ=1.000, dt=0.015s, elapsed=1.2m\n",
      "[Net8] Ep 5000: loss=0.729318, pde=0.239848, bc=0.489470, λ=1.000, dt=0.018s, elapsed=1.2m\n",
      "[Net8] Ep 5200: loss=0.631445, pde=0.217286, bc=0.414159, λ=1.000, dt=0.014s, elapsed=1.3m\n",
      "[Net8] Ep 5400: loss=0.581271, pde=0.204591, bc=0.376679, λ=1.000, dt=0.014s, elapsed=1.3m\n",
      "[Net8] Ep 5600: loss=0.739414, pde=0.397670, bc=0.341745, λ=1.000, dt=0.014s, elapsed=1.4m\n",
      "[Net8] Ep 5800: loss=0.554935, pde=0.269079, bc=0.285856, λ=1.000, dt=0.021s, elapsed=1.4m\n",
      "[Net8] Ep 6000: loss=0.539066, pde=0.299009, bc=0.240057, λ=1.000, dt=0.016s, elapsed=1.5m\n",
      "[Net8] Ep 6200: loss=0.561871, pde=0.350629, bc=0.211243, λ=1.000, dt=0.013s, elapsed=1.5m\n",
      "[Net8] Ep 6400: loss=0.310701, pde=0.124872, bc=0.185829, λ=1.000, dt=0.015s, elapsed=1.6m\n",
      "[Net8] Ep 6600: loss=0.271761, pde=0.115104, bc=0.156657, λ=1.000, dt=0.017s, elapsed=1.6m\n",
      "[Net8] Ep 6800: loss=0.301820, pde=0.155830, bc=0.145990, λ=1.000, dt=0.017s, elapsed=1.7m\n",
      "[Net8] Ep 7000: loss=0.226182, pde=0.109166, bc=0.117016, λ=1.000, dt=0.013s, elapsed=1.7m\n",
      "[Net8] Ep 7200: loss=0.364637, pde=0.260056, bc=0.104581, λ=1.000, dt=0.015s, elapsed=1.8m\n",
      "[Net8] Ep 7400: loss=0.292472, pde=0.198459, bc=0.094014, λ=1.000, dt=0.017s, elapsed=1.8m\n",
      "[Net8] Ep 7600: loss=0.185448, pde=0.105995, bc=0.079453, λ=1.000, dt=0.024s, elapsed=1.9m\n",
      "[Net8] Ep 7800: loss=0.286209, pde=0.220993, bc=0.065216, λ=1.000, dt=0.015s, elapsed=1.9m\n",
      "[Net8] Ep 8000: loss=0.177676, pde=0.118470, bc=0.059206, λ=1.000, dt=0.014s, elapsed=2.0m\n",
      "[Net8] Ep 8200: loss=0.201930, pde=0.149250, bc=0.052680, λ=1.000, dt=0.018s, elapsed=2.0m\n",
      "[Net8] Ep 8400: loss=0.298071, pde=0.242827, bc=0.055244, λ=1.000, dt=0.017s, elapsed=2.1m\n",
      "[Net8] Ep 8600: loss=0.136140, pde=0.088357, bc=0.047783, λ=1.000, dt=0.015s, elapsed=2.1m\n",
      "[Net8] Ep 8800: loss=0.203270, pde=0.159396, bc=0.043874, λ=1.000, dt=0.014s, elapsed=2.2m\n",
      "[Net8] Ep 9000: loss=0.100389, pde=0.059991, bc=0.040398, λ=1.000, dt=0.017s, elapsed=2.2m\n",
      "[Net8] Ep 9200: loss=0.106644, pde=0.069555, bc=0.037088, λ=1.000, dt=0.016s, elapsed=2.3m\n",
      "[Net8] Ep 9400: loss=0.078796, pde=0.042413, bc=0.036383, λ=1.000, dt=0.013s, elapsed=2.3m\n",
      "[Net8] Ep 9600: loss=0.082742, pde=0.047764, bc=0.034979, λ=1.000, dt=0.015s, elapsed=2.4m\n",
      "[Net8] Ep 9800: loss=0.071090, pde=0.041167, bc=0.029923, λ=1.000, dt=0.022s, elapsed=2.4m\n",
      "[Net8] Ep 10000: loss=0.113258, pde=0.080469, bc=0.032789, λ=1.000, dt=0.014s, elapsed=2.5m\n",
      "[Net8] Ep 10200: loss=0.065493, pde=0.037078, bc=0.028414, λ=1.000, dt=0.014s, elapsed=2.5m\n",
      "[Net8] Ep 10400: loss=0.141087, pde=0.114723, bc=0.026364, λ=1.000, dt=0.015s, elapsed=2.6m\n",
      "[Net8] Ep 10600: loss=0.101165, pde=0.072661, bc=0.028504, λ=1.000, dt=0.017s, elapsed=2.6m\n",
      "[Net8] Ep 10800: loss=0.065070, pde=0.040522, bc=0.024549, λ=1.000, dt=0.015s, elapsed=2.7m\n",
      "[Net8] Ep 11000: loss=0.074410, pde=0.048251, bc=0.026159, λ=1.000, dt=0.013s, elapsed=2.7m\n",
      "[Net8] Ep 11200: loss=0.077443, pde=0.051011, bc=0.026432, λ=1.000, dt=0.013s, elapsed=2.8m\n",
      "[Net8] Ep 11400: loss=0.069798, pde=0.045227, bc=0.024570, λ=1.000, dt=0.013s, elapsed=2.8m\n",
      "[Net8] Ep 11600: loss=0.061714, pde=0.038745, bc=0.022969, λ=1.000, dt=0.014s, elapsed=2.9m\n",
      "[Net8] Ep 11800: loss=0.051880, pde=0.030418, bc=0.021461, λ=1.000, dt=0.014s, elapsed=2.9m\n",
      "[Net8] Ep 12000: loss=0.076750, pde=0.055246, bc=0.021503, λ=1.000, dt=0.015s, elapsed=3.0m\n",
      "[Net8] Ep 12200: loss=0.057348, pde=0.037035, bc=0.020313, λ=1.000, dt=0.014s, elapsed=3.0m\n",
      "[Net8] Ep 12400: loss=0.047993, pde=0.028058, bc=0.019935, λ=1.000, dt=0.014s, elapsed=3.1m\n",
      "[Net8] Ep 12600: loss=0.043953, pde=0.023401, bc=0.020552, λ=1.000, dt=0.014s, elapsed=3.1m\n",
      "[Net8] Ep 12800: loss=0.045690, pde=0.026099, bc=0.019592, λ=1.000, dt=0.016s, elapsed=3.2m\n",
      "[Net8] Ep 13000: loss=0.040104, pde=0.021265, bc=0.018838, λ=1.000, dt=0.014s, elapsed=3.2m\n",
      "[Net8] Ep 13200: loss=0.046864, pde=0.028136, bc=0.018729, λ=1.000, dt=0.016s, elapsed=3.3m\n",
      "[Net8] Ep 13400: loss=0.043246, pde=0.025933, bc=0.017314, λ=1.000, dt=0.014s, elapsed=3.3m\n",
      "[Net8] Ep 13600: loss=0.042713, pde=0.025677, bc=0.017036, λ=1.000, dt=0.014s, elapsed=3.4m\n",
      "[Net8] Ep 13800: loss=0.037997, pde=0.021262, bc=0.016735, λ=1.000, dt=0.014s, elapsed=3.4m\n",
      "[Net8] Ep 14000: loss=0.039238, pde=0.023588, bc=0.015651, λ=1.000, dt=0.014s, elapsed=3.5m\n",
      "[Net8] Ep 14200: loss=0.035024, pde=0.018532, bc=0.016492, λ=1.000, dt=0.016s, elapsed=3.5m\n",
      "[Net8] Ep 14400: loss=0.032188, pde=0.016805, bc=0.015383, λ=1.000, dt=0.014s, elapsed=3.6m\n",
      "[Net8] Ep 14600: loss=0.034502, pde=0.018447, bc=0.016055, λ=1.000, dt=0.014s, elapsed=3.6m\n",
      "[Net8] Ep 14800: loss=0.033667, pde=0.018181, bc=0.015486, λ=1.000, dt=0.014s, elapsed=3.7m\n",
      "[Net8] Ep 15000: loss=0.031065, pde=0.015486, bc=0.015579, λ=1.000, dt=0.014s, elapsed=3.7m\n",
      "[Net8] Ep 15200: loss=0.030717, pde=0.016533, bc=0.014183, λ=1.000, dt=0.014s, elapsed=3.8m\n",
      "[Net8] Ep 15400: loss=0.031754, pde=0.017477, bc=0.014277, λ=1.000, dt=0.016s, elapsed=3.8m\n",
      "[Net8] Ep 15600: loss=0.031670, pde=0.018548, bc=0.013123, λ=1.000, dt=0.014s, elapsed=3.9m\n",
      "[Net8] Ep 15800: loss=0.030843, pde=0.017455, bc=0.013388, λ=1.000, dt=0.014s, elapsed=3.9m\n",
      "[Net8] Ep 16000: loss=0.028667, pde=0.014908, bc=0.013759, λ=1.000, dt=0.013s, elapsed=4.0m\n",
      "[Net8] Ep 16200: loss=0.028650, pde=0.015465, bc=0.013185, λ=1.000, dt=0.013s, elapsed=4.0m\n",
      "[Net8] Ep 16400: loss=0.026460, pde=0.013670, bc=0.012790, λ=1.000, dt=0.014s, elapsed=4.1m\n",
      "[Net8] Ep 16600: loss=0.028061, pde=0.014625, bc=0.013436, λ=1.000, dt=0.017s, elapsed=4.1m\n",
      "[Net8] Ep 16800: loss=0.026921, pde=0.014540, bc=0.012382, λ=1.000, dt=0.013s, elapsed=4.2m\n",
      "[Net8] Ep 17000: loss=0.027714, pde=0.015030, bc=0.012684, λ=1.000, dt=0.014s, elapsed=4.2m\n",
      "[Net8] Ep 17200: loss=0.024274, pde=0.011760, bc=0.012514, λ=1.000, dt=0.015s, elapsed=4.3m\n",
      "[Net8] Ep 17400: loss=0.025166, pde=0.012362, bc=0.012804, λ=1.000, dt=0.014s, elapsed=4.3m\n",
      "[Net8] Ep 17600: loss=0.025003, pde=0.012758, bc=0.012245, λ=1.000, dt=0.014s, elapsed=4.4m\n",
      "[Net8] Ep 17800: loss=0.024994, pde=0.012719, bc=0.012275, λ=1.000, dt=0.014s, elapsed=4.4m\n",
      "[Net8] Ep 18000: loss=0.023082, pde=0.011492, bc=0.011589, λ=1.000, dt=0.014s, elapsed=4.5m\n",
      "[Net8] Ep 18200: loss=0.023958, pde=0.011469, bc=0.012488, λ=1.000, dt=0.014s, elapsed=4.5m\n",
      "[Net8] Ep 18400: loss=0.023474, pde=0.011335, bc=0.012139, λ=1.000, dt=0.014s, elapsed=4.6m\n",
      "[Net8] Ep 18600: loss=0.022162, pde=0.010424, bc=0.011738, λ=1.000, dt=0.014s, elapsed=4.6m\n",
      "[Net8] Ep 18800: loss=0.023662, pde=0.010989, bc=0.012673, λ=1.000, dt=0.014s, elapsed=4.7m\n",
      "[Net8] Ep 19000: loss=0.022980, pde=0.011187, bc=0.011793, λ=1.000, dt=0.014s, elapsed=4.7m\n",
      "[Net8] Ep 19200: loss=0.023525, pde=0.011918, bc=0.011607, λ=1.000, dt=0.014s, elapsed=4.8m\n",
      "[Net8] Ep 19400: loss=0.024094, pde=0.011703, bc=0.012391, λ=1.000, dt=0.014s, elapsed=4.8m\n",
      "[Net8] Ep 19600: loss=0.021258, pde=0.010129, bc=0.011129, λ=1.000, dt=0.014s, elapsed=4.9m\n",
      "[Net8] Ep 19800: loss=0.023731, pde=0.011682, bc=0.012049, λ=1.000, dt=0.014s, elapsed=4.9m\n",
      "[Net8] Ep 20000: loss=0.021968, pde=0.010499, bc=0.011469, λ=1.000, dt=0.017s, elapsed=5.0m\n",
      "Net8 training complete in 5.0 min (final loss 0.021968)\n",
      "[Net9] Ep 1: loss=2597.014648, pde=2596.997559, bc=0.016976, λ=1.000, dt=0.015s, elapsed=0.0m\n",
      "[Net9] Ep 200: loss=37.304668, pde=27.383940, bc=9.920728, λ=1.000, dt=0.014s, elapsed=0.0m\n",
      "[Net9] Ep 400: loss=9.651303, pde=3.397774, bc=6.253530, λ=1.000, dt=0.013s, elapsed=0.1m\n",
      "[Net9] Ep 600: loss=5.315881, pde=2.431965, bc=2.883916, λ=1.000, dt=0.013s, elapsed=0.1m\n",
      "[Net9] Ep 800: loss=6.033238, pde=4.185715, bc=1.847523, λ=1.000, dt=0.013s, elapsed=0.2m\n",
      "[Net9] Ep 1000: loss=1.918751, pde=0.400351, bc=1.518400, λ=1.000, dt=0.012s, elapsed=0.2m\n",
      "[Net9] Ep 1200: loss=1.729618, pde=0.370556, bc=1.359062, λ=1.000, dt=0.015s, elapsed=0.3m\n",
      "[Net9] Ep 1400: loss=1.494059, pde=0.294912, bc=1.199146, λ=1.000, dt=0.014s, elapsed=0.3m\n",
      "[Net9] Ep 1600: loss=1.367690, pde=0.261637, bc=1.106053, λ=1.000, dt=0.013s, elapsed=0.4m\n",
      "[Net9] Ep 1800: loss=1.627012, pde=0.667459, bc=0.959553, λ=1.000, dt=0.019s, elapsed=0.4m\n",
      "[Net9] Ep 2000: loss=2.238827, pde=1.343185, bc=0.895642, λ=1.000, dt=0.012s, elapsed=0.5m\n",
      "[Net9] Ep 2200: loss=1.396533, pde=0.631540, bc=0.764993, λ=1.000, dt=0.013s, elapsed=0.5m\n",
      "[Net9] Ep 2400: loss=1.039109, pde=0.344647, bc=0.694461, λ=1.000, dt=0.013s, elapsed=0.6m\n",
      "[Net9] Ep 2600: loss=2.608644, pde=2.008350, bc=0.600294, λ=1.000, dt=0.012s, elapsed=0.6m\n",
      "[Net9] Ep 2800: loss=0.821988, pde=0.317176, bc=0.504812, λ=1.000, dt=0.014s, elapsed=0.6m\n",
      "[Net9] Ep 3000: loss=0.891119, pde=0.486583, bc=0.404536, λ=1.000, dt=0.016s, elapsed=0.7m\n",
      "[Net9] Ep 3200: loss=2.055972, pde=1.723228, bc=0.332744, λ=1.000, dt=0.012s, elapsed=0.7m\n",
      "[Net9] Ep 3400: loss=0.628686, pde=0.335927, bc=0.292760, λ=1.000, dt=0.013s, elapsed=0.8m\n",
      "[Net9] Ep 3600: loss=1.462633, pde=1.206542, bc=0.256090, λ=1.000, dt=0.012s, elapsed=0.8m\n",
      "[Net9] Ep 3800: loss=4.911799, pde=4.727766, bc=0.184033, λ=1.000, dt=0.016s, elapsed=0.9m\n",
      "[Net9] Ep 4000: loss=0.267057, pde=0.105365, bc=0.161692, λ=1.000, dt=0.012s, elapsed=0.9m\n",
      "[Net9] Ep 4200: loss=0.464318, pde=0.337104, bc=0.127215, λ=1.000, dt=0.012s, elapsed=1.0m\n",
      "[Net9] Ep 4400: loss=0.283761, pde=0.167229, bc=0.116532, λ=1.000, dt=0.013s, elapsed=1.0m\n",
      "[Net9] Ep 4600: loss=0.479084, pde=0.383554, bc=0.095530, λ=1.000, dt=0.012s, elapsed=1.0m\n",
      "[Net9] Ep 4800: loss=1.067796, pde=0.979054, bc=0.088743, λ=1.000, dt=0.013s, elapsed=1.1m\n",
      "[Net9] Ep 5000: loss=0.315360, pde=0.232946, bc=0.082413, λ=1.000, dt=0.013s, elapsed=1.1m\n",
      "[Net9] Ep 5200: loss=0.747178, pde=0.674636, bc=0.072542, λ=1.000, dt=0.014s, elapsed=1.2m\n",
      "[Net9] Ep 5400: loss=0.540321, pde=0.473596, bc=0.066725, λ=1.000, dt=0.012s, elapsed=1.2m\n",
      "[Net9] Ep 5600: loss=0.226861, pde=0.164396, bc=0.062466, λ=1.000, dt=0.015s, elapsed=1.3m\n",
      "[Net9] Ep 5800: loss=0.163285, pde=0.104694, bc=0.058590, λ=1.000, dt=0.014s, elapsed=1.3m\n",
      "[Net9] Ep 6000: loss=0.187788, pde=0.133361, bc=0.054428, λ=1.000, dt=0.012s, elapsed=1.4m\n",
      "[Net9] Ep 6200: loss=0.204315, pde=0.155398, bc=0.048916, λ=1.000, dt=0.013s, elapsed=1.4m\n",
      "[Net9] Ep 6400: loss=0.140956, pde=0.092699, bc=0.048257, λ=1.000, dt=0.013s, elapsed=1.5m\n",
      "[Net9] Ep 6600: loss=0.192889, pde=0.148941, bc=0.043948, λ=1.000, dt=0.013s, elapsed=1.5m\n",
      "[Net9] Ep 6800: loss=0.152935, pde=0.107300, bc=0.045636, λ=1.000, dt=0.012s, elapsed=1.6m\n",
      "[Net9] Ep 7000: loss=0.108095, pde=0.066866, bc=0.041229, λ=1.000, dt=0.013s, elapsed=1.6m\n",
      "[Net9] Ep 7200: loss=0.538599, pde=0.497397, bc=0.041202, λ=1.000, dt=0.014s, elapsed=1.6m\n",
      "[Net9] Ep 7400: loss=0.118743, pde=0.077510, bc=0.041233, λ=1.000, dt=0.020s, elapsed=1.7m\n",
      "[Net9] Ep 7600: loss=0.337038, pde=0.299709, bc=0.037329, λ=1.000, dt=0.013s, elapsed=1.7m\n",
      "[Net9] Ep 7800: loss=0.144078, pde=0.112346, bc=0.031732, λ=1.000, dt=0.013s, elapsed=1.8m\n",
      "[Net9] Ep 8000: loss=0.075861, pde=0.043722, bc=0.032139, λ=1.000, dt=0.015s, elapsed=1.8m\n",
      "[Net9] Ep 8200: loss=0.220954, pde=0.192568, bc=0.028385, λ=1.000, dt=0.016s, elapsed=1.9m\n",
      "[Net9] Ep 8400: loss=0.491574, pde=0.458153, bc=0.033422, λ=1.000, dt=0.012s, elapsed=1.9m\n",
      "[Net9] Ep 8600: loss=0.067940, pde=0.037536, bc=0.030404, λ=1.000, dt=0.013s, elapsed=2.0m\n",
      "[Net9] Ep 8800: loss=0.097853, pde=0.071301, bc=0.026552, λ=1.000, dt=0.013s, elapsed=2.0m\n",
      "[Net9] Ep 9000: loss=0.137780, pde=0.112676, bc=0.025104, λ=1.000, dt=0.012s, elapsed=2.1m\n",
      "[Net9] Ep 9200: loss=0.079495, pde=0.055628, bc=0.023868, λ=1.000, dt=0.013s, elapsed=2.1m\n",
      "[Net9] Ep 9400: loss=0.065647, pde=0.041538, bc=0.024108, λ=1.000, dt=0.012s, elapsed=2.2m\n",
      "[Net9] Ep 9600: loss=0.094155, pde=0.071202, bc=0.022953, λ=1.000, dt=0.014s, elapsed=2.2m\n",
      "[Net9] Ep 9800: loss=0.057895, pde=0.037965, bc=0.019930, λ=1.000, dt=0.013s, elapsed=2.2m\n",
      "[Net9] Ep 10000: loss=0.078246, pde=0.056872, bc=0.021374, λ=1.000, dt=0.015s, elapsed=2.3m\n",
      "[Net9] Ep 10200: loss=0.062226, pde=0.042784, bc=0.019443, λ=1.000, dt=0.012s, elapsed=2.3m\n",
      "[Net9] Ep 10400: loss=0.095615, pde=0.077865, bc=0.017750, λ=1.000, dt=0.013s, elapsed=2.4m\n",
      "[Net9] Ep 10600: loss=0.050802, pde=0.032142, bc=0.018660, λ=1.000, dt=0.013s, elapsed=2.4m\n",
      "[Net9] Ep 10800: loss=0.094747, pde=0.078344, bc=0.016403, λ=1.000, dt=0.012s, elapsed=2.5m\n",
      "[Net9] Ep 11000: loss=0.169275, pde=0.151108, bc=0.018167, λ=1.000, dt=0.013s, elapsed=2.5m\n",
      "[Net9] Ep 11200: loss=0.180877, pde=0.162439, bc=0.018437, λ=1.000, dt=0.013s, elapsed=2.6m\n",
      "[Net9] Ep 11400: loss=0.081947, pde=0.065704, bc=0.016243, λ=1.000, dt=0.012s, elapsed=2.6m\n",
      "[Net9] Ep 11600: loss=0.038495, pde=0.023130, bc=0.015366, λ=1.000, dt=0.013s, elapsed=2.7m\n",
      "[Net9] Ep 11800: loss=0.053516, pde=0.038935, bc=0.014581, λ=1.000, dt=0.013s, elapsed=2.7m\n",
      "[Net9] Ep 12000: loss=0.218977, pde=0.205056, bc=0.013921, λ=1.000, dt=0.014s, elapsed=2.7m\n",
      "[Net9] Ep 12200: loss=0.040715, pde=0.027338, bc=0.013377, λ=1.000, dt=0.013s, elapsed=2.8m\n",
      "[Net9] Ep 12400: loss=0.052045, pde=0.038996, bc=0.013049, λ=1.000, dt=0.013s, elapsed=2.8m\n",
      "[Net9] Ep 12600: loss=0.053989, pde=0.041229, bc=0.012760, λ=1.000, dt=0.016s, elapsed=2.9m\n",
      "[Net9] Ep 12800: loss=0.047846, pde=0.034985, bc=0.012861, λ=1.000, dt=0.012s, elapsed=2.9m\n",
      "[Net9] Ep 13000: loss=0.053016, pde=0.040831, bc=0.012185, λ=1.000, dt=0.012s, elapsed=3.0m\n",
      "[Net9] Ep 13200: loss=0.036046, pde=0.023614, bc=0.012432, λ=1.000, dt=0.012s, elapsed=3.0m\n",
      "[Net9] Ep 13400: loss=0.055715, pde=0.044940, bc=0.010776, λ=1.000, dt=0.012s, elapsed=3.1m\n",
      "[Net9] Ep 13600: loss=0.025834, pde=0.015145, bc=0.010689, λ=1.000, dt=0.013s, elapsed=3.1m\n",
      "[Net9] Ep 13800: loss=0.023797, pde=0.012803, bc=0.010994, λ=1.000, dt=0.012s, elapsed=3.2m\n",
      "[Net9] Ep 14000: loss=0.024808, pde=0.014813, bc=0.009995, λ=1.000, dt=0.012s, elapsed=3.2m\n",
      "[Net9] Ep 14200: loss=0.024315, pde=0.013973, bc=0.010342, λ=1.000, dt=0.013s, elapsed=3.2m\n",
      "[Net9] Ep 14400: loss=0.034720, pde=0.025574, bc=0.009146, λ=1.000, dt=0.015s, elapsed=3.3m\n",
      "[Net9] Ep 14600: loss=0.032178, pde=0.021944, bc=0.010234, λ=1.000, dt=0.012s, elapsed=3.3m\n",
      "[Net9] Ep 14800: loss=0.025225, pde=0.016332, bc=0.008893, λ=1.000, dt=0.012s, elapsed=3.4m\n",
      "[Net9] Ep 15000: loss=0.019968, pde=0.010307, bc=0.009661, λ=1.000, dt=0.012s, elapsed=3.4m\n",
      "[Net9] Ep 15200: loss=0.025044, pde=0.016272, bc=0.008772, λ=1.000, dt=0.013s, elapsed=3.5m\n",
      "[Net9] Ep 15400: loss=0.023127, pde=0.014493, bc=0.008634, λ=1.000, dt=0.013s, elapsed=3.5m\n",
      "[Net9] Ep 15600: loss=0.020752, pde=0.012778, bc=0.007975, λ=1.000, dt=0.012s, elapsed=3.6m\n",
      "[Net9] Ep 15800: loss=0.017748, pde=0.009411, bc=0.008337, λ=1.000, dt=0.013s, elapsed=3.6m\n",
      "[Net9] Ep 16000: loss=0.018561, pde=0.011181, bc=0.007380, λ=1.000, dt=0.013s, elapsed=3.6m\n",
      "[Net9] Ep 16200: loss=0.017672, pde=0.010057, bc=0.007616, λ=1.000, dt=0.015s, elapsed=3.7m\n",
      "[Net9] Ep 16400: loss=0.018363, pde=0.010556, bc=0.007807, λ=1.000, dt=0.012s, elapsed=3.7m\n",
      "[Net9] Ep 16600: loss=0.020141, pde=0.011864, bc=0.008277, λ=1.000, dt=0.013s, elapsed=3.8m\n",
      "[Net9] Ep 16800: loss=0.016710, pde=0.009122, bc=0.007588, λ=1.000, dt=0.018s, elapsed=3.8m\n",
      "[Net9] Ep 17000: loss=0.017803, pde=0.010139, bc=0.007664, λ=1.000, dt=0.013s, elapsed=3.9m\n",
      "[Net9] Ep 17200: loss=0.015380, pde=0.007502, bc=0.007878, λ=1.000, dt=0.012s, elapsed=3.9m\n",
      "[Net9] Ep 17400: loss=0.015994, pde=0.008464, bc=0.007531, λ=1.000, dt=0.012s, elapsed=4.0m\n",
      "[Net9] Ep 17600: loss=0.015680, pde=0.007941, bc=0.007739, λ=1.000, dt=0.012s, elapsed=4.0m\n",
      "[Net9] Ep 17800: loss=0.015830, pde=0.008489, bc=0.007341, λ=1.000, dt=0.013s, elapsed=4.0m\n",
      "[Net9] Ep 18000: loss=0.014360, pde=0.007340, bc=0.007020, λ=1.000, dt=0.018s, elapsed=4.1m\n",
      "[Net9] Ep 18200: loss=0.015753, pde=0.007837, bc=0.007916, λ=1.000, dt=0.012s, elapsed=4.1m\n",
      "[Net9] Ep 18400: loss=0.014759, pde=0.007665, bc=0.007094, λ=1.000, dt=0.013s, elapsed=4.2m\n",
      "[Net9] Ep 18600: loss=0.014166, pde=0.007597, bc=0.006569, λ=1.000, dt=0.013s, elapsed=4.2m\n",
      "[Net9] Ep 18800: loss=0.014664, pde=0.007125, bc=0.007539, λ=1.000, dt=0.012s, elapsed=4.3m\n",
      "[Net9] Ep 19000: loss=0.013966, pde=0.006784, bc=0.007181, λ=1.000, dt=0.012s, elapsed=4.3m\n",
      "[Net9] Ep 19200: loss=0.015188, pde=0.008189, bc=0.006999, λ=1.000, dt=0.012s, elapsed=4.4m\n",
      "[Net9] Ep 19400: loss=0.013480, pde=0.006306, bc=0.007174, λ=1.000, dt=0.012s, elapsed=4.4m\n",
      "[Net9] Ep 19600: loss=0.012637, pde=0.005953, bc=0.006684, λ=1.000, dt=0.013s, elapsed=4.4m\n",
      "[Net9] Ep 19800: loss=0.013799, pde=0.006634, bc=0.007165, λ=1.000, dt=0.022s, elapsed=4.5m\n",
      "[Net9] Ep 20000: loss=0.013257, pde=0.006012, bc=0.007245, λ=1.000, dt=0.013s, elapsed=4.5m\n",
      "Net9 training complete in 4.5 min (final loss 0.013257)\n",
      "[Net10] Ep 1: loss=2597.274658, pde=2597.261719, bc=0.012914, λ=1.000, dt=0.094s, elapsed=0.0m\n",
      "[Net10] Ep 200: loss=1684.875244, pde=1651.584351, bc=33.290943, λ=1.000, dt=0.008s, elapsed=0.0m\n",
      "[Net10] Ep 400: loss=1594.999756, pde=1561.520142, bc=33.479633, λ=1.000, dt=0.010s, elapsed=0.1m\n",
      "[Net10] Ep 600: loss=1263.767700, pde=1226.824585, bc=36.943157, λ=1.000, dt=0.008s, elapsed=0.1m\n",
      "[Net10] Ep 800: loss=1329.659058, pde=1290.913086, bc=38.745972, λ=1.000, dt=0.008s, elapsed=0.1m\n",
      "[Net10] Ep 1000: loss=1335.911133, pde=1301.498291, bc=34.412827, λ=1.000, dt=0.010s, elapsed=0.1m\n",
      "[Net10] Ep 1200: loss=1627.087036, pde=1587.704346, bc=39.382664, λ=1.000, dt=0.009s, elapsed=0.2m\n",
      "[Net10] Ep 1400: loss=1307.266724, pde=1273.795288, bc=33.471397, λ=1.000, dt=0.008s, elapsed=0.2m\n",
      "[Net10] Ep 1600: loss=1310.841309, pde=1279.867310, bc=30.973959, λ=1.000, dt=0.008s, elapsed=0.2m\n",
      "[Net10] Ep 1800: loss=1150.236206, pde=1126.436401, bc=23.799862, λ=1.000, dt=0.008s, elapsed=0.3m\n",
      "[Net10] Ep 2000: loss=1252.720947, pde=1226.807495, bc=25.913412, λ=1.000, dt=0.009s, elapsed=0.3m\n",
      "[Net10] Ep 2200: loss=1191.111328, pde=1167.311401, bc=23.799891, λ=1.000, dt=0.008s, elapsed=0.3m\n",
      "[Net10] Ep 2400: loss=1226.654419, pde=1205.240479, bc=21.413900, λ=1.000, dt=0.014s, elapsed=0.4m\n",
      "[Net10] Ep 2600: loss=1213.377686, pde=1186.172974, bc=27.204752, λ=1.000, dt=0.008s, elapsed=0.4m\n",
      "[Net10] Ep 2800: loss=1256.312988, pde=1232.526855, bc=23.786171, λ=1.000, dt=0.008s, elapsed=0.4m\n",
      "[Net10] Ep 3000: loss=1260.023682, pde=1237.700439, bc=22.323296, λ=1.000, dt=0.009s, elapsed=0.4m\n",
      "[Net10] Ep 3200: loss=1212.359131, pde=1191.448975, bc=20.910170, λ=1.000, dt=0.009s, elapsed=0.5m\n",
      "[Net10] Ep 3400: loss=1269.430176, pde=1246.154541, bc=23.275642, λ=1.000, dt=0.008s, elapsed=0.5m\n",
      "[Net10] Ep 3600: loss=1212.475586, pde=1191.228149, bc=21.247375, λ=1.000, dt=0.010s, elapsed=0.5m\n",
      "[Net10] Ep 3800: loss=1175.976562, pde=1161.690308, bc=14.286233, λ=1.000, dt=0.008s, elapsed=0.6m\n",
      "[Net10] Ep 4000: loss=1204.408813, pde=1186.465332, bc=17.943539, λ=1.000, dt=0.008s, elapsed=0.6m\n",
      "[Net10] Ep 4200: loss=1254.081055, pde=1239.266846, bc=14.814153, λ=1.000, dt=0.008s, elapsed=0.6m\n",
      "[Net10] Ep 4400: loss=1138.135376, pde=1122.367188, bc=15.768238, λ=1.000, dt=0.008s, elapsed=0.7m\n",
      "[Net10] Ep 4600: loss=1161.325684, pde=1143.045898, bc=18.279755, λ=1.000, dt=0.008s, elapsed=0.7m\n",
      "[Net10] Ep 4800: loss=1206.842407, pde=1191.121338, bc=15.721065, λ=1.000, dt=0.008s, elapsed=0.7m\n",
      "[Net10] Ep 5000: loss=1248.006836, pde=1231.759644, bc=16.247248, λ=1.000, dt=0.010s, elapsed=0.7m\n",
      "[Net10] Ep 5200: loss=1180.780029, pde=1163.811035, bc=16.968988, λ=1.000, dt=0.008s, elapsed=0.8m\n",
      "[Net10] Ep 5400: loss=1140.167969, pde=1124.255127, bc=15.912868, λ=1.000, dt=0.008s, elapsed=0.8m\n",
      "[Net10] Ep 5600: loss=1180.166748, pde=1166.811768, bc=13.354934, λ=1.000, dt=0.008s, elapsed=0.8m\n",
      "[Net10] Ep 5800: loss=1126.896484, pde=1116.243774, bc=10.652741, λ=1.000, dt=0.008s, elapsed=0.9m\n",
      "[Net10] Ep 6000: loss=1232.896606, pde=1219.722290, bc=13.174351, λ=1.000, dt=0.009s, elapsed=0.9m\n",
      "[Net10] Ep 6200: loss=1198.227783, pde=1185.516724, bc=12.711052, λ=1.000, dt=0.010s, elapsed=0.9m\n",
      "[Net10] Ep 6400: loss=1166.158325, pde=1153.966187, bc=12.192161, λ=1.000, dt=0.013s, elapsed=1.0m\n",
      "[Net10] Ep 6600: loss=1160.005981, pde=1147.503784, bc=12.502185, λ=1.000, dt=0.009s, elapsed=1.0m\n",
      "[Net10] Ep 6800: loss=1272.945923, pde=1257.323608, bc=15.622319, λ=1.000, dt=0.008s, elapsed=1.0m\n",
      "[Net10] Ep 7000: loss=1233.050537, pde=1218.714111, bc=14.336437, λ=1.000, dt=0.008s, elapsed=1.1m\n",
      "[Net10] Ep 7200: loss=1260.021729, pde=1246.536987, bc=13.484753, λ=1.000, dt=0.009s, elapsed=1.1m\n",
      "[Net10] Ep 7400: loss=1216.052124, pde=1202.459106, bc=13.593063, λ=1.000, dt=0.008s, elapsed=1.1m\n",
      "[Net10] Ep 7600: loss=1081.797119, pde=1071.930664, bc=9.866445, λ=1.000, dt=0.008s, elapsed=1.1m\n",
      "[Net10] Ep 7800: loss=1121.936646, pde=1108.791992, bc=13.144711, λ=1.000, dt=0.008s, elapsed=1.2m\n",
      "[Net10] Ep 8000: loss=1346.887085, pde=1327.378540, bc=19.508583, λ=1.000, dt=0.008s, elapsed=1.2m\n",
      "[Net10] Ep 8200: loss=1265.107422, pde=1249.529297, bc=15.578071, λ=1.000, dt=0.008s, elapsed=1.2m\n",
      "[Net10] Ep 8400: loss=1282.902344, pde=1273.682129, bc=9.220168, λ=1.000, dt=0.008s, elapsed=1.3m\n",
      "[Net10] Ep 8600: loss=1228.940674, pde=1220.042236, bc=8.898380, λ=1.000, dt=0.009s, elapsed=1.3m\n",
      "[Net10] Ep 8800: loss=1159.582031, pde=1148.725464, bc=10.856622, λ=1.000, dt=0.008s, elapsed=1.3m\n",
      "[Net10] Ep 9000: loss=1182.245361, pde=1171.066406, bc=11.178994, λ=1.000, dt=0.010s, elapsed=1.3m\n",
      "[Net10] Ep 9200: loss=1182.027832, pde=1170.970703, bc=11.057070, λ=1.000, dt=0.008s, elapsed=1.4m\n",
      "[Net10] Ep 9400: loss=1169.679688, pde=1161.869629, bc=7.810112, λ=1.000, dt=0.013s, elapsed=1.4m\n",
      "[Net10] Ep 9600: loss=1139.444336, pde=1125.518188, bc=13.926119, λ=1.000, dt=0.009s, elapsed=1.4m\n",
      "[Net10] Ep 9800: loss=1222.987915, pde=1214.388184, bc=8.599742, λ=1.000, dt=0.010s, elapsed=1.5m\n",
      "[Net10] Ep 10000: loss=1172.048462, pde=1164.840332, bc=7.208076, λ=1.000, dt=0.013s, elapsed=1.5m\n",
      "[Net10] Ep 10200: loss=1173.984497, pde=1160.059326, bc=13.925123, λ=1.000, dt=0.008s, elapsed=1.5m\n",
      "[Net10] Ep 10400: loss=1257.203125, pde=1248.328369, bc=8.874773, λ=1.000, dt=0.010s, elapsed=1.6m\n",
      "[Net10] Ep 10600: loss=1237.811646, pde=1223.542114, bc=14.269584, λ=1.000, dt=0.008s, elapsed=1.6m\n",
      "[Net10] Ep 10800: loss=1226.541992, pde=1217.390015, bc=9.152039, λ=1.000, dt=0.008s, elapsed=1.6m\n",
      "[Net10] Ep 11000: loss=1287.270630, pde=1280.454956, bc=6.815692, λ=1.000, dt=0.008s, elapsed=1.6m\n",
      "[Net10] Ep 11200: loss=1187.989258, pde=1178.059692, bc=9.929572, λ=1.000, dt=0.008s, elapsed=1.7m\n",
      "[Net10] Ep 11400: loss=1235.434082, pde=1228.815552, bc=6.618579, λ=1.000, dt=0.008s, elapsed=1.7m\n",
      "[Net10] Ep 11600: loss=1246.916992, pde=1237.551880, bc=9.365063, λ=1.000, dt=0.008s, elapsed=1.7m\n",
      "[Net10] Ep 11800: loss=1270.366577, pde=1261.390991, bc=8.975616, λ=1.000, dt=0.012s, elapsed=1.8m\n",
      "[Net10] Ep 12000: loss=1282.797852, pde=1273.044312, bc=9.753507, λ=1.000, dt=0.008s, elapsed=1.8m\n",
      "[Net10] Ep 12200: loss=1260.161255, pde=1252.516846, bc=7.644402, λ=1.000, dt=0.008s, elapsed=1.8m\n",
      "[Net10] Ep 12400: loss=1204.387207, pde=1197.488037, bc=6.899189, λ=1.000, dt=0.010s, elapsed=1.8m\n",
      "[Net10] Ep 12600: loss=1222.939819, pde=1210.305664, bc=12.634196, λ=1.000, dt=0.008s, elapsed=1.9m\n",
      "[Net10] Ep 12800: loss=1146.814209, pde=1138.668457, bc=8.145695, λ=1.000, dt=0.008s, elapsed=1.9m\n",
      "[Net10] Ep 13000: loss=1193.967529, pde=1182.897583, bc=11.069943, λ=1.000, dt=0.008s, elapsed=1.9m\n",
      "[Net10] Ep 13200: loss=1283.339844, pde=1273.894287, bc=9.445531, λ=1.000, dt=0.013s, elapsed=2.0m\n",
      "[Net10] Ep 13400: loss=1337.630005, pde=1331.048462, bc=6.581561, λ=1.000, dt=0.008s, elapsed=2.0m\n",
      "[Net10] Ep 13600: loss=1268.797119, pde=1262.594727, bc=6.202425, λ=1.000, dt=0.008s, elapsed=2.0m\n",
      "[Net10] Ep 13800: loss=1188.125244, pde=1181.266235, bc=6.859037, λ=1.000, dt=0.008s, elapsed=2.1m\n",
      "[Net10] Ep 14000: loss=1167.021484, pde=1156.538574, bc=10.482873, λ=1.000, dt=0.008s, elapsed=2.1m\n",
      "[Net10] Ep 14200: loss=1286.979492, pde=1278.505859, bc=8.473667, λ=1.000, dt=0.008s, elapsed=2.1m\n",
      "[Net10] Ep 14400: loss=1255.983887, pde=1249.456665, bc=6.527194, λ=1.000, dt=0.010s, elapsed=2.1m\n",
      "[Net10] Ep 14600: loss=1201.158081, pde=1193.886353, bc=7.271684, λ=1.000, dt=0.008s, elapsed=2.2m\n",
      "[Net10] Ep 14800: loss=1279.359863, pde=1271.282959, bc=8.076910, λ=1.000, dt=0.008s, elapsed=2.2m\n",
      "[Net10] Ep 15000: loss=1166.643677, pde=1159.474976, bc=7.168741, λ=1.000, dt=0.008s, elapsed=2.2m\n",
      "[Net10] Ep 15200: loss=1113.518799, pde=1105.345825, bc=8.172999, λ=1.000, dt=0.009s, elapsed=2.3m\n",
      "[Net10] Ep 15400: loss=1169.477905, pde=1163.637939, bc=5.839971, λ=1.000, dt=0.008s, elapsed=2.3m\n",
      "[Net10] Ep 15600: loss=1218.289307, pde=1210.640991, bc=7.648374, λ=1.000, dt=0.008s, elapsed=2.3m\n",
      "[Net10] Ep 15800: loss=1087.011475, pde=1080.538818, bc=6.472690, λ=1.000, dt=0.010s, elapsed=2.4m\n",
      "[Net10] Ep 16000: loss=1239.824951, pde=1232.610962, bc=7.214007, λ=1.000, dt=0.008s, elapsed=2.4m\n",
      "[Net10] Ep 16200: loss=1136.414551, pde=1129.234131, bc=7.180462, λ=1.000, dt=0.008s, elapsed=2.4m\n",
      "[Net10] Ep 16400: loss=1117.861328, pde=1112.822266, bc=5.039029, λ=1.000, dt=0.008s, elapsed=2.4m\n",
      "[Net10] Ep 16600: loss=1116.335327, pde=1109.557617, bc=6.777730, λ=1.000, dt=0.008s, elapsed=2.5m\n",
      "[Net10] Ep 16800: loss=1099.125000, pde=1093.298706, bc=5.826321, λ=1.000, dt=0.008s, elapsed=2.5m\n",
      "[Net10] Ep 17000: loss=1185.547485, pde=1176.378296, bc=9.169238, λ=1.000, dt=0.008s, elapsed=2.5m\n",
      "[Net10] Ep 17200: loss=1098.829712, pde=1091.394409, bc=7.435336, λ=1.000, dt=0.010s, elapsed=2.6m\n",
      "[Net10] Ep 17400: loss=1199.879272, pde=1193.152466, bc=6.726826, λ=1.000, dt=0.009s, elapsed=2.6m\n",
      "[Net10] Ep 17600: loss=1354.827881, pde=1348.678589, bc=6.149269, λ=1.000, dt=0.008s, elapsed=2.6m\n",
      "[Net10] Ep 17800: loss=1195.409790, pde=1189.435547, bc=5.974220, λ=1.000, dt=0.008s, elapsed=2.6m\n",
      "[Net10] Ep 18000: loss=1153.177002, pde=1145.349487, bc=7.827510, λ=1.000, dt=0.008s, elapsed=2.7m\n",
      "[Net10] Ep 18200: loss=1162.817261, pde=1156.743774, bc=6.073544, λ=1.000, dt=0.009s, elapsed=2.7m\n",
      "[Net10] Ep 18400: loss=1342.980225, pde=1337.093506, bc=5.886759, λ=1.000, dt=0.008s, elapsed=2.7m\n",
      "[Net10] Ep 18600: loss=1160.630371, pde=1155.171265, bc=5.459124, λ=1.000, dt=0.013s, elapsed=2.8m\n",
      "[Net10] Ep 18800: loss=1246.233154, pde=1240.004272, bc=6.228923, λ=1.000, dt=0.008s, elapsed=2.8m\n",
      "[Net10] Ep 19000: loss=1181.886597, pde=1174.895142, bc=6.991471, λ=1.000, dt=0.008s, elapsed=2.8m\n",
      "[Net10] Ep 19200: loss=1125.551147, pde=1119.233765, bc=6.317415, λ=1.000, dt=0.008s, elapsed=2.9m\n",
      "[Net10] Ep 19400: loss=1177.807861, pde=1171.773560, bc=6.034309, λ=1.000, dt=0.008s, elapsed=2.9m\n",
      "[Net10] Ep 19600: loss=1187.656128, pde=1181.460815, bc=6.195265, λ=1.000, dt=0.008s, elapsed=2.9m\n",
      "[Net10] Ep 19800: loss=1343.450806, pde=1336.747559, bc=6.703252, λ=1.000, dt=0.011s, elapsed=2.9m\n",
      "[Net10] Ep 20000: loss=1285.512573, pde=1280.332520, bc=5.180048, λ=1.000, dt=0.008s, elapsed=3.0m\n",
      "Net10 training complete in 3.0 min (final loss 1285.512573)\n",
      "[Net11] Ep 1: loss=2597.071289, pde=2597.069092, bc=0.002189, λ=1.000, dt=0.028s, elapsed=0.0m\n",
      "[Net11] Ep 200: loss=6.765602, pde=4.424163, bc=2.341439, λ=1.000, dt=0.019s, elapsed=0.1m\n",
      "[Net11] Ep 400: loss=16.745928, pde=15.005482, bc=1.740446, λ=1.000, dt=0.020s, elapsed=0.1m\n",
      "[Net11] Ep 600: loss=2.936384, pde=1.633488, bc=1.302896, λ=1.000, dt=0.019s, elapsed=0.2m\n",
      "[Net11] Ep 800: loss=4.245499, pde=3.216052, bc=1.029447, λ=1.000, dt=0.020s, elapsed=0.3m\n",
      "[Net11] Ep 1000: loss=1.754158, pde=0.893760, bc=0.860398, λ=1.000, dt=0.021s, elapsed=0.3m\n",
      "[Net11] Ep 1200: loss=1.585124, pde=0.887258, bc=0.697866, λ=1.000, dt=0.019s, elapsed=0.4m\n",
      "[Net11] Ep 1400: loss=1.301615, pde=0.725092, bc=0.576523, λ=1.000, dt=0.019s, elapsed=0.5m\n",
      "[Net11] Ep 1600: loss=4.398864, pde=3.872202, bc=0.526661, λ=1.000, dt=0.019s, elapsed=0.5m\n",
      "[Net11] Ep 1800: loss=3.456857, pde=3.002585, bc=0.454272, λ=1.000, dt=0.019s, elapsed=0.6m\n",
      "[Net11] Ep 2000: loss=2.034286, pde=1.623175, bc=0.411111, λ=1.000, dt=0.019s, elapsed=0.7m\n",
      "[Net11] Ep 2200: loss=8.633132, pde=8.295162, bc=0.337969, λ=1.000, dt=0.019s, elapsed=0.7m\n",
      "[Net11] Ep 2400: loss=0.490979, pde=0.124636, bc=0.366343, λ=1.000, dt=0.020s, elapsed=0.8m\n",
      "[Net11] Ep 2600: loss=0.449070, pde=0.162661, bc=0.286409, λ=1.000, dt=0.019s, elapsed=0.9m\n",
      "[Net11] Ep 2800: loss=1.155288, pde=0.681523, bc=0.473765, λ=1.000, dt=0.019s, elapsed=0.9m\n",
      "[Net11] Ep 3000: loss=0.402334, pde=0.091662, bc=0.310672, λ=1.000, dt=0.019s, elapsed=1.0m\n",
      "[Net11] Ep 3200: loss=0.374653, pde=0.127064, bc=0.247589, λ=1.000, dt=0.020s, elapsed=1.1m\n",
      "[Net11] Ep 3400: loss=0.317171, pde=0.085144, bc=0.232027, λ=1.000, dt=0.019s, elapsed=1.2m\n",
      "[Net11] Ep 3600: loss=1.461090, pde=1.261198, bc=0.199892, λ=1.000, dt=0.020s, elapsed=1.2m\n",
      "[Net11] Ep 3800: loss=1.752451, pde=1.580104, bc=0.172347, λ=1.000, dt=0.019s, elapsed=1.3m\n",
      "[Net11] Ep 4000: loss=2.466789, pde=2.290107, bc=0.176682, λ=1.000, dt=0.019s, elapsed=1.4m\n",
      "[Net11] Ep 4200: loss=0.273060, pde=0.149215, bc=0.123845, λ=1.000, dt=0.019s, elapsed=1.4m\n",
      "[Net11] Ep 4400: loss=0.360687, pde=0.189127, bc=0.171560, λ=1.000, dt=0.019s, elapsed=1.5m\n",
      "[Net11] Ep 4600: loss=0.799239, pde=0.676383, bc=0.122857, λ=1.000, dt=0.019s, elapsed=1.6m\n",
      "[Net11] Ep 4800: loss=1.860542, pde=1.747668, bc=0.112874, λ=1.000, dt=0.019s, elapsed=1.6m\n",
      "[Net11] Ep 5000: loss=1.983566, pde=1.881473, bc=0.102092, λ=1.000, dt=0.022s, elapsed=1.7m\n",
      "[Net11] Ep 5200: loss=3.040761, pde=2.943376, bc=0.097386, λ=1.000, dt=0.030s, elapsed=1.8m\n",
      "[Net11] Ep 5400: loss=2.041045, pde=1.949759, bc=0.091286, λ=1.000, dt=0.024s, elapsed=1.8m\n",
      "[Net11] Ep 5600: loss=0.190910, pde=0.104901, bc=0.086009, λ=1.000, dt=0.019s, elapsed=1.9m\n",
      "[Net11] Ep 5800: loss=0.236130, pde=0.156013, bc=0.080117, λ=1.000, dt=0.025s, elapsed=2.0m\n",
      "[Net11] Ep 6000: loss=0.190056, pde=0.120525, bc=0.069531, λ=1.000, dt=0.019s, elapsed=2.0m\n",
      "[Net11] Ep 6200: loss=0.204679, pde=0.132931, bc=0.071748, λ=1.000, dt=0.019s, elapsed=2.1m\n",
      "[Net11] Ep 6400: loss=1.956059, pde=1.900191, bc=0.055868, λ=1.000, dt=0.024s, elapsed=2.2m\n",
      "[Net11] Ep 6600: loss=1.733333, pde=1.676341, bc=0.056992, λ=1.000, dt=0.018s, elapsed=2.2m\n",
      "[Net11] Ep 6800: loss=0.659474, pde=0.591268, bc=0.068206, λ=1.000, dt=0.019s, elapsed=2.3m\n",
      "[Net11] Ep 7000: loss=3.098606, pde=3.046762, bc=0.051844, λ=1.000, dt=0.026s, elapsed=2.4m\n",
      "[Net11] Ep 7200: loss=0.263383, pde=0.216271, bc=0.047111, λ=1.000, dt=0.019s, elapsed=2.5m\n",
      "[Net11] Ep 7400: loss=0.588921, pde=0.537578, bc=0.051343, λ=1.000, dt=0.019s, elapsed=2.5m\n",
      "[Net11] Ep 7600: loss=5.903148, pde=5.856747, bc=0.046401, λ=1.000, dt=0.028s, elapsed=2.6m\n",
      "[Net11] Ep 7800: loss=0.083307, pde=0.046626, bc=0.036681, λ=1.000, dt=0.019s, elapsed=2.7m\n",
      "[Net11] Ep 8000: loss=0.433302, pde=0.399061, bc=0.034241, λ=1.000, dt=0.019s, elapsed=2.7m\n",
      "[Net11] Ep 8200: loss=0.138439, pde=0.107334, bc=0.031105, λ=1.000, dt=0.020s, elapsed=2.8m\n",
      "[Net11] Ep 8400: loss=0.214462, pde=0.177822, bc=0.036640, λ=1.000, dt=0.019s, elapsed=2.9m\n",
      "[Net11] Ep 8600: loss=0.549448, pde=0.517200, bc=0.032248, λ=1.000, dt=0.019s, elapsed=2.9m\n",
      "[Net11] Ep 8800: loss=0.605263, pde=0.578411, bc=0.026851, λ=1.000, dt=0.019s, elapsed=3.0m\n",
      "[Net11] Ep 9000: loss=0.058733, pde=0.029238, bc=0.029495, λ=1.000, dt=0.020s, elapsed=3.1m\n",
      "[Net11] Ep 9200: loss=0.850223, pde=0.821061, bc=0.029162, λ=1.000, dt=0.019s, elapsed=3.1m\n",
      "[Net11] Ep 9400: loss=0.128196, pde=0.104098, bc=0.024098, λ=1.000, dt=0.019s, elapsed=3.2m\n",
      "[Net11] Ep 9600: loss=0.072086, pde=0.048398, bc=0.023688, λ=1.000, dt=0.025s, elapsed=3.3m\n",
      "[Net11] Ep 9800: loss=0.133293, pde=0.112219, bc=0.021075, λ=1.000, dt=0.019s, elapsed=3.3m\n",
      "[Net11] Ep 10000: loss=0.304754, pde=0.282700, bc=0.022054, λ=1.000, dt=0.019s, elapsed=3.4m\n",
      "[Net11] Ep 10200: loss=0.062919, pde=0.042853, bc=0.020066, λ=1.000, dt=0.019s, elapsed=3.5m\n",
      "[Net11] Ep 10400: loss=0.234047, pde=0.215777, bc=0.018270, λ=1.000, dt=0.019s, elapsed=3.5m\n",
      "[Net11] Ep 10600: loss=0.523529, pde=0.504937, bc=0.018591, λ=1.000, dt=0.019s, elapsed=3.6m\n",
      "[Net11] Ep 10800: loss=0.438683, pde=0.419206, bc=0.019477, λ=1.000, dt=0.019s, elapsed=3.7m\n",
      "[Net11] Ep 11000: loss=0.162615, pde=0.144565, bc=0.018050, λ=1.000, dt=0.019s, elapsed=3.7m\n",
      "[Net11] Ep 11200: loss=0.164333, pde=0.145015, bc=0.019318, λ=1.000, dt=0.020s, elapsed=3.8m\n",
      "[Net11] Ep 11400: loss=0.167564, pde=0.151235, bc=0.016329, λ=1.000, dt=0.019s, elapsed=3.9m\n",
      "[Net11] Ep 11600: loss=0.149911, pde=0.134158, bc=0.015753, λ=1.000, dt=0.019s, elapsed=4.0m\n",
      "[Net11] Ep 11800: loss=0.146025, pde=0.130102, bc=0.015923, λ=1.000, dt=0.020s, elapsed=4.0m\n",
      "[Net11] Ep 12000: loss=0.067426, pde=0.053120, bc=0.014306, λ=1.000, dt=0.020s, elapsed=4.1m\n",
      "[Net11] Ep 12200: loss=0.057672, pde=0.043101, bc=0.014571, λ=1.000, dt=0.019s, elapsed=4.2m\n",
      "[Net11] Ep 12400: loss=0.047354, pde=0.034574, bc=0.012780, λ=1.000, dt=0.019s, elapsed=4.2m\n",
      "[Net11] Ep 12600: loss=0.071516, pde=0.058245, bc=0.013270, λ=1.000, dt=0.019s, elapsed=4.3m\n",
      "[Net11] Ep 12800: loss=0.029775, pde=0.016689, bc=0.013086, λ=1.000, dt=0.019s, elapsed=4.4m\n",
      "[Net11] Ep 13000: loss=0.080125, pde=0.067128, bc=0.012997, λ=1.000, dt=0.019s, elapsed=4.4m\n",
      "[Net11] Ep 13200: loss=0.025270, pde=0.012169, bc=0.013101, λ=1.000, dt=0.028s, elapsed=4.5m\n",
      "[Net11] Ep 13400: loss=0.027012, pde=0.014686, bc=0.012327, λ=1.000, dt=0.030s, elapsed=4.6m\n",
      "[Net11] Ep 13600: loss=0.061742, pde=0.049595, bc=0.012147, λ=1.000, dt=0.020s, elapsed=4.6m\n",
      "[Net11] Ep 13800: loss=0.026454, pde=0.014773, bc=0.011681, λ=1.000, dt=0.019s, elapsed=4.7m\n",
      "[Net11] Ep 14000: loss=0.045455, pde=0.034535, bc=0.010920, λ=1.000, dt=0.024s, elapsed=4.8m\n",
      "[Net11] Ep 14200: loss=0.031552, pde=0.020300, bc=0.011251, λ=1.000, dt=0.019s, elapsed=4.8m\n",
      "[Net11] Ep 14400: loss=0.022115, pde=0.011762, bc=0.010353, λ=1.000, dt=0.019s, elapsed=4.9m\n",
      "[Net11] Ep 14600: loss=0.019841, pde=0.009043, bc=0.010798, λ=1.000, dt=0.024s, elapsed=5.0m\n",
      "[Net11] Ep 14800: loss=0.022921, pde=0.012652, bc=0.010268, λ=1.000, dt=0.019s, elapsed=5.0m\n",
      "[Net11] Ep 15000: loss=0.021673, pde=0.011290, bc=0.010383, λ=1.000, dt=0.019s, elapsed=5.1m\n",
      "[Net11] Ep 15200: loss=0.017824, pde=0.008648, bc=0.009176, λ=1.000, dt=0.024s, elapsed=5.2m\n",
      "[Net11] Ep 15400: loss=0.018768, pde=0.009136, bc=0.009632, λ=1.000, dt=0.019s, elapsed=5.3m\n",
      "[Net11] Ep 15600: loss=0.022719, pde=0.013558, bc=0.009161, λ=1.000, dt=0.019s, elapsed=5.3m\n",
      "[Net11] Ep 15800: loss=0.019423, pde=0.010284, bc=0.009139, λ=1.000, dt=0.027s, elapsed=5.4m\n",
      "[Net11] Ep 16000: loss=0.015530, pde=0.006804, bc=0.008726, λ=1.000, dt=0.019s, elapsed=5.5m\n",
      "[Net11] Ep 16200: loss=0.015814, pde=0.007445, bc=0.008369, λ=1.000, dt=0.019s, elapsed=5.5m\n",
      "[Net11] Ep 16400: loss=0.015892, pde=0.007261, bc=0.008631, λ=1.000, dt=0.019s, elapsed=5.6m\n",
      "[Net11] Ep 16600: loss=0.015866, pde=0.006654, bc=0.009212, λ=1.000, dt=0.019s, elapsed=5.7m\n",
      "[Net11] Ep 16800: loss=0.016888, pde=0.008699, bc=0.008189, λ=1.000, dt=0.019s, elapsed=5.7m\n",
      "[Net11] Ep 17000: loss=0.015096, pde=0.006782, bc=0.008314, λ=1.000, dt=0.019s, elapsed=5.8m\n",
      "[Net11] Ep 17200: loss=0.013682, pde=0.005010, bc=0.008672, λ=1.000, dt=0.019s, elapsed=5.9m\n",
      "[Net11] Ep 17400: loss=0.013635, pde=0.005458, bc=0.008177, λ=1.000, dt=0.020s, elapsed=5.9m\n",
      "[Net11] Ep 17600: loss=0.014225, pde=0.005949, bc=0.008276, λ=1.000, dt=0.019s, elapsed=6.0m\n",
      "[Net11] Ep 17800: loss=0.013190, pde=0.005240, bc=0.007950, λ=1.000, dt=0.019s, elapsed=6.1m\n",
      "[Net11] Ep 18000: loss=0.013150, pde=0.005533, bc=0.007617, λ=1.000, dt=0.019s, elapsed=6.1m\n",
      "[Net11] Ep 18200: loss=0.013958, pde=0.005766, bc=0.008192, λ=1.000, dt=0.019s, elapsed=6.2m\n",
      "[Net11] Ep 18400: loss=0.011834, pde=0.004124, bc=0.007710, λ=1.000, dt=0.019s, elapsed=6.3m\n",
      "[Net11] Ep 18600: loss=0.012080, pde=0.004832, bc=0.007247, λ=1.000, dt=0.024s, elapsed=6.3m\n",
      "[Net11] Ep 18800: loss=0.012677, pde=0.004507, bc=0.008170, λ=1.000, dt=0.021s, elapsed=6.4m\n",
      "[Net11] Ep 19000: loss=0.012108, pde=0.004568, bc=0.007540, λ=1.000, dt=0.021s, elapsed=6.5m\n",
      "[Net11] Ep 19200: loss=0.013045, pde=0.005769, bc=0.007276, λ=1.000, dt=0.031s, elapsed=6.6m\n",
      "[Net11] Ep 19400: loss=0.012112, pde=0.004139, bc=0.007973, λ=1.000, dt=0.019s, elapsed=6.6m\n",
      "[Net11] Ep 19600: loss=0.011297, pde=0.004100, bc=0.007197, λ=1.000, dt=0.019s, elapsed=6.7m\n",
      "[Net11] Ep 19800: loss=0.011685, pde=0.003761, bc=0.007924, λ=1.000, dt=0.026s, elapsed=6.8m\n",
      "[Net11] Ep 20000: loss=0.011531, pde=0.004037, bc=0.007494, λ=1.000, dt=0.020s, elapsed=6.8m\n",
      "Net11 training complete in 6.8 min (final loss 0.011531)\n",
      "[Net12] Ep 1: loss=2597.149902, pde=2597.149658, bc=0.000247, λ=1.000, dt=0.026s, elapsed=0.0m\n",
      "[Net12] Ep 200: loss=1453.734985, pde=1453.438965, bc=0.296064, λ=1.000, dt=0.020s, elapsed=0.1m\n",
      "[Net12] Ep 400: loss=137.289017, pde=127.805397, bc=9.483618, λ=1.000, dt=0.035s, elapsed=0.2m\n",
      "[Net12] Ep 600: loss=49.118637, pde=34.902206, bc=14.216429, λ=1.000, dt=0.021s, elapsed=0.2m\n",
      "[Net12] Ep 800: loss=41.574730, pde=27.208031, bc=14.366697, λ=1.000, dt=0.022s, elapsed=0.3m\n",
      "[Net12] Ep 1000: loss=23.076830, pde=11.144678, bc=11.932152, λ=1.000, dt=0.021s, elapsed=0.4m\n",
      "[Net12] Ep 1200: loss=12.997630, pde=5.080323, bc=7.917307, λ=1.000, dt=0.021s, elapsed=0.4m\n",
      "[Net12] Ep 1400: loss=12.754531, pde=6.604487, bc=6.150044, λ=1.000, dt=0.021s, elapsed=0.5m\n",
      "[Net12] Ep 1600: loss=7.640549, pde=3.270202, bc=4.370347, λ=1.000, dt=0.020s, elapsed=0.6m\n",
      "[Net12] Ep 1800: loss=12.915306, pde=9.374436, bc=3.540870, λ=1.000, dt=0.020s, elapsed=0.7m\n",
      "[Net12] Ep 2000: loss=5.132552, pde=2.518760, bc=2.613791, λ=1.000, dt=0.035s, elapsed=0.7m\n",
      "[Net12] Ep 2200: loss=5.649742, pde=3.592940, bc=2.056802, λ=1.000, dt=0.020s, elapsed=0.8m\n",
      "[Net12] Ep 2400: loss=3.436424, pde=1.647545, bc=1.788879, λ=1.000, dt=0.021s, elapsed=0.9m\n",
      "[Net12] Ep 2600: loss=2.870608, pde=1.199024, bc=1.671584, λ=1.000, dt=0.021s, elapsed=1.0m\n",
      "[Net12] Ep 2800: loss=3.701980, pde=2.207343, bc=1.494636, λ=1.000, dt=0.021s, elapsed=1.0m\n",
      "[Net12] Ep 3000: loss=2.439957, pde=1.052817, bc=1.387140, λ=1.000, dt=0.020s, elapsed=1.1m\n",
      "[Net12] Ep 3200: loss=4.337110, pde=3.005801, bc=1.331309, λ=1.000, dt=0.021s, elapsed=1.2m\n",
      "[Net12] Ep 3400: loss=2.550781, pde=1.195204, bc=1.355577, λ=1.000, dt=0.020s, elapsed=1.3m\n",
      "[Net12] Ep 3600: loss=2.318421, pde=0.917358, bc=1.401064, λ=1.000, dt=0.026s, elapsed=1.3m\n",
      "[Net12] Ep 3800: loss=3.941190, pde=2.677382, bc=1.263807, λ=1.000, dt=0.021s, elapsed=1.4m\n",
      "[Net12] Ep 4000: loss=2.377995, pde=1.110951, bc=1.267043, λ=1.000, dt=0.020s, elapsed=1.5m\n",
      "[Net12] Ep 4200: loss=2.872111, pde=1.721743, bc=1.150368, λ=1.000, dt=0.021s, elapsed=1.6m\n",
      "[Net12] Ep 4400: loss=2.066139, pde=0.871370, bc=1.194768, λ=1.000, dt=0.021s, elapsed=1.6m\n",
      "[Net12] Ep 4600: loss=3.490854, pde=2.342984, bc=1.147869, λ=1.000, dt=0.021s, elapsed=1.7m\n",
      "[Net12] Ep 4800: loss=2.286927, pde=1.197778, bc=1.089149, λ=1.000, dt=0.022s, elapsed=1.8m\n",
      "[Net12] Ep 5000: loss=1.736154, pde=0.588152, bc=1.148003, λ=1.000, dt=0.022s, elapsed=1.9m\n",
      "[Net12] Ep 5200: loss=1.735879, pde=0.667790, bc=1.068090, λ=1.000, dt=0.027s, elapsed=1.9m\n",
      "[Net12] Ep 5400: loss=2.702655, pde=1.641313, bc=1.061342, λ=1.000, dt=0.024s, elapsed=2.0m\n",
      "[Net12] Ep 5600: loss=1.675237, pde=0.618882, bc=1.056355, λ=1.000, dt=0.022s, elapsed=2.1m\n",
      "[Net12] Ep 5800: loss=3.914860, pde=2.889456, bc=1.025404, λ=1.000, dt=0.021s, elapsed=2.2m\n",
      "[Net12] Ep 6000: loss=2.495700, pde=1.533223, bc=0.962477, λ=1.000, dt=0.022s, elapsed=2.2m\n",
      "[Net12] Ep 6200: loss=2.028307, pde=1.073099, bc=0.955208, λ=1.000, dt=0.020s, elapsed=2.3m\n",
      "[Net12] Ep 6400: loss=1.613667, pde=0.721072, bc=0.892595, λ=1.000, dt=0.021s, elapsed=2.4m\n",
      "[Net12] Ep 6600: loss=1.284781, pde=0.395114, bc=0.889667, λ=1.000, dt=0.022s, elapsed=2.5m\n",
      "[Net12] Ep 6800: loss=1.466032, pde=0.552740, bc=0.913292, λ=1.000, dt=0.027s, elapsed=2.5m\n",
      "[Net12] Ep 7000: loss=1.144643, pde=0.302730, bc=0.841913, λ=1.000, dt=0.020s, elapsed=2.6m\n",
      "[Net12] Ep 7200: loss=1.423413, pde=0.570892, bc=0.852521, λ=1.000, dt=0.021s, elapsed=2.7m\n",
      "[Net12] Ep 7400: loss=1.266540, pde=0.408349, bc=0.858191, λ=1.000, dt=0.020s, elapsed=2.8m\n",
      "[Net12] Ep 7600: loss=1.233988, pde=0.441736, bc=0.792252, λ=1.000, dt=0.020s, elapsed=2.8m\n",
      "[Net12] Ep 7800: loss=1.409558, pde=0.656861, bc=0.752697, λ=1.000, dt=0.021s, elapsed=2.9m\n",
      "[Net12] Ep 8000: loss=1.093795, pde=0.360493, bc=0.733302, λ=1.000, dt=0.021s, elapsed=3.0m\n",
      "[Net12] Ep 8200: loss=0.995337, pde=0.287766, bc=0.707571, λ=1.000, dt=0.021s, elapsed=3.0m\n",
      "[Net12] Ep 8400: loss=1.069828, pde=0.334990, bc=0.734838, λ=1.000, dt=0.020s, elapsed=3.1m\n",
      "[Net12] Ep 8600: loss=0.949989, pde=0.302233, bc=0.647756, λ=1.000, dt=0.020s, elapsed=3.2m\n",
      "[Net12] Ep 8800: loss=0.941072, pde=0.266368, bc=0.674704, λ=1.000, dt=0.020s, elapsed=3.3m\n",
      "[Net12] Ep 9000: loss=0.899233, pde=0.252089, bc=0.647144, λ=1.000, dt=0.036s, elapsed=3.3m\n",
      "[Net12] Ep 9200: loss=0.841285, pde=0.220766, bc=0.620520, λ=1.000, dt=0.020s, elapsed=3.4m\n",
      "[Net12] Ep 9400: loss=1.511250, pde=0.913267, bc=0.597982, λ=1.000, dt=0.020s, elapsed=3.5m\n",
      "[Net12] Ep 9600: loss=0.907887, pde=0.325158, bc=0.582729, λ=1.000, dt=0.023s, elapsed=3.6m\n",
      "[Net12] Ep 9800: loss=0.833380, pde=0.271078, bc=0.562302, λ=1.000, dt=0.020s, elapsed=3.6m\n",
      "[Net12] Ep 10000: loss=0.827774, pde=0.245338, bc=0.582436, λ=1.000, dt=0.020s, elapsed=3.7m\n",
      "[Net12] Ep 10200: loss=0.914604, pde=0.349984, bc=0.564619, λ=1.000, dt=0.021s, elapsed=3.8m\n",
      "[Net12] Ep 10400: loss=0.707855, pde=0.179004, bc=0.528851, λ=1.000, dt=0.020s, elapsed=3.9m\n",
      "[Net12] Ep 10600: loss=0.737776, pde=0.214451, bc=0.523326, λ=1.000, dt=0.029s, elapsed=3.9m\n",
      "[Net12] Ep 10800: loss=0.728735, pde=0.223273, bc=0.505463, λ=1.000, dt=0.021s, elapsed=4.0m\n",
      "[Net12] Ep 11000: loss=0.764872, pde=0.244281, bc=0.520591, λ=1.000, dt=0.020s, elapsed=4.1m\n",
      "[Net12] Ep 11200: loss=0.703433, pde=0.200717, bc=0.502716, λ=1.000, dt=0.020s, elapsed=4.2m\n",
      "[Net12] Ep 11400: loss=0.783252, pde=0.329342, bc=0.453910, λ=1.000, dt=0.021s, elapsed=4.2m\n",
      "[Net12] Ep 11600: loss=0.677840, pde=0.223973, bc=0.453867, λ=1.000, dt=0.022s, elapsed=4.3m\n",
      "[Net12] Ep 11800: loss=0.612390, pde=0.145534, bc=0.466856, λ=1.000, dt=0.022s, elapsed=4.4m\n",
      "[Net12] Ep 12000: loss=0.570966, pde=0.133363, bc=0.437603, λ=1.000, dt=0.021s, elapsed=4.5m\n",
      "[Net12] Ep 12200: loss=0.552646, pde=0.128719, bc=0.423927, λ=1.000, dt=0.033s, elapsed=4.5m\n",
      "[Net12] Ep 12400: loss=0.515643, pde=0.135520, bc=0.380123, λ=1.000, dt=0.021s, elapsed=4.6m\n",
      "[Net12] Ep 12600: loss=0.551690, pde=0.136414, bc=0.415276, λ=1.000, dt=0.020s, elapsed=4.7m\n",
      "[Net12] Ep 12800: loss=0.547118, pde=0.173475, bc=0.373643, λ=1.000, dt=0.038s, elapsed=4.8m\n",
      "[Net12] Ep 13000: loss=0.511157, pde=0.131349, bc=0.379807, λ=1.000, dt=0.020s, elapsed=4.8m\n",
      "[Net12] Ep 13200: loss=0.522234, pde=0.142371, bc=0.379862, λ=1.000, dt=0.023s, elapsed=4.9m\n",
      "[Net12] Ep 13400: loss=0.514285, pde=0.109223, bc=0.405062, λ=1.000, dt=0.021s, elapsed=5.0m\n",
      "[Net12] Ep 13600: loss=0.491332, pde=0.127603, bc=0.363730, λ=1.000, dt=0.022s, elapsed=5.0m\n",
      "[Net12] Ep 13800: loss=0.479671, pde=0.109323, bc=0.370348, λ=1.000, dt=0.022s, elapsed=5.1m\n",
      "[Net12] Ep 14000: loss=0.469958, pde=0.112852, bc=0.357106, λ=1.000, dt=0.020s, elapsed=5.2m\n",
      "[Net12] Ep 14200: loss=0.462888, pde=0.117355, bc=0.345533, λ=1.000, dt=0.020s, elapsed=5.3m\n",
      "[Net12] Ep 14400: loss=0.453314, pde=0.128659, bc=0.324655, λ=1.000, dt=0.028s, elapsed=5.3m\n",
      "[Net12] Ep 14600: loss=0.419362, pde=0.097079, bc=0.322283, λ=1.000, dt=0.020s, elapsed=5.4m\n",
      "[Net12] Ep 14800: loss=0.421258, pde=0.105592, bc=0.315666, λ=1.000, dt=0.022s, elapsed=5.5m\n",
      "[Net12] Ep 15000: loss=0.416382, pde=0.098789, bc=0.317594, λ=1.000, dt=0.020s, elapsed=5.6m\n",
      "[Net12] Ep 15200: loss=0.387431, pde=0.093435, bc=0.293996, λ=1.000, dt=0.020s, elapsed=5.6m\n",
      "[Net12] Ep 15400: loss=0.404039, pde=0.088453, bc=0.315586, λ=1.000, dt=0.022s, elapsed=5.7m\n",
      "[Net12] Ep 15600: loss=0.387423, pde=0.088191, bc=0.299232, λ=1.000, dt=0.020s, elapsed=5.8m\n",
      "[Net12] Ep 15800: loss=0.367483, pde=0.082919, bc=0.284564, λ=1.000, dt=0.021s, elapsed=5.9m\n",
      "[Net12] Ep 16000: loss=0.362629, pde=0.087809, bc=0.274821, λ=1.000, dt=0.027s, elapsed=5.9m\n",
      "[Net12] Ep 16200: loss=0.332731, pde=0.078797, bc=0.253934, λ=1.000, dt=0.022s, elapsed=6.0m\n",
      "[Net12] Ep 16400: loss=0.351472, pde=0.078680, bc=0.272792, λ=1.000, dt=0.021s, elapsed=6.1m\n",
      "[Net12] Ep 16600: loss=0.348666, pde=0.077155, bc=0.271511, λ=1.000, dt=0.021s, elapsed=6.2m\n",
      "[Net12] Ep 16800: loss=0.326144, pde=0.072181, bc=0.253963, λ=1.000, dt=0.020s, elapsed=6.2m\n",
      "[Net12] Ep 17000: loss=0.344704, pde=0.080541, bc=0.264163, λ=1.000, dt=0.023s, elapsed=6.3m\n",
      "[Net12] Ep 17200: loss=0.365182, pde=0.080901, bc=0.284282, λ=1.000, dt=0.021s, elapsed=6.4m\n",
      "[Net12] Ep 17400: loss=0.346458, pde=0.077440, bc=0.269019, λ=1.000, dt=0.021s, elapsed=6.5m\n",
      "[Net12] Ep 17600: loss=0.334612, pde=0.078962, bc=0.255651, λ=1.000, dt=0.026s, elapsed=6.5m\n",
      "[Net12] Ep 17800: loss=0.339465, pde=0.079897, bc=0.259568, λ=1.000, dt=0.021s, elapsed=6.6m\n",
      "[Net12] Ep 18000: loss=0.337725, pde=0.074132, bc=0.263593, λ=1.000, dt=0.022s, elapsed=6.7m\n",
      "[Net12] Ep 18200: loss=0.302512, pde=0.063917, bc=0.238596, λ=1.000, dt=0.020s, elapsed=6.8m\n",
      "[Net12] Ep 18400: loss=0.316823, pde=0.072571, bc=0.244252, λ=1.000, dt=0.021s, elapsed=6.8m\n",
      "[Net12] Ep 18600: loss=0.313974, pde=0.075908, bc=0.238066, λ=1.000, dt=0.020s, elapsed=6.9m\n",
      "[Net12] Ep 18800: loss=0.318547, pde=0.067521, bc=0.251026, λ=1.000, dt=0.023s, elapsed=7.0m\n",
      "[Net12] Ep 19000: loss=0.327797, pde=0.065878, bc=0.261919, λ=1.000, dt=0.020s, elapsed=7.1m\n",
      "[Net12] Ep 19200: loss=0.306684, pde=0.067096, bc=0.239588, λ=1.000, dt=0.027s, elapsed=7.1m\n",
      "[Net12] Ep 19400: loss=0.298229, pde=0.064910, bc=0.233319, λ=1.000, dt=0.021s, elapsed=7.2m\n",
      "[Net12] Ep 19600: loss=0.310322, pde=0.068770, bc=0.241552, λ=1.000, dt=0.027s, elapsed=7.3m\n",
      "[Net12] Ep 19800: loss=0.316204, pde=0.061269, bc=0.254935, λ=1.000, dt=0.020s, elapsed=7.4m\n",
      "[Net12] Ep 20000: loss=0.314510, pde=0.069112, bc=0.245397, λ=1.000, dt=0.020s, elapsed=7.4m\n",
      "Net12 training complete in 7.4 min (final loss 0.314510)\n"
     ]
    }
   ],
   "source": [
    "nets = [\n",
    "    (Net1(), \"Net1\"),\n",
    "    (Net2(), \"Net2\"),\n",
    "    (Net3(), \"Net3\"),\n",
    "    (Net4(), \"Net4\"),\n",
    "    (Net5(), \"Net5\"),\n",
    "    (Net6(), \"Net6\"),\n",
    "    (Net7(), \"Net7\"),\n",
    "    (Net8(), \"Net8\"),\n",
    "    (Net9(), \"Net9\"),\n",
    "    (Net10(), \"Net10\"),\n",
    "    (Net11(), \"Net11\"),\n",
    "    (Net12(), \"Net12\"),\n",
    "]\n",
    "# Addestra tutte le reti e salva i risultati\n",
    "lambda_values = [1.0]\n",
    "for net in nets:\n",
    "  train_pinn_poly(\n",
    "          net=net[0],\n",
    "          net_name=net[1],\n",
    "          lambda_weight=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 419638,
     "status": "ok",
     "timestamp": 1747758364951,
     "user": {
      "displayName": "Du Ysuwiqiq",
      "userId": "09648783062728575936"
     },
     "user_tz": -120
    },
    "id": "sR--OS_GoktG",
    "outputId": "06e642b8-44d6-4852-cf78-0d47fc31b7bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Net11()] Ep 1: loss=2597.104980, pde=2597.100342, bc=0.004758, λ=1.000, dt=0.021s, elapsed=0.0m\n",
      "[Net11()] Ep 200: loss=7.744564, pde=4.538638, bc=3.205926, λ=1.000, dt=0.019s, elapsed=0.1m\n",
      "[Net11()] Ep 400: loss=10.807238, pde=8.378482, bc=2.428756, λ=1.000, dt=0.024s, elapsed=0.1m\n",
      "[Net11()] Ep 600: loss=3.055820, pde=1.103853, bc=1.951967, λ=1.000, dt=0.019s, elapsed=0.2m\n",
      "[Net11()] Ep 800: loss=4.268519, pde=2.590521, bc=1.677999, λ=1.000, dt=0.020s, elapsed=0.3m\n",
      "[Net11()] Ep 1000: loss=2.593805, pde=1.106048, bc=1.487757, λ=1.000, dt=0.019s, elapsed=0.4m\n",
      "[Net11()] Ep 1200: loss=1.925068, pde=0.614765, bc=1.310303, λ=1.000, dt=0.020s, elapsed=0.4m\n",
      "[Net11()] Ep 1400: loss=1.737033, pde=0.572749, bc=1.164283, λ=1.000, dt=0.019s, elapsed=0.5m\n",
      "[Net11()] Ep 1600: loss=8.160249, pde=7.059542, bc=1.100707, λ=1.000, dt=0.019s, elapsed=0.6m\n",
      "[Net11()] Ep 1800: loss=3.437308, pde=2.468033, bc=0.969275, λ=1.000, dt=0.020s, elapsed=0.6m\n",
      "[Net11()] Ep 2000: loss=2.634519, pde=1.683024, bc=0.951495, λ=1.000, dt=0.034s, elapsed=0.7m\n",
      "[Net11()] Ep 2200: loss=4.618815, pde=3.816246, bc=0.802569, λ=1.000, dt=0.019s, elapsed=0.8m\n",
      "[Net11()] Ep 2400: loss=4.573468, pde=3.772681, bc=0.800787, λ=1.000, dt=0.020s, elapsed=0.8m\n",
      "[Net11()] Ep 2600: loss=1.543873, pde=0.838910, bc=0.704963, λ=1.000, dt=0.025s, elapsed=0.9m\n",
      "[Net11()] Ep 2800: loss=1.025492, pde=0.389854, bc=0.635639, λ=1.000, dt=0.019s, elapsed=1.0m\n",
      "[Net11()] Ep 3000: loss=1.976188, pde=1.420386, bc=0.555802, λ=1.000, dt=0.019s, elapsed=1.0m\n",
      "[Net11()] Ep 3200: loss=1.040527, pde=0.530708, bc=0.509819, λ=1.000, dt=0.027s, elapsed=1.1m\n",
      "[Net11()] Ep 3400: loss=2.941175, pde=2.486515, bc=0.454660, λ=1.000, dt=0.020s, elapsed=1.2m\n",
      "[Net11()] Ep 3600: loss=0.870041, pde=0.442050, bc=0.427991, λ=1.000, dt=0.019s, elapsed=1.3m\n",
      "[Net11()] Ep 3800: loss=0.707531, pde=0.353719, bc=0.353812, λ=1.000, dt=0.019s, elapsed=1.3m\n",
      "[Net11()] Ep 4000: loss=0.435394, pde=0.124058, bc=0.311336, λ=1.000, dt=0.021s, elapsed=1.4m\n",
      "[Net11()] Ep 4200: loss=1.948226, pde=1.656542, bc=0.291684, λ=1.000, dt=0.019s, elapsed=1.5m\n",
      "[Net11()] Ep 4400: loss=3.369949, pde=3.128447, bc=0.241502, λ=1.000, dt=0.019s, elapsed=1.5m\n",
      "[Net11()] Ep 4600: loss=0.795862, pde=0.580765, bc=0.215097, λ=1.000, dt=0.019s, elapsed=1.6m\n",
      "[Net11()] Ep 4800: loss=5.787616, pde=5.620975, bc=0.166642, λ=1.000, dt=0.019s, elapsed=1.7m\n",
      "[Net11()] Ep 5000: loss=2.682487, pde=2.518705, bc=0.163782, λ=1.000, dt=0.019s, elapsed=1.7m\n",
      "[Net11()] Ep 5200: loss=0.667656, pde=0.509606, bc=0.158050, λ=1.000, dt=0.021s, elapsed=1.8m\n",
      "[Net11()] Ep 5400: loss=1.437773, pde=1.319546, bc=0.118227, λ=1.000, dt=0.020s, elapsed=1.9m\n",
      "[Net11()] Ep 5600: loss=0.235728, pde=0.098228, bc=0.137500, λ=1.000, dt=0.020s, elapsed=2.0m\n",
      "[Net11()] Ep 5800: loss=0.725796, pde=0.626481, bc=0.099315, λ=1.000, dt=0.019s, elapsed=2.0m\n",
      "[Net11()] Ep 6000: loss=0.579114, pde=0.490582, bc=0.088532, λ=1.000, dt=0.021s, elapsed=2.1m\n",
      "[Net11()] Ep 6200: loss=2.519861, pde=2.434711, bc=0.085150, λ=1.000, dt=0.019s, elapsed=2.2m\n",
      "[Net11()] Ep 6400: loss=0.123534, pde=0.051908, bc=0.071626, λ=1.000, dt=0.019s, elapsed=2.2m\n",
      "[Net11()] Ep 6600: loss=0.371022, pde=0.304409, bc=0.066614, λ=1.000, dt=0.030s, elapsed=2.3m\n",
      "[Net11()] Ep 6800: loss=0.721124, pde=0.633155, bc=0.087970, λ=1.000, dt=0.019s, elapsed=2.4m\n",
      "[Net11()] Ep 7000: loss=0.176469, pde=0.120708, bc=0.055761, λ=1.000, dt=0.019s, elapsed=2.4m\n",
      "[Net11()] Ep 7200: loss=0.609607, pde=0.556420, bc=0.053186, λ=1.000, dt=0.025s, elapsed=2.5m\n",
      "[Net11()] Ep 7400: loss=1.438217, pde=1.390108, bc=0.048109, λ=1.000, dt=0.019s, elapsed=2.6m\n",
      "[Net11()] Ep 7600: loss=0.232975, pde=0.183740, bc=0.049234, λ=1.000, dt=0.020s, elapsed=2.6m\n",
      "[Net11()] Ep 7800: loss=3.550757, pde=3.487193, bc=0.063565, λ=1.000, dt=0.029s, elapsed=2.7m\n",
      "[Net11()] Ep 8000: loss=0.081977, pde=0.043289, bc=0.038688, λ=1.000, dt=0.019s, elapsed=2.8m\n",
      "[Net11()] Ep 8200: loss=0.183412, pde=0.150834, bc=0.032578, λ=1.000, dt=0.021s, elapsed=2.9m\n",
      "[Net11()] Ep 8400: loss=0.235990, pde=0.195948, bc=0.040042, λ=1.000, dt=0.021s, elapsed=2.9m\n",
      "[Net11()] Ep 8600: loss=0.189987, pde=0.159453, bc=0.030534, λ=1.000, dt=0.021s, elapsed=3.0m\n",
      "[Net11()] Ep 8800: loss=0.221934, pde=0.193564, bc=0.028370, λ=1.000, dt=0.019s, elapsed=3.1m\n",
      "[Net11()] Ep 9000: loss=0.132287, pde=0.105473, bc=0.026814, λ=1.000, dt=0.019s, elapsed=3.1m\n",
      "[Net11()] Ep 9200: loss=0.216387, pde=0.191541, bc=0.024845, λ=1.000, dt=0.020s, elapsed=3.2m\n",
      "[Net11()] Ep 9400: loss=0.057094, pde=0.034189, bc=0.022905, λ=1.000, dt=0.021s, elapsed=3.3m\n",
      "[Net11()] Ep 9600: loss=0.113573, pde=0.090417, bc=0.023156, λ=1.000, dt=0.019s, elapsed=3.4m\n",
      "[Net11()] Ep 9800: loss=0.849064, pde=0.827515, bc=0.021549, λ=1.000, dt=0.019s, elapsed=3.4m\n",
      "[Net11()] Ep 10000: loss=0.242046, pde=0.222099, bc=0.019947, λ=1.000, dt=0.020s, elapsed=3.5m\n",
      "[Net11()] Ep 10200: loss=0.143178, pde=0.123446, bc=0.019732, λ=1.000, dt=0.019s, elapsed=3.6m\n",
      "[Net11()] Ep 10400: loss=0.178523, pde=0.161040, bc=0.017483, λ=1.000, dt=0.020s, elapsed=3.6m\n",
      "[Net11()] Ep 10600: loss=0.102846, pde=0.085257, bc=0.017589, λ=1.000, dt=0.019s, elapsed=3.7m\n",
      "[Net11()] Ep 10800: loss=0.143571, pde=0.127386, bc=0.016185, λ=1.000, dt=0.019s, elapsed=3.8m\n",
      "[Net11()] Ep 11000: loss=0.058275, pde=0.042110, bc=0.016164, λ=1.000, dt=0.019s, elapsed=3.8m\n",
      "[Net11()] Ep 11200: loss=0.037813, pde=0.019817, bc=0.017997, λ=1.000, dt=0.025s, elapsed=3.9m\n",
      "[Net11()] Ep 11400: loss=0.044208, pde=0.029640, bc=0.014568, λ=1.000, dt=0.020s, elapsed=4.0m\n",
      "[Net11()] Ep 11600: loss=0.034808, pde=0.021333, bc=0.013475, λ=1.000, dt=0.019s, elapsed=4.0m\n",
      "[Net11()] Ep 11800: loss=0.154623, pde=0.141413, bc=0.013210, λ=1.000, dt=0.029s, elapsed=4.1m\n",
      "[Net11()] Ep 12000: loss=0.070764, pde=0.058213, bc=0.012552, λ=1.000, dt=0.020s, elapsed=4.2m\n",
      "[Net11()] Ep 12200: loss=0.030757, pde=0.018678, bc=0.012080, λ=1.000, dt=0.020s, elapsed=4.3m\n",
      "[Net11()] Ep 12400: loss=0.035455, pde=0.024025, bc=0.011429, λ=1.000, dt=0.019s, elapsed=4.3m\n",
      "[Net11()] Ep 12600: loss=0.040898, pde=0.029078, bc=0.011819, λ=1.000, dt=0.018s, elapsed=4.4m\n",
      "[Net11()] Ep 12800: loss=0.041643, pde=0.030095, bc=0.011548, λ=1.000, dt=0.019s, elapsed=4.5m\n",
      "[Net11()] Ep 13000: loss=0.039153, pde=0.027707, bc=0.011446, λ=1.000, dt=0.019s, elapsed=4.5m\n",
      "[Net11()] Ep 13200: loss=0.029329, pde=0.017818, bc=0.011511, λ=1.000, dt=0.020s, elapsed=4.6m\n",
      "[Net11()] Ep 13400: loss=0.041190, pde=0.031117, bc=0.010072, λ=1.000, dt=0.021s, elapsed=4.7m\n",
      "[Net11()] Ep 13600: loss=0.109333, pde=0.099203, bc=0.010130, λ=1.000, dt=0.019s, elapsed=4.8m\n",
      "[Net11()] Ep 13800: loss=0.031540, pde=0.021712, bc=0.009828, λ=1.000, dt=0.019s, elapsed=4.8m\n",
      "[Net11()] Ep 14000: loss=0.037037, pde=0.028116, bc=0.008921, λ=1.000, dt=0.019s, elapsed=4.9m\n",
      "[Net11()] Ep 14200: loss=0.091220, pde=0.081633, bc=0.009587, λ=1.000, dt=0.019s, elapsed=5.0m\n",
      "[Net11()] Ep 14400: loss=0.022338, pde=0.013614, bc=0.008725, λ=1.000, dt=0.020s, elapsed=5.0m\n",
      "[Net11()] Ep 14600: loss=0.031289, pde=0.021909, bc=0.009379, λ=1.000, dt=0.020s, elapsed=5.1m\n",
      "[Net11()] Ep 14800: loss=0.028148, pde=0.019379, bc=0.008769, λ=1.000, dt=0.019s, elapsed=5.2m\n",
      "[Net11()] Ep 15000: loss=0.029442, pde=0.020441, bc=0.009001, λ=1.000, dt=0.019s, elapsed=5.2m\n",
      "[Net11()] Ep 15200: loss=0.023059, pde=0.015168, bc=0.007890, λ=1.000, dt=0.030s, elapsed=5.3m\n",
      "[Net11()] Ep 15400: loss=0.017992, pde=0.009656, bc=0.008335, λ=1.000, dt=0.020s, elapsed=5.4m\n",
      "[Net11()] Ep 15600: loss=0.018993, pde=0.011292, bc=0.007701, λ=1.000, dt=0.019s, elapsed=5.4m\n",
      "[Net11()] Ep 15800: loss=0.019684, pde=0.011927, bc=0.007757, λ=1.000, dt=0.025s, elapsed=5.5m\n",
      "[Net11()] Ep 16000: loss=0.017540, pde=0.010059, bc=0.007481, λ=1.000, dt=0.019s, elapsed=5.6m\n",
      "[Net11()] Ep 16200: loss=0.017567, pde=0.010237, bc=0.007329, λ=1.000, dt=0.020s, elapsed=5.7m\n",
      "[Net11()] Ep 16400: loss=0.014055, pde=0.006478, bc=0.007577, λ=1.000, dt=0.025s, elapsed=5.7m\n",
      "[Net11()] Ep 16600: loss=0.021948, pde=0.013928, bc=0.008021, λ=1.000, dt=0.019s, elapsed=5.8m\n",
      "[Net11()] Ep 16800: loss=0.017422, pde=0.010249, bc=0.007173, λ=1.000, dt=0.019s, elapsed=5.9m\n",
      "[Net11()] Ep 17000: loss=0.014426, pde=0.007251, bc=0.007175, λ=1.000, dt=0.030s, elapsed=5.9m\n",
      "[Net11()] Ep 17200: loss=0.013449, pde=0.005943, bc=0.007506, λ=1.000, dt=0.021s, elapsed=6.0m\n",
      "[Net11()] Ep 17400: loss=0.013334, pde=0.006288, bc=0.007046, λ=1.000, dt=0.020s, elapsed=6.1m\n",
      "[Net11()] Ep 17600: loss=0.013343, pde=0.006068, bc=0.007275, λ=1.000, dt=0.019s, elapsed=6.1m\n",
      "[Net11()] Ep 17800: loss=0.013687, pde=0.006749, bc=0.006938, λ=1.000, dt=0.020s, elapsed=6.2m\n",
      "[Net11()] Ep 18000: loss=0.012772, pde=0.006139, bc=0.006633, λ=1.000, dt=0.021s, elapsed=6.3m\n",
      "[Net11()] Ep 18200: loss=0.013104, pde=0.005854, bc=0.007250, λ=1.000, dt=0.021s, elapsed=6.4m\n",
      "[Net11()] Ep 18400: loss=0.011479, pde=0.004523, bc=0.006956, λ=1.000, dt=0.020s, elapsed=6.4m\n",
      "[Net11()] Ep 18600: loss=0.012077, pde=0.005621, bc=0.006456, λ=1.000, dt=0.019s, elapsed=6.5m\n",
      "[Net11()] Ep 18800: loss=0.012839, pde=0.005504, bc=0.007336, λ=1.000, dt=0.019s, elapsed=6.6m\n",
      "[Net11()] Ep 19000: loss=0.011907, pde=0.005072, bc=0.006835, λ=1.000, dt=0.020s, elapsed=6.6m\n",
      "[Net11()] Ep 19200: loss=0.013234, pde=0.006616, bc=0.006617, λ=1.000, dt=0.020s, elapsed=6.7m\n",
      "[Net11()] Ep 19400: loss=0.012671, pde=0.005426, bc=0.007246, λ=1.000, dt=0.022s, elapsed=6.8m\n",
      "[Net11()] Ep 19600: loss=0.011391, pde=0.004994, bc=0.006397, λ=1.000, dt=0.019s, elapsed=6.8m\n",
      "[Net11()] Ep 19800: loss=0.011603, pde=0.004788, bc=0.006814, λ=1.000, dt=0.026s, elapsed=6.9m\n",
      "[Net11()] Ep 20000: loss=0.011503, pde=0.004852, bc=0.006652, λ=1.000, dt=0.024s, elapsed=7.0m\n",
      "Net11() training complete in 7.0 min (final loss 0.011503)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "nets = [\n",
    "    (Net1(), \"Net1\"),\n",
    "    (Net2(), \"Net2\"),\n",
    "    (Net3(), \"Net3\"),\n",
    "    (Net4(), \"Net4\"),\n",
    "    (Net5(), \"Net5\"),\n",
    "    (Net6(), \"Net6\"),\n",
    "    (Net7(), \"Net7\"),\n",
    "    (Net8(), \"Net8\"),\n",
    "    (Net9(), \"Net9\"),\n",
    "    (Net10(), \"Net10\"),\n",
    "    (Net11(), \"Net11\"),\n",
    "    (Net12(), \"Net12\"),\n",
    "]\"\"\"\n",
    "# Addestra tutte le reti e salva i risultati\n",
    "lambda_values = [1.0]\n",
    "net = Net11()\n",
    "net_name = 'Net11()'\n",
    "train_pinn_poly(\n",
    "        net=net,\n",
    "        net_name=net_name,\n",
    "        lambda_weight=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1747758388441,
     "user": {
      "displayName": "Du Ysuwiqiq",
      "userId": "09648783062728575936"
     },
     "user_tz": -120
    },
    "id": "0RIIKx1yFtGA",
    "outputId": "478a4ff5-6985-4e39-fab3-6f83a84892fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinate shape: (4623, 2)\n",
      "Prime 5 coordinate: [[0.9298771  0.46188024]\n",
      " [0.91123877 0.45266557]\n",
      " [0.93072317 0.44540916]\n",
      " [0.48698419 0.27382701]\n",
      " [0.4980866  0.26058181]]\n",
      "(4623, 2)\n"
     ]
    }
   ],
   "source": [
    "quadr_coords = []\n",
    "file_path = \"/content/SolutionOnPoints_0.inp\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    next(f)  # salto header se serve\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        # Provo a convertire, se fallisce salto la riga\n",
    "        try:\n",
    "            idx = int(parts[0])\n",
    "            x = float(parts[1])\n",
    "            y = float(parts[2])\n",
    "        except ValueError:\n",
    "            # Righe non dati, le salto\n",
    "            continue\n",
    "\n",
    "        quadr_coords.append((x, y))\n",
    "quadr_coords = np.array(quadr_coords)  # shape (N,2)\n",
    "print(\"Coordinate shape:\", quadr_coords.shape)\n",
    "print(\"Prime 5 coordinate:\", quadr_coords[:5])\n",
    "print(quadr_coords.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1747758392025,
     "user": {
      "displayName": "Du Ysuwiqiq",
      "userId": "09648783062728575936"
     },
     "user_tz": -120
    },
    "id": "PV-X8fwshC_9",
    "outputId": "102157d9-87c6-4e6e-e2a1-e07eaee04293"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84508049, 0.3486542 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose new random seed bc we need a new test set\n",
    "#np.random.seed(23) #gia fatto sopra\n",
    "\n",
    "test_set = np.random.uniform(low=0.1, high=1, size=(100, 2)) # test set\n",
    "test_set[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1747758393494,
     "user": {
      "displayName": "Du Ysuwiqiq",
      "userId": "09648783062728575936"
     },
     "user_tz": -120
    },
    "id": "rIhz2BjeGinJ",
    "outputId": "0e33cb85-db75-4172-9b2f-3f1d9cb22a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvataggio completato: predizioni_punti_quadratura_net11.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.eval() # gia sta in gpu dal train\n",
    "\n",
    "coords_tensor = torch.tensor(quadr_coords, dtype=torch.float32)  # (N, 2)\n",
    "\n",
    "all_preds = []  # lista per salvare le predizioni per ogni mu\n",
    "\n",
    "with torch.no_grad():\n",
    "    for mu in test_set:\n",
    "        mu0_val, mu1_val = mu\n",
    "\n",
    "        mu0_tensor = torch.full((coords_tensor.shape[0], 1), fill_value=mu0_val, dtype=torch.float32)\n",
    "        mu1_tensor = torch.full((coords_tensor.shape[0], 1), fill_value=mu1_val, dtype=torch.float32)\n",
    "\n",
    "        inputs = torch.cat([coords_tensor, mu0_tensor, mu1_tensor], dim=1)  # (N,4)\n",
    "        inputs = inputs.to(device) #manda in gpu\n",
    "        u_pred = net(inputs).cpu().numpy().flatten()  # (N,)\n",
    "\n",
    "        all_preds.append(u_pred)\n",
    "\n",
    "# Trasponi per avere righe = punti quadratura, colonne = diversi mu\n",
    "all_preds = np.array(all_preds).T  # shape (N_punti, N_mu)\n",
    "\n",
    "# Crea nomi colonne (es: mu0_0.5_mu1_0.5)\n",
    "col_names = [f\"mu0_{mu[0]:.3f}_mu1_{mu[1]:.3f}\" for mu in test_set]\n",
    "\n",
    "# Crea DataFrame pandas\n",
    "df = pd.DataFrame(all_preds, columns=col_names)\n",
    "\n",
    "# Salva su CSV\n",
    "df.to_csv(\"predizioni_punti_quadratura_net11.csv\", index=False)\n",
    "\n",
    "print(\"Salvataggio completato: predizioni_punti_quadratura_net11.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmSljAHwFDx_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1747559213614,
     "user": {
      "displayName": "auro",
      "userId": "01708090046865201755"
     },
     "user_tz": -120
    },
    "id": "VplhDoX0flcT",
    "outputId": "0b6f1d77-8218-45a0-a856-10219bc7f8cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84508049, 0.3486542 ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_mu = test_set[-1]\n",
    "last_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgcs1ileoo-K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
